<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=Edge"><meta http-equiv=permissions-policy content="interest-cohort=()"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#2b7bb5"><meta name=copyright content="Copyright (c)2025 Erik McClure"><meta name=keywords content="games,music,code,erik mcclure,erikmcclure,aurora theory,sweetie bot,discord,feathergui,fading light,tinyoal,cloud hop"><meta name=robots content="index,follow"><meta name=googlebot content="index,follow"><meta name=google-site-verification content="Oxb2ia8HjcHLXlvstA8xPpQO3BO_y15Ds2Ia-feq1MQ"><meta name=generator content="Hugo 0.119.0"><link rel=me href=https://equestria.social/@cloudhop><link rel=canonical href=https://erikmcclure.com/blog/technological-tsunami/><link rel=apple-touch-icon href=https://erikmcclure.com/favicon.ico><link rel="shortcut icon" type=image/x-icon href=https://erikmcclure.com/favicon.ico><link rel=stylesheet href=https://erikmcclure.com/css/main.css><link rel=stylesheet href=https://erikmcclure.com/css/prism.css rel=stylesheet><link rel=stylesheet href=https://erikmcclure.com/css/katex.min.css rel=stylesheet><link rel=stylesheet href=https://erikmcclure.com/css/fontawesome.min.css><link rel=stylesheet href=https://erikmcclure.com/css/regular.min.css><link rel=stylesheet href=https://erikmcclure.com/css/brands.min.css><link rel=stylesheet href=https://erikmcclure.com/css/solid.min.css><link rel=alternate type=application/rss+xml title="Erik McClure - RSS" href=https://erikmcclure.com/blog/index.xml><meta property="og:type" content="article"><title>The Technological Tsunami</title><meta property="og:title" content="The Technological Tsunami"><meta name=twitter:title content="The Technological Tsunami"><meta itemprop=name content="The Technological Tsunami"><meta name=description content="My relationship with AI is getting increasingly strange. Generalist AIs are still mostly useless, but narrow AIs continue to produce very impressive results. We have plenty of AIs that are better than any human at specific tasks like spotting cancer, but no AIs that can exercise common sense. We can synthesize terrifyingly realistic recreations of almost anyone&rsquo;s voice, but they must be handheld by humans to produce consistent emotional inflection. We have self-driving cars that work fine at noon on a clear day with no construction, but an errant traffic cone makes them panic."><meta property="og:description" content="My relationship with AI is getting increasingly strange. Generalist AIs are still mostly useless, but narrow AIs continue to produce very impressive results. We have plenty of AIs that are better than any human at specific tasks like spotting cancer, but no AIs that can exercise common sense. We can synthesize terrifyingly realistic recreations of almost anyone&rsquo;s voice, but they must be handheld by humans to produce consistent emotional inflection. We have self-driving cars that work fine at noon on a clear day with no construction, but an errant traffic cone makes them panic."><meta name=twitter:description content="My relationship with AI is getting increasingly strange. Generalist AIs are still mostly useless, but narrow AIs continue to produce very impressive results. We have plenty of AIs that are better than any human at specific tasks like spotting cancer, but no AIs that can exercise common sense. We can synthesize terrifyingly realistic recreations of almost anyone&rsquo;s voice, but they must be handheld by humans to produce consistent emotional inflection. We have self-driving cars that work fine at noon on a clear day with no construction, but an errant traffic cone makes them panic."><meta itemprop=description content="My relationship with AI is getting increasingly strange. Generalist AIs are still mostly useless, but narrow AIs continue to produce very impressive results. We have plenty of AIs that are better than any human at specific tasks like spotting cancer, but no AIs that can exercise common sense. We can synthesize terrifyingly realistic recreations of almost anyone&rsquo;s voice, but they must be handheld by humans to produce consistent emotional inflection. We have self-driving cars that work fine at noon on a clear day with no construction, but an errant traffic cone makes them panic."><meta property="og:url" content="https://erikmcclure.com/"><meta property="og:site_name" content="Erik McClure"><meta property="og:image" content="https://erikmcclure.com/img/avatar.png"><meta property="og:locale" content="en-US"><meta property="article:author" content="Erik McClure"><meta name=twitter:card content="summary"><meta name=twitter:site content="@erikmcclure0173"><meta name=twitter:creator content="@erikmcclure0173"><meta name=twitter:image content="https://erikmcclure.com/img/avatar.png"><meta name=twitter:dnt content="on"><link href=https://plus.google.com/104896885003230920472 rel=publisher><meta itemprop=image content="https://erikmcclure.com/img/avatar.png"><meta name=last-updated content="20251122-22:40:00.000"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-63026815-3"></script>
<script defer src=https://erikmcclure.com/syntax-prism.js></script>
<script defer src=https://erikmcclure.com/katex.min.js></script>
<script defer src=https://erikmcclure.com/mathtex-script-type.min.js></script>
<script defer src=https://erikmcclure.com/auto-render.min.js></script>
<script defer src=https://cdn.commento.io/js/count.js></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-63026815-3")</script></head><body><div id=container><header><nav><ul><li><a href=/blog/ title=Blog><i class="fa-solid fa-book fa-fw fa-lg"></i>&nbsp;<p>Blog</p></a></li><li><a href=/projects/ title=Projects><i class="fa-solid fa-briefcase fa-fw fa-lg"></i>&nbsp;<p>Projects</p></a></li><li><a href=https://erikmcclure.bandcamp.com title=Bandcamp><i class="fa-brands fa-bandcamp fa-fw fa-lg"></i>&nbsp;<p>Bandcamp</p></a></li><li><a href=https://github.com/erikmcclure title=Github><i class="fa-brands fa-github fa-fw fa-lg"></i>&nbsp;<p>Github</p></a></li><li><a href=/web/ title=Websites><i class="fa-solid fa-globe fa-fw fa-lg"></i>&nbsp;<p>Websites</p></a></li></ul></nav><div class=dim><h1>Erik McClure</h1></div></header><main class=blog><section><meta itemprop=mainEntityOfPage itemtype=https://schema.org/WebPage content="https://erikmcclure.com/"><meta itemprop=dateModified content="2025-11-22T22:40:00+00:00"><meta itemprop=headline content="The Technological Tsunami"><meta itemprop=description content="My relationship with AI is getting increasingly strange. Generalist AIs are still mostly useless, but narrow AIs continue to produce very impressive results. We have plenty of AIs that are better than any human at specific tasks like spotting cancer, but no AIs that can exercise common sense. We can synthesize terrifyingly realistic recreations of almost anyone&rsquo;s voice, but they must be handheld by humans to produce consistent emotional inflection. We have self-driving cars that work fine at noon on a clear day with no construction, but an errant traffic cone makes them panic."><meta itemprop=url content="https://erikmcclure.com/blog/technological-tsunami/"><div itemprop=publisher itemscope itemtype=https://schema.org/Organization><div itemprop=logo itemscope itemtype=https://schema.org/ImageObject><meta itemprop=url content="https://erikmcclure.com/img/avatar.png"><meta itemprop=width content="140"><meta itemprop=height content="140"></div><meta itemprop=name content="Erik McClure"></div><div itemprop=author itemscope itemtype=https://schema.org/Person><meta itemprop=name content="Erik McClure"></div><article><h4>The Technological Tsunami</h4><hr><div class=padding><p>My relationship with AI is getting increasingly strange. Generalist AIs are still mostly useless, but narrow AIs continue to produce very impressive results. We have plenty of AIs that are better than any human at specific tasks like spotting cancer, but no AIs that can exercise common sense. We can synthesize terrifyingly realistic recreations of almost anyone&rsquo;s voice, but they must be handheld by humans to produce consistent emotional inflection. We have self-driving cars that work fine at noon on a clear day with no construction, but an errant traffic cone makes them panic.</p><p>This is called &ldquo;spiky intelligence&rdquo;, and it is why ChatGPT can solve incredibly difficult math olympiad questions but struggle to push a button on a webpage. It seems to me that all these smart people with PhDs saying that AI will take over the workforce are convinced that, if AIs can continually get better at tackling difficult problems, eventually they&rsquo;ll be able to train AIs that can also handle &ldquo;easy&rdquo; problems.</p><p>This is the exact same error that resulted in the second <a href=https://en.wikipedia.org/wiki/AI_winter>AI winter</a> of the 90s - when researchers built expert systems could outperform humans in narrow situations, they simply assumed they would soon be able to outperform all humans in all situations. This, obviously, didn&rsquo;t happen, but task-specific engines did emerge from this, and now it is a well-known fact that your phone has enough processing power to effortlessly destroy every single human chess grandmaster that has ever lived. We still play chess, though.</p><p>What worries me is that this kind of spiky intelligence, despite lacking general common sense, will still radically upend the economy in ways that are simply impossible for a human brain to anticipate, because these AIs are, by definition, alien intelligences that defy all human intuition. The coming AI revolution is dangerous not because it&rsquo;s going to destroy the whole world if we get it wrong, but because <em><strong>it is almost impossible to anticipate</strong></em> in any meaningful way. No human is capable of accurately guessing what weirdly specific task an AI might find easy or extraordinarily difficult. It&rsquo;s <a href=https://xkcd.com/1425/>XKCD #1425</a> but randomized for every single task on the entire planet:</p><div class=imgwrap style=max-width:267px><a href=https://imgs.xkcd.com/comics/tasks.png target=_blank><img src=https://imgs.xkcd.com/comics/tasks.png alt="In CS, it can be hard to explain the difference between the easy and the virtually impossible" width=100%></a></div><p>AI enthusiasts often like to talk about <a href=https://en.wikipedia.org/wiki/Technological_singularity>The Singularity</a>, a point in time when technological progress accelerates beyond human understanding and thus the future beyond it becomes unknowable. To me, this is not a very useful thing to think about. After all, we&rsquo;re already incapable of predicting what society will look like 10 years from now. What&rsquo;s concerning is that we&rsquo;re used to being able to prepare for the next 2-3 years (ignoring <a href=https://en.wikipedia.org/wiki/Black_swan_theory>black swan events</a>), and I anticipate that AI will cause economic chaos in ways we cannot predict, precisely because it will rapidly automate entire categories of human employment out of existence, <em>randomly</em>. What will happen when we start automating entire industries faster than we can retrain people? What happens when someone tries to migrate to another career only to have that career automated away the moment they graduate?</p><p>We already struggle to keep up with the rapid pace of change, and AI is about to automate everything even faster, in <em>extremely</em> unpredictable ways. There may be a moment when it becomes impossible to anticipate the trajectory of your career three months from now, without AGI ever happening. <strong>We don&rsquo;t <em>need</em> a superintelligent godlike AI to fuck everything up</strong>, the extraordinarily powerful narrow AIs we&rsquo;re working on right now can fuck up the whole global economy by themselves. This moment is the only &ldquo;Singularity&rdquo; that I care about - a sort of Technological Tsunami, when entire economic sectors are swept away by rapid automation so quickly that workers can&rsquo;t course correct to a new field fast enough.</p><p>We have options, since we know that it <em>will</em> fuck up the economy, we just don&rsquo;t know <em>how</em>. The easiest and most pragmatic solution is <a href=https://en.wikipedia.org/wiki/Universal_basic_income>UBI</a>, but this seems difficult to make happen in a society run by rich people who are largely rewarded by how evil they are. There are plenty of political groups that are pushing for these kinds of solutions, but global policymakers appear to have <a href=https://www.france24.com/en/live-news/20251115-eu-bows-to-pressure-on-loosening-ai-privacy-rules>been captured by AI money</a>, which is only concerned about the dangers of a mythological AGI superintelligence instead of the impending economic catastrophe that is already beginning to develop. Because of this, I think there is a real question over whether or not human society will survive the coming technological tsunami. Again, we don&rsquo;t need to invent AGI to destroy ourselves. We didn&rsquo;t need AI to build nukes.</p><p>With that said, some people seem to deny that AGI will <em>ever</em> happen, which is also clearly wrong, at least to me. There are many things that will <em>eventually</em> happen, based on our current understanding of physics (and assuming we don&rsquo;t blow ourselves up). <em>Eventually</em> we&rsquo;ll cure cancer. <em>Eventually</em> we&rsquo;ll reverse aging. <em>Eventually</em> we&rsquo;ll have cybernetic implants and androids. <em>Eventually</em> we&rsquo;ll be able to upload human minds to a computer. <em>Eventually</em> we&rsquo;ll build a general artificial intelligence capable of improving itself. It might take 10 years or 100 years or 1000 years, but these are all things that will almost certainly happen given enough time and effort, we just don&rsquo;t know when. At the very least, if you build as much computational power as the entire combined brainpower of the human race, you&rsquo;ll be able to brute-force a superintelligence of some kind, and we&rsquo;d have better solved the alignment problem by then, or augmented ourselves enough to handle it.</p><p>I think that sometimes, we use denial as a way to cope with knowing that cures to our ailments might be discovered just months after we die, because we don&rsquo;t want to think about being that astronomically unlucky. Given that immunotherapy is already <a href=https://www.rockefeller.edu/news/38120-immunotherapy-drug-eliminates-aggressive-cancers-in-clinical-trial/>curing some inoperable cancers in early stage trials</a> and primitive gene therapy has <a href=https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html>successfully cured a baby&rsquo;s genetic disorder</a>, the discussion about who gets to be the lucky generation that hits life-extension escape velocity is rapidly becoming unavoidable. Depending on when that happens, I could be part of the luckiest generation in all of history, or the unluckiest generation in all of history. As a society, we <em>could</em> leverage the automation boom to train more scientists and enhance research velocity, thereby giving ourselves a better chance, but many people don&rsquo;t dare to give themselves that kind of hope. Without the right cultural framework, that kind of hope can be <em>extremely painful</em> to even think about. It&rsquo;s much easier to assume there is nothing anyone can do, and that everyone alive today - especially the billionaires ruining everyone&rsquo;s lives - will eventually die.</p><p>It&rsquo;s important not to judge each other too harshly for having these kinds of irrational responses, because the moment when death&rsquo;s hourglass is finally shattered only happens once in the entire history of our species. We will always have to deal with death itself, as you&rsquo;ll always die if you get hit by a truck (or the <a href="https://www.youtube.com/watch?v=qu8nx1qUwEU">entire concentrated power of the sun</a>), but biological immortality fundamentally alters how we think about <em>everything</em>. We were never designed for this. Our minds can grasp the abstract concept, but none of us can truly comprehend the implications of no longer having to worry about dying of old age until it actually happens. Human brains cannot process this kind of knowledge beyond an abstract level, just like we cannot comprehend how big and empty space <em>really</em> is until we actually go to another planet.</p><p>At the same time, very smart people continue making wild extrapolations about the capabilities of AIs that simply don&rsquo;t line up with real world performance. You cannot assume that an AI that can score better than all humans at every test will actually be good at anything other than taking tests, even if humans who score highly on those tests sometimes accomplish amazing things. I have a friend who was placed in Mensa at a very young age after scoring high on an IQ test. The only thing this organization of very smart people do is argue about how to run the organization and what the latest cool puzzles are.</p><blockquote>if you know you are actually much more intelligent than the statistical average, increase your humility. It is too easy to believe your own judgements, to get stuck in your own bullshit. Being smart does not make you wise. Wisdom comes from constantly doubting yourself, and questioning your own thoughts and beliefs. Never think, even for a moment, that you have 'settled' anything completely. It's okay to know you are bright, it is not okay to think that gives you any certainty or authority of understanding. &mdash; <a href=https://www.fimfiction.net/blog/1132722/one-oclock-just-possibly-before-midnight>Chatoyance</a></blockquote><p>The world&rsquo;s smartest people are struggling to extrapolate the capabilities of an extremely spiky and utterly alien narrow intelligence, because it defies basic human intuition. Assuming an AI will be good at performing arbitrary tasks because it scored well on a test is the same kind of attribution error that happens with experts in a specific field - people will trust the expert&rsquo;s opinion on something the expert has no experience with, like the economy, even though this almost <a href=https://en.wikipedia.org/wiki/South_Sea_Company#Quotations_prompted_by_the_collapse>never works out</a>. This is such a persistent problem because highly intelligent people can invent plausible sounding arguments to support almost any position, and it can be exceedingly difficult to find the logical error in them. We are lucky that our current LLMs <em>usually</em> make egregious errors that are obviously wrong, instead of extremely subtle errors that would be almost impossible to detect.</p><p>We are in the middle of an AI revolution that will create new, extraordinarily powerful tools whose effects are almost impossible to predict. Instead of doing anything about the impending economic catastrophe, we are chasing AI safety hysteria and telling AGI superintelligence ghost stories that will likely not happen for decades, if not centuries. Otherwise intelligent people are convincing themselves that there&rsquo;s no point worrying about the economy crashing if AGI makes humans irrelevant. We&rsquo;re so busy trying to avoid flying too close to the sun we haven&rsquo;t noticed a technological tsunami rising up beneath us, and if we continue ignoring it, we&rsquo;ll drown before we even become airborne.</p></div><hr><time itemprop=datePublished pubdate=pubdate datetime=2025-11-22T22:40:00+00:00><i class="fa-regular fa-clock fa-fw"></i>&nbsp;Published on <a href=https://erikmcclure.com/blog/technological-tsunami/>November 22, 2025 at 10:40pm</a></time><aside><i class="fa-regular fa-share-square fa-fw"></i>&nbsp;share:<ul><li><a href="javascript:window.open('https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2ferikmcclure.com%2fblog%2ftechnological-tsunami%2f','popup','width=600,height=400');"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="twtr-icon twtr-color-fill--blue-dark has-hover"><style>.st0{fill:#666}</style><path class="st0" d="M16.75 9H13.5V7a1 1 0 011-1h2V3H14a4 4 0 00-4 4V9H8v3h2v9h3.5V12H16z"/></svg></a><li><a href="javascript:window.open('https://twitter.com/intent/tweet?text=The%20Technological%20Tsunami&url=https%3a%2f%2ferikmcclure.com%2fblog%2ftechnological-tsunami%2f','popup','width=600,height=256');"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="twtr-icon twtr-color-fill--blue-dark has-hover"><style>.st0{fill:#666}</style><path class="st0" opacity="0" d="M0 0h24v24H0z"/><path class="st0" d="M23.643 4.937c-.835.37-1.732.62-2.675.733.962-.576 1.7-1.49 2.048-2.578-.9.534-1.897.922-2.958 1.13-.85-.904-2.06-1.47-3.4-1.47-2.572.0-4.658 2.086-4.658 4.66.0.364.042.718.12 1.06-3.873-.195-7.304-2.05-9.602-4.868-.4.69-.63 1.49-.63 2.342.0 1.616.823 3.043 2.072 3.878-.764-.025-1.482-.234-2.11-.583v.06c0 2.257 1.605 4.14 3.737 4.568-.392.106-.803.162-1.227.162-.3.0-.593-.028-.877-.082.593 1.85 2.313 3.198 4.352 3.234-1.595 1.25-3.604 1.995-5.786 1.995-.376.0-.747-.022-1.112-.065 2.062 1.323 4.51 2.093 7.14 2.093 8.57.0 13.255-7.098 13.255-13.254.0-.2-.005-.402-.014-.602.91-.658 1.7-1.477 2.323-2.41z"/></svg></a></li><li><a href="javascript:window.open('https://www.linkedin.com/sharing/share-offsite/?url=https%3a%2f%2ferikmcclure.com%2fblog%2ftechnological-tsunami%2f','popup','width=700,height=380');"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="twtr-icon twtr-color-fill--blue-dark has-hover"><style>.st0{fill:#666}</style><rect class="st0" height="11" width="4" x="3" y="9"/><circle class="st0" cx="5" cy="5" r="2"/><path class="st0" d="M16.5 8.25A4.47251 4.47251.0 0013 9.95343V9H9V20h4V13a2 2 0 014 0v7h4V12.75a4.5 4.5.0 00-4.5-4.5z"/></svg></a></li></ul></aside></article><article><a name=comments><h4>Comments</h4></a><hr><div class=comments></div><div class=comments></div><div class=wrapcommento><div id=commento></div><script src=https://cdn.commento.io/js/commento.js></script></div></form></article></section><section class=archive><article><img src=https://erikmcclure.com/img/avatar.th.png alt=Avatar width=180 height=180><h2>Archive</h2><ol><li><details><summary>2025</summary><ul><li><a href=/blog/technological-tsunami/>The Technological Tsunami</a></li><li><a href=/blog/purity-spiral/>Leftists Are In A Purity Death Spiral</a></li><li><a href=/blog/discord-overlay-breaks-gsync/>The New Discord Overlay Breaks GSync and Borderless Optimizations</a></li><li><a href=/blog/do-you-really-think-we-have-gender-in-the-future/>Do You Really Think We'll Have Genders In The Future?</a></li></ul></details></li><li><details><summary>2024</summary><ul><li><a href=/blog/stop-making-me-memorize-borrow-checker/>Stop Making Me Memorize The Borrow Checker</a></li><li><a href=/blog/rust-async-makes-me-want-to-gouge-my-eyes-out/>Rust Async Makes Me Want To Gouge My Eyes Out</a></li><li><a href=/blog/engineers-only-get-paid-something-broken/>Engineers Only Get Paid If Something Is Broken</a></li><li><a href=/blog/measuring-competence-is-epistemic-hell/>Measuring Competence Is Epistemic Hell</a></li><li><a href=/blog/we-could-fix-everything-we-just-dont/>We Could Fix Everything, We Just Don't</a></li></ul></details></li><li><details><summary>2023</summary><ul><li><a href=/blog/people-cant-care-about-everything/>People Can't Care About Everything</a></li><li><a href=/blog/discord-should-remove-usernames/>Discord Should Remove Usernames Entirely</a></li></ul></details></li><li><details><summary>2022</summary><ul><li><a href=/blog/money-is-fake/>Money Is Fake. It's Not Real. It's Made Up.</a></li><li><a href=/blog/we-need-new-motherboards/>We Need New Motherboards Before GPUs Collapse Under Their Own Gravity</a></li></ul></details></li><li><details><summary>2021</summary><ul><li><a href=/blog/cpp-constructors-memory-and-lifetimes/>C++ Constructors, Memory, and Lifetimes</a></li><li><a href=/blog/factorio-is-best-interview-we-have/>Factorio Is The Best Technical Interview We Have</a></li></ul></details></li><li><details><summary>2020</summary><ul><li><a href=/blog/why-you-cant-use-prebuilt-llvm-with-cpp17/>Why You Can't Use Prebuilt LLVM 10.0 with C++17</a></li><li><a href=/blog/pressure-based-anti-spam-for-discord-bots/>Pressure Based Anti-Spam for Discord Bots</a></li></ul></details></li><li><details><summary>2019</summary><ul><li><a href=/blog/name-shadowing-should-be-an-operator/>Name Shadowing Should Be An Operator</a></li><li><a href=/blog/a-rant-on-terra/>A Rant On Terra</a></li><li><a href=/blog/risc-is-fundamentally-unscalable/>RISC Is Fundamentally Unscalable</a></li></ul></details></li><li><details><summary>2018</summary><ul><li><a href=/blog/migrating-to-static-blog/>Migrating To A Static Blog</a></li><li><a href=/blog/how-to-avoid-memorizing-times-tables/>How To Avoid Memorizing Times Tables</a></li></ul></details></li><li><details><summary>2017</summary><ul><li><a href=/blog/ignoring-outliers-creates-racist/>Ignoring Outliers Creates Racist Algorithms</a></li><li><a href=/blog/i-used-to-want-to-work-for-google/>I Used To Want To Work For Google</a></li><li><a href=/blog/integrating-luajit-and-autogenerating-c/>Integrating LuaJIT and Autogenerating C Bindings In Visual Studio</a></li><li><a href=/blog/discord-rise-of-bot-wars/>Discord: Rise Of The Bot Wars</a></li><li><a href=/blog/companies-cant-be-apolitical/>Companies Can't Be Apolitical</a></li><li><a href=/blog/windows-wont-let-my-program-crash/>Windows Won't Let My Program Crash</a></li><li><a href=/blog/directx-is-terrifying/>DirectX Is Terrifying</a></li></ul></details></li><li><details><summary>2016</summary><ul><li><a href=/blog/everyone-does-srgb-wrong-because/>Everyone Does sRGB Wrong Because Everyone Else Does sRGB Wrong</a></li><li><a href=/blog/mathematical-notation-is-awful/>Mathematical Notation Is Awful</a></li></ul></details></li><li><details><summary>2015</summary><ul><li><a href=/blog/i-tried-to-install-linux-and-now-i/>I Tried To Install Linux And Now I Regret Everything</a></li><li><a href=/blog/you-arent-designing-software-for-robots/>We Aren't Designing Software For Robots</a></li><li><a href=/blog/using-data-to-balance-your-game-pony/>Using Data To Balance Your Game: Pony Clicker Analysis</a></li><li><a href=/blog/does-anyone-actually-want-good-software/>Does Anyone Actually Want Good Software?</a></li></ul></details></li><li><details><summary>2014</summary><ul><li><a href=/blog/how-not-to-sell-software/>How Not To Install Software</a></li><li><a href=/blog/can-we-choose-what-we-enjoy/>Can We Choose What We Enjoy?</a></li><li><a href=/blog/how-to-make-your-profiler-10x-faster/>How To Make Your Profiler 10x Faster</a></li><li><a href=/blog/the-problem-with-photorealism/>The Problem With Photorealism</a></li></ul></details></li><li><details><summary>2013</summary><ul><li><a href=/blog/googles-decline-really-bugs-me/>Google's Decline Really Bugs Me</a></li><li><a href=/blog/the-educational-imbroglio/>The Educational Imbroglio</a></li><li><a href=/blog/write-less-code/>Write Less Code</a></li><li><a href=/blog/most-people-have-shitty-computers/>Most People Have Shitty Computers</a></li><li><a href=/blog/leap-motion-impressions-input/>Leap Motion Impressions, Input Sanitation, and 3D Gesture Ideas</a></li><li><a href=/blog/aurora-theory-released/>Aurora Theory Released!</a></li><li><a href=/blog/what-i-learned-in-college/>What I Learned In College</a></li><li><a href=/blog/course-notes/>Course Notes</a></li><li><a href=/blog/contact/>Contact</a></li><li><a href=/blog/the-dark-side-of-htmlcss/>The Dark Side of Web Development</a></li><li><a href=/blog/windows-breaks-assert-inside/>Windows Breaks assert() Inside WM_CANCELMODE</a></li><li><a href=/blog/the-productivity-fallacy/>The Productivity Fallacy</a></li></ul></details></li><li><details><summary>2012</summary><ul><li><a href=/blog/c-to-c-tutorial-part-4-operator-overload/>C# to C++ Tutorial - Part 4: Operator Overload</a></li><li><a href=/blog/7-problems-raytracing-doesnt-solve/>7 Problems Raytracing Doesn't Solve</a></li><li><a href=/blog/analyzing-xkcd-click-and-drag/>Analyzing XKCD: Click and Drag</a></li><li><a href=/blog/coordinate-systems-and-cascading/>Coordinate Systems And Cascading Stupidity</a></li><li><a href=/blog/how-joysticks-ruined-my-graphics-engine/>How Joysticks Ruined My Graphics Engine</a></li><li><a href=/blog/multithreading-problems-in-game-design/>Multithreading Problems In Game Design</a></li><li><a href=/blog/stop-following-rules/>Stop Following The Rules</a></li><li><a href=/blog/why-windows-8-does-right-thing-wrong/>Why Windows 8 Does The Right Thing The Wrong Way</a></li><li><a href=/blog/visual-studio-broke-my-computer/>Visual Studio Broke My Computer</a></li><li><a href=/blog/implicit-ui-design/>Implicit UI Design</a></li><li><a href=/blog/linux-mint-12-kde/>Linux Mint 12 KDE</a></li><li><a href=/blog/new-post/>'Programmer' is an Overgeneralization</a></li></ul></details></li><li><details><summary>2011</summary><ul><li><a href=/blog/great-mystery-of-linear-gradient/>The Great Mystery of Linear Gradient Lighting</a></li><li><a href=/blog/signed-integers-considered-stupid-like/>Signed Integers Considered Stupid (Like This Title)</a></li><li><a href=/blog/c-to-c-tutorial-part-3-classes-and/>C# to C++ Tutorial - Part 3: Classes and Structs and Inheritance (OH MY!)</a></li><li><a href=/blog/problem-of-vsync/>The Problem of Vsync</a></li><li><a href=/blog/c-to-c-tutorial-part-2-pointers/>C# to C++ Tutorial - Part 2: Pointers Everywhere!</a></li><li><a href=/blog/c-to-c-tutorial-part-1-basics-of-syntax/>C# to C++ Tutorial - Part 1: Basics of Syntax</a></li><li><a href=/blog/ninth-circle-of-bugs/>The Ninth Circle of Bugs</a></li><li><a href=/blog/investigating-low-level-cpu-performance/>Investigating Low-level CPU Performance</a></li></ul></details></li><li><details><summary>2010</summary><ul><li><a href=/blog/im-failure/>The IM Failure</a></li><li><a href=/blog/album-for-sale-renascent/>Album For Sale! [Renascent]</a></li><li><a href=/blog/wavsaver/>WavSaver</a></li><li><a href=/blog/pixel-perfect-hit-testing/>Pixel Perfect Hit Testing</a></li><li><a href=/blog/8-bit-color-cycling/>8-bit color cycling</a></li><li><a href=/blog/physics-networking/>Physics Networking</a></li><li><a href=/blog/assembly-cas-implementation/>Assembly CAS implementation</a></li><li><a href=/blog/function-pointer-speed/>Function Pointer Speed</a></li><li><a href=/blog/most-bizarre-error-ever/>Most Bizarre Error Ever</a></li></ul></details></li><li><details><summary>2009</summary><ul><li><a href=/blog/physics-oriented-network-interpolation/>Physics-oriented Network Interpolation</a></li></ul></details></li></ol></article></section></main><footer><p><span>Copyright &copy;2025 Erik McClure</span> <a href=https://erikmcclure.com/sitemap.xml>Sitemap</a> | <a href=https://erikmcclure.com/blog/index.xml>RSS Feed</a></p></footer></div><script>"use strict";window.onload=function(){e=document.getElementsByClassName("math");for(var e,t=0,n=e.length;t<n;t++)renderMathInElement(e[t],{delimiters:[{left:"$$",right:"$$",display:!1},{left:"\\[",right:"\\]",display:!0}]})}</script></body></html>