<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=Edge"><meta http-equiv=permissions-policy content="interest-cohort=()"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#2b7bb5"><meta name=copyright content="Copyright (c)2025 Erik McClure"><meta name=keywords content="games,music,code,erik mcclure,erikmcclure,aurora theory,sweetie bot,discord,feathergui,fading light,tinyoal,cloud hop"><meta name=robots content="index,follow"><meta name=googlebot content="index,follow"><meta name=google-site-verification content="Oxb2ia8HjcHLXlvstA8xPpQO3BO_y15Ds2Ia-feq1MQ"><meta name=generator content="Hugo 0.119.0"><link rel=me href=https://equestria.social/@cloudhop><link rel=canonical href=https://erikmcclure.com/categories/blog/><link rel=apple-touch-icon href=https://erikmcclure.com/favicon.ico><link rel="shortcut icon" type=image/x-icon href=https://erikmcclure.com/favicon.ico><link rel=stylesheet href=https://erikmcclure.com/css/main.css><link rel=stylesheet href=https://erikmcclure.com/css/prism.css rel=stylesheet><link rel=stylesheet href=https://erikmcclure.com/css/katex.min.css rel=stylesheet><link rel=stylesheet href=https://erikmcclure.com/css/fontawesome.min.css><link rel=stylesheet href=https://erikmcclure.com/css/regular.min.css><link rel=stylesheet href=https://erikmcclure.com/css/brands.min.css><link rel=stylesheet href=https://erikmcclure.com/css/solid.min.css><link rel=alternate type=application/rss+xml title="Erik McClure - RSS" href=https://erikmcclure.com/blog/index.xml><meta property="og:type" content="article"><title>blog</title><meta property="og:title" content="blog"><meta name=twitter:title content="blog"><meta itemprop=name content="blog"><meta name=description content="Applied mathematician and software architect who occasionally writes music."><meta property="og:description" content="Applied mathematician and software architect who occasionally writes music."><meta name=twitter:description content="Applied mathematician and software architect who occasionally writes music."><meta itemprop=description content="Applied mathematician and software architect who occasionally writes music."><meta property="og:url" content="https://erikmcclure.com/"><meta property="og:site_name" content="Erik McClure"><meta property="og:image" content="https://erikmcclure.com/img/avatar.png"><meta property="og:locale" content="en-US"><meta property="article:author" content="Erik McClure"><meta name=twitter:card content="summary"><meta name=twitter:site content="@erikmcclure0173"><meta name=twitter:creator content="@erikmcclure0173"><meta name=twitter:image content="https://erikmcclure.com/img/avatar.png"><meta name=twitter:dnt content="on"><link href=https://plus.google.com/104896885003230920472 rel=publisher><meta itemprop=image content="https://erikmcclure.com/img/avatar.png"><meta name=last-updated content="20250430-05:35:00.000"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-63026815-3"></script>
<script defer src=https://erikmcclure.com/syntax-prism.js></script>
<script defer src=https://erikmcclure.com/katex.min.js></script>
<script defer src=https://erikmcclure.com/mathtex-script-type.min.js></script>
<script defer src=https://erikmcclure.com/auto-render.min.js></script>
<script defer src=https://cdn.commento.io/js/count.js></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-63026815-3")</script></head><body><div id=container><header><nav><ul><li><a href=/blog/ title=Blog><i class="fa-solid fa-book fa-fw fa-lg"></i>&nbsp;<p>Blog</p></a></li><li><a href=/projects/ title=Projects><i class="fa-solid fa-briefcase fa-fw fa-lg"></i>&nbsp;<p>Projects</p></a></li><li><a href=https://erikmcclure.bandcamp.com title=Bandcamp><i class="fa-brands fa-bandcamp fa-fw fa-lg"></i>&nbsp;<p>Bandcamp</p></a></li><li><a href=https://github.com/erikmcclure title=Github><i class="fa-brands fa-github fa-fw fa-lg"></i>&nbsp;<p>Github</p></a></li><li><a href=/web/ title=Websites><i class="fa-solid fa-globe fa-fw fa-lg"></i>&nbsp;<p>Websites</p></a></li></ul></nav><div class=dim><h1>Erik McClure</h1></div></header><main class=projects><section><div class=dim><aside><h2>Leftists Are In A Purity Death Spiral</h2><ul></ul></aside><article><p>It seems almost impossible to describe this very simple concept to an increasingly large percentage of leftists: If you disagree with someone&rsquo;s opinion on <code>[Political Position A]</code>, but agree with them on <code>[Political Position B]</code>, <em>you can still work with them</em> to make <code>[Political Position B]</code> happen, without compromising your stance on <code>[Political Position A]</code>. This is called forming a <em>political coalition</em>, a temporary alliance to achieve a common goal. Importantly, you must understand that a political party <em>is not</em> and <em>will never be</em> a cohesive collection of people who all agree with each other. It is <em>literally impossible</em> because a political party is so huge and diverse.</p><p>Diversity means a diversity of opinions and takes. Due to the existence of an average person being <a href=https://www.thestar.com/news/insight/when-u-s-air-force-discovered-the-flaw-of-averages/article_e3231734-e5da-5bf5-9496-a34e52d60bd9.html>a fallacy</a>, every single person you know is statistically likely to have at least one really weird or messed up opinion about <em>something</em>. If you are lucky, it&rsquo;s about something you don&rsquo;t care about, but the more purity tests you have, the more lines you draw, the more statistically impossible it becomes for anyone to actually pass them all. This applies for any large group - no matter how &ldquo;cohesive&rdquo; a particular group seems, statistically it must be formed through the alliance of many different smaller subgroups, recursively. This recursion usually continues until you reach a group with a couple hundred people, which is the size of an average human tribe, and the largest socially cohesive unit that is possible. Every larger group is, in actuality, multiple subgroups that have come together, each with slightly different views.</p><div class=imgwrap style=max-width:645px><a href=/img/purity_groups.svg target=_blank><img src=/img/purity_groups.svg alt="A series of circles representing groups" width=100%></a></div><p>Every &ldquo;bad opinion&rdquo; you refuse to engage with is another line in the sand. It cuts you off from potential allies. It shrinks the size of your coalition.</p><div class=imgwrap style=max-width:645px><a href=/img/purity_groups_cut.svg target=_blank><img src=/img/purity_groups_cut.svg alt="A series of circles representing groups with a dotted line cutting through them" width=100%></a></div><p>Eventually, you enter a <a href=https://en.wikipedia.org/wiki/Purity_spiral>purity spiral</a>, where almost no one can satisfy your demand for moral purity. Everyone has made mistakes. Everyone has bad takes on things sometimes. You cannot affect change if your social group has excluded the entire rest of humanity from it:</p><div class=imgwrap style=max-width:645px><a href=/img/purity_groups_none.svg target=_blank><img src=/img/purity_groups_none.svg alt="A series of circles representing groups with a dotted line cutting through them" width=100%></a></div><p>This purity spiral has strangled so many leftist spaces that it has become a well-known problem. I see people complaining about it constantly, in many different places. They&rsquo;re scared and frustrated, because every time anyone has a disagreement over something, it&rsquo;s treated as you being a potential right-wing infiltrator trying to destroy everything, instead of an honest disagreement. This happens because leftists often cannot concieve of someone who is &ldquo;morally good&rdquo; having such an &ldquo;obviously bad take&rdquo;, except they don&rsquo;t consider that <a href=https://www.lesswrong.com/posts/Zp6wG5eQFLGWwcG6j/focus-on-the-places-where-you-feel-shocked-everyone-s>maybe the problem isn&rsquo;t as obvious to everyone else</a>. This happens way more often than you think! Why? Because <em>humans are incredibly diverse!</em> But instead of celebrating this diversity of ideas, the left has cultivated a <a href=https://soatok.blog/2022/03/30/on-the-insecurity-of-social-media-callouts/>callout culture problem</a> that severely punishes any deviance from their idea of Moral Purity, which itself is inconsistent and depends on who stumbled on your old tweets.</p><blockquote><blockquote class=mastodon-embed data-embed-url=https://tech.lgbt/@ninafelwitch/114415826138025679/embed style="background:#fcf8ff;border-radius:8px;border:1px solid #c9c4da;margin:0;max-width:540px;min-width:270px;overflow:hidden;padding:0"><a href=https://tech.lgbt/@ninafelwitch/114415826138025679 target=_blank style="align-items:center;color:#1c1a25;display:flex;flex-direction:column;font-family:system-ui,-apple-system,BlinkMacSystemFont,segoe ui,Oxygen,Ubuntu,Cantarell,fira sans,droid sans,helvetica neue,Roboto,sans-serif;font-size:14px;justify-content:center;letter-spacing:.25px;line-height:20px;padding:24px;text-decoration:none"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 79 75"><path d="M74.7135 16.6043C73.6199 8.54587 66.5351 2.19527 58.1366.964691 56.7196.756754 51.351.0 38.9148.0H38.822C26.3824.0 23.7135.756754 22.2966.964691 14.1319 2.16118 6.67571 7.86752 4.86669 16.0214 3.99657 20.0369 3.90371 24.4888 4.06535 28.5726c.23043 5.8563.27514 11.7024.81165 17.5349C5.24791 49.9817 5.89495 53.8251 6.81328 57.6088c1.7196 6.988 8.68052 12.8034 15.50052 15.176 7.3017 2.4742 15.1542 2.8849 22.6781 1.1862C45.8196 73.7801 46.6381 73.5586 47.4475 73.3063 49.2737 72.7302 51.4164 72.086 52.9915 70.9542 53.0131 70.9384 53.0308 70.9178 53.0433 70.8942 53.0558 70.8706 53.0628 70.8445 53.0637 70.8179V65.1661C53.0634 65.1412 53.0574 65.1167 53.0462 65.0944 53.035 65.0721 53.0189 65.0525 52.9992 65.0371 52.9794 65.0218 52.9564 65.011 52.9318 65.0056 52.9073 65.0002 52.8819 65.0003 52.8574 65.0059 48.0369 66.1472 43.0971 66.7193 38.141 66.7103 29.6118 66.7103 27.3178 62.6981 26.6609 61.0278 26.1329 59.5842 25.7976 58.0784 25.6636 56.5486 25.6622 56.5229 25.667 56.4973 25.6775 56.4738 25.688 56.4502 25.7039 56.4295 25.724 56.4132 25.7441 56.397 25.7678 56.3856 25.7931 56.3801 25.8185 56.3746 25.8448 56.3751 25.8699 56.3816c4.7402 1.1335 9.5994 1.7057 14.4756 1.7044 1.1728.0 2.3421.0 3.5149-.030699999999996C48.7647 57.919 53.9339 57.6701 58.7591 56.7361 58.8794 56.7123 58.9998 56.6918 59.103 56.6611 66.7139 55.2124 73.9569 50.665 74.6929 39.1501 74.7204 38.6967 74.7892 34.4016 74.7892 33.9312 74.7926 32.3325 75.3085 22.5901 74.7135 16.6043zM62.9996 45.3371h-8.003V25.9069c0-4.0906-1.7196-6.1767-5.2173-6.1767-3.845.0-5.771 2.4679-5.771 7.3425V37.7082H36.0534V27.0727c0-4.8746-1.9294-7.3425-5.7744-7.3425-3.4771.0-5.2139 2.0861-5.2173 6.1767V45.3371H17.0656V25.3172c0-4.0906 1.0535-7.3403 3.1606-9.7492 2.1736-2.4032 5.0247-3.6372 8.5636-3.6372 4.0961.0 7.1914 1.5612 9.2549 4.6803l1.9913 3.3134 1.9948-3.3134c2.0635-3.1191 5.1588-4.6803 9.248-4.6803 3.53550000000001.0 6.3866 1.234 8.5671 3.6372 2.107 2.4066 3.1606 5.6563 3.1606 9.7492L62.9996 45.3371z" fill="currentcolor"/></svg><div style=color:#787588;margin-top:16px>Post by @ninafelwitch@tech.lgbt</div><div style=font-weight:500>View on Mastodon</div></a></blockquote><script data-allowed-prefixes=https://tech.lgbt/ async src=https://tech.lgbt/embed.js></script></blockquote><p>This kind of behavior is incredibly counterproductive. It creates a low-trust environment where everyone is looking over their shoulders, where people are constantly worried about associating with someone who did something vaguely questionable five years ago. An environment ruled by fear is not one that engenders cooperation. In fact, it does the opposite, because a social environment where people are constantly terrified of internet hate mobs is the perfect environment for fascism to flourish. <strong>The left did this to itself.</strong> It continues to reject allies that don&rsquo;t adhere to subgroup&rsquo;s specific set of beliefs, which are all mutually incompatible with the others. Nuclear power, scientific research, economic systems, voting systems, guns, crypto, AI, you name it, we have a purity test for it. Leftists think this is keeping their movement &ldquo;pure&rdquo; when in reality it&rsquo;s keeping their movement from actually stopping the fascists.</p><blockquote><blockquote class=mastodon-embed data-embed-url=https://mastodon.social/@contrasocial/114415434392892616/embed style="background:#fcf8ff;border-radius:8px;border:1px solid #c9c4da;margin:0;max-width:540px;min-width:270px;overflow:hidden;padding:0"><a href=https://mastodon.social/@contrasocial/114415434392892616 target=_blank style="align-items:center;color:#1c1a25;display:flex;flex-direction:column;font-family:system-ui,-apple-system,BlinkMacSystemFont,segoe ui,Oxygen,Ubuntu,Cantarell,fira sans,droid sans,helvetica neue,Roboto,sans-serif;font-size:14px;justify-content:center;letter-spacing:.25px;line-height:20px;padding:24px;text-decoration:none"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="32" height="32" viewBox="0 0 79 75"><path d="M63 45.3v-20c0-4.1-1-7.3-3.2-9.7-2.1-2.4-5-3.7-8.5-3.7-4.1.0-7.2 1.6-9.3 4.7l-2 3.3-2-3.3c-2-3.1-5.1-4.7-9.2-4.7-3.5.0-6.4 1.3-8.6 3.7-2.1 2.4-3.1 5.6-3.1 9.7v20h8V25.9c0-4.1 1.7-6.2 5.2-6.2 3.8.0 5.8 2.5 5.8 7.4V37.7H44V27.1c0-4.9 1.9-7.4 5.8-7.4 3.5.0 5.2 2.1 5.2 6.2V45.3h8zM74.7 16.6c.6 6 .1 15.7.1 17.3.0.5-.1 4.8-.1 5.3-.7 11.5-8 16-15.6 17.5-.1.0-.2.0-.3.0-4.9 1-10 1.2-14.9 1.4-1.2.0-2.4.0-3.6.0-4.8.0-9.7-.6-14.4-1.7h-.1s-.1.0-.1.0.0.1.0.1.0.0.0.0c.1 1.6.4 3.1 1 4.5.6 1.7 2.9 5.7 11.4 5.7 5 0 9.9-.6 14.8-1.7h.1v.1c.1.0.1.0.1.1v5.6s0 .1-.1.1v.1c-1.6 1.1-3.7 1.7-5.6 2.3-.8.3-1.6.5-2.4.7-7.5 1.7-15.4 1.3-22.7-1.2-6.8-2.4-13.8-8.2-15.5-15.2-.9-3.8-1.6-7.6-1.9-11.5-.6-5.8-.6-11.7-.8-17.5C3.9 24.5 4 20 4.9 16 6.7 7.9 14.1 2.2 22.3 1c1.4-.2 4.1-1 16.5-1h.1C51.4.0 56.7.8 58.1 1c8.4 1.2 15.5 7.5 16.6 15.6z" fill="currentcolor"/></svg><div style=color:#787588;margin-top:16px>Post by @contrasocial@mastodon.social</div><div style=font-weight:500>View on Mastodon</div></a></blockquote><script data-allowed-prefixes=https://mastodon.social/ async src=https://mastodon.social/embed.js></script></blockquote><p>Refusing to work with anyone else who doesn&rsquo;t satisfy your particular moral purity test isn&rsquo;t &ldquo;standing for something&rdquo;, it simply means you are doing the fascists work for them. An old poem comes to mind:</p><blockquote>First, they came for the cryptobros, and I did not speak out—<br>  Because cryptocurrencies are evil, and the world is better off without them.<br><br>Then, they came for the ai artists, and I did not speak out—<br>  Because ai slop is evil, and the world is better off without those that debase art.<br><br>Then, they came for the gun enthusiasts, and I did not speak out—<br>  Because we needed better gun control anyway, we're better off without them.<br><br>Then, they came for me—<br>  and there was no one left to stop them.</blockquote><p>Let&rsquo;s go through some common objections:</p><blockquote>"that's not fair, the original poem wasn't about people who were evil! You've used evil people, as if they would help me!"</blockquote><p>Yes, <strong>that&rsquo;s the fucking point.</strong> They will, in fact, help you, in the right context, under the right circumstances. Refusing that help <em>is suicide</em>.</p><blockquote>"I don't care if it's suicide! Unlike you, I'm willing to die for what I believe in!"</blockquote><p><strong>Then go ahead and die</strong>. You can take your moral purity with you, because the fascists will shoot you all the same. Purity tests are just a convenient way for you to <a href=https://www.damemagazine.com/2025/04/29/why-i-am-leaving-the-usa/>sabotage any effective resistance we could have mobilized against fascists</a>, and once they&rsquo;ve killed everyone who disagrees with them, the only people left on the planet will be racist psychopaths, and your moral purity will have succeeded in creating a worse future for everyone. You will sway nobody, because you worked with nobody. You vilified every other potential ally, and so they will simply let you die, and you&rsquo;ll take your morals to the grave.</p><p>What&rsquo;s frustrating about this particular claim is that it is usually a complete fabrication. Almost <em>nobody</em> is actually this dumb. If the fascists start hunting down gay people and a cryptobro offers to smuggle you into another country to save your life, you&rsquo;re gonna accept the help even if you hate cryptocurrencies, because you don&rsquo;t wanna die. The thing is, even if it requires a life-threatening situation to force some people to begrudgingly accept help from those they don&rsquo;t like, nothing ever fundamentally changed - <em>the cryptobro never hated gay people in the first place!</em> They would have been willing to help you <em><strong>the entire time</strong></em>, but you were too obstinate to accept the help until you had a gun pointed at your head!</p><blockquote>"If that's the case, then humanity deserves to go extinct"</blockquote><p>If this is your honest belief, you either need therapy or you&rsquo;re some kind of hardcore transhumanist (in which case, <em><strong>𝓯𝓲𝓷𝓮, 𝓘 𝓰𝓾𝓮𝓼𝓼</strong></em>). Either way, leave the rest of us alone while we try to actually fix things instead of participate in a doomer death cult. The conservatives have enough of those already.</p><p>Now, if anyone is still here, let&rsquo;s walk through the steps required to build a coalition that enshrines trans rights and ends the Gaza genocide, followed by forming a new coalition that bans generative AI, without compromising any morals in the process. First, we must recognize that active genocide and stripping human rights are higher priority than most other issues we care about. This requires internalizing that, while we <em>can</em> find many allies willing to help us end the genocide in Palestine, many of them will have some pretty shitty opinions on things! You&rsquo;ll have to put up with:</p><ul><li>People who like cryptocurrencies</li><li>People who like capitalism</li><li>People who don&rsquo;t like <em>[your preferred economic system]</em></li><li>People who like the military</li><li>Yes, people who like AI too.</li><li>Even people who disagree with you about <em>[that other thing you really care about]</em></li></ul><p>All of these groups are potential allies. You must internalize that you are only working with these groups to achieve <em>one particular result</em>, and that is where your loyalty ends. You are not chaining yourself to cryptobros, or ai artists, or gun nuts, or libertarians. You are only recognizing that, despite the fact that these people have some pretty terrible beliefs sometimes, we all agree that <strong>genocide is bad</strong> and <em>stripping trans people of human rights</em> is also bad.</p><p>Now, the alliance with people who like the military requires <em>using nuance</em>. Yes, I know twitter has apparently make it impossible for people to use nuance, but you need to understand that &ldquo;people who like the military&rdquo; is an enormous section of the populace, so there are going to be a lot of subgroups within it. The people who simply believe the strong should rule the weak are the fascists, which are the ones you can <em>never</em> work with. The people who believe that violence should be used to defend your values and never against civilians, on the other hand, will likely be strongly against any kind of genocide.</p><p>Use this fact to drive a wedge between the two subgroups, separating out the ones that support you while causing in-fighting that weakens the ones against your position. Once you&rsquo;ve separated the two subgroups, it won&rsquo;t be hard to show that the current attacks on trans people is preparing for a genocide as well, which will make it much easier to convince your new allies to also oppose the current attacks on trans groups, even if they didn&rsquo;t previously care that much. By maximizing the number of people you get on your side (the side of &ldquo;we shouldn&rsquo;t let Israel murder innocent people&rdquo; and &ldquo;the attacks on trans rights is a precursor to genocide&rdquo;) you can finally become a real, viable threat to the democrats, who don&rsquo;t seem to have any actual values anymore, so I can&rsquo;t actually list them.</p><div class=imgwrap style=max-width:645px><a href=/img/coalition.svg target=_blank><img src=/img/coalition.svg alt="A complex venn diagram of the various subgroups the coalition is made of" width=100%></a></div><p>Now that we&rsquo;ve replaced the current democrats in office, we build a coalition for banning generative AI, leveraging our previous work. We&rsquo;ll start with the cryptobros. Yes, I know you hate the cryptobros, and sometimes it&rsquo;s for a good reason. Now it&rsquo;s time to use one of those reasons, by driving another wedge between subgroups by identifying the ones who support generative AI versus the ones who don&rsquo;t care one way or the other. This is easier if you can express exactly why you object to generative AI - even if you have many reasons, picking one (like it&rsquo;s impact on artistic livelihoods and worker rights in general) gives you a sharper cognitive knife to work with, metaphorically speaking. Knowing exactly what you want makes you less vulnerable to ideas a charismatic person says are good that don&rsquo;t actually further your goals.</p><p>Then you need to go to the AI people. Your goal here is not to throw the entire group under the bus, but to once again leverage nuance to drive apart individual subgroups. When a previously cohesive group realizes it doesn&rsquo;t actually agree about everything, the group as a whole is greatly weakened. There are many subgroups within AI that only care about AI that has actual research value, like folding proteins or identifying breast cancer or detecting blood cancers. These groups would be happy to support targeted legislation that bans generative AI, like LLMs and image generators, especially if you focus on a specific harmful aspect, like AI being used for misinformation (many AI researchers legitimately want to help society, not harm it). By acquiring allies from some of the AI supporters, without attacking the entire concept of AI as a whole, you&rsquo;ve shattered their group coherence and greatly weakened the proponents of generative AI.</p><p>The other groups likely have a random smattering of support or opposition to generative AI. Pulling in ones already against it will be easy, but the majority of the groups likely don&rsquo;t care - your job is to pull them to your side, and the best way to do this is by trying to find something they care about that is negatively impacted by AI, like their jobs. Remember that the opposing groups will also be recruiting people to their side, so it is crucial you find a <em>sharp reason</em>, a specific thing that aligns with something that subgroup <em>does</em> care about. It is going to be rare that you can make anyone else care about every single issue you care about, because <a href=http://erikmcclure.com/blog/people-cant-care-about-everything/>people can&rsquo;t care about everything</a>, but if you successfully accomplished something with them before, they might find it prudent to listen to you instead of the other side.</p><p>In some cases, if there isn&rsquo;t an obvious shared value, you may need to offer them something, like joining a different coalition to address one of their key issues. This still doesn&rsquo;t require you to sacrifice any of your morals - you simply need to find an issue that you both agree on, like universal healthcare or implementing UBI or treating veterans better. In exchange for you working with them in the future on one of those issues, they might be willing to side with you on an issue they essentially have no opinion on. It is not necessary to convince everyone in the entire world to share your exact political opinions, only for them to <em>agree to help you</em>.</p><p>Furthermore, after you manage to ban generative AI and you start working on passing UBI, <em>you can go right back to the generative AI supporters</em>. All you need to do is point out that it will be much easier for them to unban generative AI if some form of UBI is passed, and they&rsquo;ll be willing to help you pass UBI <em>even though you previously worked against them</em>. Despite your past differences, it is still in everyone&rsquo;s best interest to work together, and no one has to compromise on their morals. You can still oppose generative AI even after it&rsquo;s proponents help you pass UBI. Nobody needs to compromise on their morals because fundamentally opposed goals <em>can still share some values</em>. It is crucial to recognize when someone you disagree with shares your values in another area of society, and work with them to further that specific value.</p><p>This is largely how any functioning political system works. However, leftist circles keep getting hijacked by moral puritans who insist that even <em>working</em> with anyone who has ever done anything slightly bad will somehow &ldquo;corrupt&rdquo; the movement and everyone in it will magically turn into <del>witches</del> democrats. This isn&rsquo;t really possible, because the democrats don&rsquo;t actually do anything right now, and even worse, it could be a propaganda tactic. The FBI deliberately used similar tactics against the civil rights movement in the 1960s: they would <a href="https://books.google.com/books?id=zBgDPupR_lMC&amp;pg=PA184#v=onepage&amp;q&amp;f=false">send inflammatory anonymous letters</a> falsely accusing an african-american organization of misusing funds. These were literally the period equivalent of our modern social media callout posts, except now callout posts can come from astroturfed accounts that seem like real people. All these moral puritans insisting that we shouldn&rsquo;t &ldquo;compromise our morals&rdquo; by cooperating with other people might just be Russian agents, or real people manipulated by Russian agents.</p><p>Regardless, whether the moral purity panics that repeatedly consume leftist circles are real or astroturfed by Russian propaganda, they cannot be allowed to continue. If leftists want a snowball&rsquo;s chance in hell of actually stopping the fascists, they must learn how to cooperate with their fellow human beings instead of demanding moral purity that simply serves to destroy their own movement.</p></article></div></section><section><div class=dim><aside><h2>The New Discord Overlay Breaks GSync and Borderless Optimizations</h2><ul></ul></aside><article><p>The new discord overlay no longer uses DLL injection, and is instead a permanent <code>HWND_TOPMOST</code> window glued to whatever window it happens to think is a game. Ignoring the fact that discord seems to think FL Studio, the minecraft launcher, and SteamVR&rsquo;s desktop widget are &ldquo;video games&rdquo;, the real problem is that <strong>this breaks the Borderless Windowed Optimizations</strong>, which has the most obvious effect of disabling GSync/FreeSync on all games that the overlay enables itself on.</p><blockquote class=bluesky-embed data-bluesky-uri=at://did:plc:hdemcqizsy4u42x5ydxga34w/app.bsky.feed.post/3lljyfbtlgs2a data-bluesky-cid=bafyreiehr7atlqaf7qbv4ztdzkhql2b4o7obc5nqptfwohxyuphqujyw6i data-bluesky-embed-color-mode=system><p lang=en>so it seems it also yoinks gsync, which means the game running underneath doesn't use gsync anymore. Wonderful!</p>&mdash; Tawmy (<a href="https://bsky.app/profile/did:plc:hdemcqizsy4u42x5ydxga34w?ref_src=embed">@tawmy.dev</a>) <a href="https://bsky.app/profile/did:plc:hdemcqizsy4u42x5ydxga34w/post/3lljyfbtlgs2a?ref_src=embed">March 29, 2025 at 11:45 AM</a></blockquote><script async src=https://embed.bsky.app/static/embed.js></script><p>We can tell that it&rsquo;s a normal window instead of DLL injection by simply finding the window in the win32 UI tree using <a href=https://learn.microsoft.com/en-us/windows/win32/winauto/inspect-objects>inspect.exe</a>:</p><a href=/img/discord-inspect.png target=_blank><img src=/img/discord-inspect.png alt="Discord Overlay Window"></a><p>Interestingly, they still seem to be using a D3D window to render the overlay. This might be a quirk of using Electron, or it might be a result of whatever library they&rsquo;re using to render the overlay:</p><a href=/img/discord-inspect-2.png target=_blank><img src=/img/discord-inspect-2.png alt="Intermediate D3D sibling window?"></a><p>The reason this breaks everything is because the borderless windowed optimization works using <a href=https://learn.microsoft.com/en-us/windows/win32/direct3ddxgi/for-best-performance--use-dxgi-flip-model>a new flip model</a> called the <a href=https://learn.microsoft.com/en-us/windows/win32/direct3ddxgi/dxgi-flip-model>DXGI flip model</a>. Instead of copying the contents of the backbuffer to another intermediate buffer used by the desktop for compositing, the compositor can use the backbuffer directly when it is compositing. This flip model was augmented in Windows 10 with <a href="https://www.youtube.com/watch?v=E3wTajGZOsA">Direct Flip</a>, which allows this shared surface to <strong>bypass the compositor entirely</strong> and send frames to the monitor directly:</p><blockquote>Depending on window and buffer configuration, it is possible to bypass desktop composition entirely and directly send application frames to the screen, in the same way that exclusive fullscreen does.</blockquote><p>All modern gaming is built on top of this key optimization, because it allows seamless Alt-Tab behavior by allowing the DWM compositor to &ldquo;wake up&rdquo; and start compositing the screen like a normal application, then &ldquo;go to sleep&rdquo; once it knows a single borderless fullscreen application is the only thing rendering to that monitor, by simply piping it&rsquo;s backbuffer directly to the device. If a combination of <code>DXGI_FEATURE_PRESENT_ALLOW_TEARING</code> and the right VSync mode is enabled, the app can update it&rsquo;s backbuffer completely out-of-band from the rest of the desktop compositor, which is the only thing that allows GSync/FreeSync to work, as the monitor must sync it&rsquo;s own refresh rate to whenever the game happens to complete a frame.</p><p>If any part of this pipeline is disrupted, it is no longer possible to forward frames to the monitor outside the normal update sequence of the compositor. <em>Many</em> things can break this, like <a href=https://support.microsoft.com/en-us/windows/optimizations-for-windowed-games-in-windows-11-3f006843-2c7e-4ed0-9a5e-f9389e535952>not turning on optimizations for windowed games</a> or having windows fail to recognize something as a game. If the vsync mode is set up wrong, it will break. If the flip mode is wrong, it will break. And most importantly, if even <em>a single pixel of another app</em> is displayed over the game, then in order to display that pixel, the compositor has to composite the window outputs together onto a secondary buffer, which must then be presented at the native refresh rate of the monitor because it has inputs from two different programs at different refresh rates, thus breaking GSync/FreeSync. It will also introduce additional frames of lag even if you don&rsquo;t use GSync/FreeSync, which you may have noticed when a notification pops up while playing a game and it suddenly felt laggy until the notification went away.</p><p>DLL Injection was originally used for in-game overlays because games often used exclusive fullscreen. The drawback of DLL injection is that it crashes games when implemented incorrectly and also makes virus scanners very unhappy. With the new flip models, games don&rsquo;t need to ask for exclusive fullscreen to get low latency, but <strong>they still have to be the only thing on the screen or it doesn&rsquo;t work</strong>. Discord has either ignored why DLL injection was originally used, or decided that the drawbacks of DLL injection aren&rsquo;t worth it and instead simply broken all the optimizations for windowed games that Microsoft introduced. Any half-decent graphics programmer would know this would happen, so it&rsquo;s obvious that one of two things happened:</p><ol><li>Discord never involved a single graphics dev or gamedev with any experience in how games work about how their new overlay <em>for games</em> would interact <em>with games</em>.</li><li>There is a very angry dev stalking the halls of discord HQ right now, cursing at the shadows because she knew. SHE KNEW. <em>SHE WARNED THEM</em>. <strong>BUT THEY DIDN&rsquo;T LISTEN</strong>. Her manager probably ignored her warnings, or overruled them, saying &ldquo;most gamers won&rsquo;t even notice&rdquo; or &ldquo;DLL Injection has too many problems&rdquo;. And now, if she shares this article with said manager, her manager will look bad and probably try to fire her for the crime of being competent, because that&rsquo;s how big corporations work.</li></ol><p>Usually I default to option (2), but option (1) is also possible if they already laid off the person who knew this would happen <a href=https://www.theverge.com/2024/1/11/24034705/discord-layoffs-17-percent-employees>last year</a>. But hey, if you are a graphics dev at discord who tried to warn your managers about this trashfire and you got &ldquo;laid off&rdquo; under mysterious circumstances, send me a DM on bluesky, I&rsquo;m always interested in talking to actual competent engineers.</p><p>If the new overlay was turned on without your consent (which is what happened to me), you can turn it off again by going to <strong>User Settings → Activity Settings → Game Overlay → Enable Overlay</strong>. Be careful though, because flipping this option off has crashed the video drivers for two of my friends so far, requiring a full reboot. If you want to uninstall Discord, you can do so from Add/Remove programs, but good luck finding another chat app your friends actually want to use.</p></article></div></section><section><div class=dim><aside><h2>Do You Really Think We'll Have Genders In The Future?</h2><ul></ul></aside><article><p>Something that is very common is for people pushing the boundaries of technology (or pretending to, anyway) to hold weirdly conservative social views. This is why &ldquo;techbro&rdquo; is now a thing, and it&rsquo;s not really surprising - there are plenty of engineers that are only good at engineering, not participating in a society, or even understanding how human social interactions work. It is kinda weird when <a href=https://paulgraham.com/woke.html>Paul Graham does it</a>, though.</p><p>That said, when I see futurists or transhumanists talk about a timeline where humans are uploaded or become cyborgs or whatever, and then they turn around and say stuff like &ldquo;<a href=https://x.com/RokoMijic/status/1847300707252052085>feminism is bad</a>&rdquo;, I am utterly baffled. This level of cognitive dissonance is a bit hard to swallow, even in our current political quagmire of fake news and conspiracy theories. These futurists are talking about augmenting human minds beyond our current capabilities, of modifying our bodies to do things we could only dream of, and <em>[checks notes]</em> also transgender people are mentally ill. Wait, what?</p><p>The moment we get access to any kind of augmentation, we&rsquo;re not just going to make a perfect human. Nobody even agrees on what the &ldquo;perfect body&rdquo; is in the first place, and people&rsquo;s response to disagreement seems to be to assert that their subjective opinion is the objectively correct one. For most people, I can excuse this as a lack of imagination. Maybe their response is a genuine question: &ldquo;well, what else would you do? How can a perfect body be anything other than human?&rdquo;. But we&rsquo;re talking about <em>futurists</em> here, they&rsquo;re literally imagining entire new technological paradigms! They definitely have an imagination!</p><p>Oftentimes, as so <a href="https://www.youtube.com/watch?v=QVilpxowsUQ">eloquently explained by Philosophy Tube</a>, such viewpoints exist only to shield ourselves from truths we would rather not think about. Just as we construct phantasms to avoid thinking about our own mortality, some people don&rsquo;t want to confront the possibility that they aren&rsquo;t actually comfortable in their own body, because there is very little they can do about it right now. If they convince themselves that, given futuristic technology they would simply give themselves a perfect human body and then they would finally be happy with how they look, they don&rsquo;t have to confront the lingering doubts eating away at the back of their mind - <em>What if I&rsquo;m still not happy?</em> <em>What if I actually want to be a different gender?</em> <em>What if I&rsquo;m actually gay?</em> <em>What if I don&rsquo;t even want a human form?</em>. Even if a technological solution currently exists, many people trap themselves in social structures that would destroy them if they expressed themselves.</p><p>One way to salvage this worldview - to rationalize the phantasm - is to argue that, despite all the <em>potential</em> chaos that technology will unleash, only those civilizations who <a href=https://x.com/RokoMijic/status/1770045723188383975>manage to hold on to Heterosexual Western Values</a> will be successful in the long-term, usually backed up with a remarkably bad understanding of statistics. This has several problems, the first being that it has zero historical precedent - successful civilizations are always whoever can socially adapt to new technological paradigms. The other problem is that it ignores who is going to be first in line to get themselves augmented. Some rich people will, sure, but who do you think is going to be most willing to undergo an extremely risky procedure to give themselves a new limb?</p><p>Any guesses?</p><p>It&rsquo;s furries. Furries want tails. Normal people cannot understand how badly furries want tails. They will literally invent new technologies just to give themselves tails. The first novel limbs that aren&rsquo;t replacements are going to be tails invented by furries, and the furries will get them in droves. The first adopters of mechanical augmentation, cyborgs, and mind uploading will be furries, transgender people, and anyone else who doesn&rsquo;t feel comfortable in their own body. Anyone who has not trapped their dysmorphia in a phantasm to try to escape it. This is going to give them a massive first-mover advantage, which will be incredibly difficult to catch up to because the compounding benefits of augmentation create exponential benefits.</p><p>It should be obvious to anyone who has seen technological trends play out that the furries won&rsquo;t stop at <em>just</em> tails. People will immediately begin augmenting themselves in increasingly exotic ways, bounded only by technological limitations. <a href=https://www.youtube.com/@isaacarthurSFIA>Good futurists</a> already talk about not just transhuman futures, but <a href="https://www.youtube.com/watch?v=aQvIk4ySQ1U"><em>posthuman</em> futures</a>, where fragments of humanity inevitably transform into something barely recognizable. This is a fairly common aspect in almost any science fiction talking about <a href="https://www.youtube.com/watch?v=o48X3_XQ9to">what the deep future might look like</a>, because it is logically inescapable. The only constant in nature is change, and so there is simply no possible way that humans would remain static for thousands of years in a civilization that continues to innovate.</p><p>All of this gets even more ridiculous if mind-uploading happens soon, because&mldr; you&rsquo;ll be in a computer. You could be literally anything. You don&rsquo;t even need a physical form anymore. Once mind-uploading happens, do you really think we&rsquo;ll even have a notion of gender anymore? Of sexual orientation? Archaic notions might survive, but whatever &ldquo;genders&rdquo; people choose to be once all physical limitations are removed will be utterly incomprehensible to us. They&rsquo;ll have much weirder things to debate, like whether or not it&rsquo;s okay for an uploaded human to have a relationship with an AI with an IQ of 1040. &ldquo;Genderfluid&rdquo; isn&rsquo;t going to scratch the surface of all the weird shit people will get up to.</p><p>Perhaps now you might understand why I am so utterly baffled by bigoted futurists, who would not survive in their own predicted futures. They seem to have constructed some kind of phantasm out of their contradictory beliefs, although what frightening truths that phantasm is protecting them against, I can&rsquo;t say. I wonder if upcoming advances in VR might be more important than we think - perhaps better and more immersive VR could provide people with a safe place to explore alternative physical forms. Maybe then, some people might start to look past the phantasm.</p></article></div></section><section><div class=dim><aside><h2>Stop Making Me Memorize The Borrow Checker</h2><ul></ul></aside><article><p>I started learning Rust about 3 or 4 years ago. I am now knee-deep in several very complex Rust projects that keep slamming into the limitations of the Rust compiler. One of the most common and obnoxious problems is hitting a situation the borrow-checker can&rsquo;t deal with and realizing that I need to completely re-architect how my program works, because lifetimes are &ldquo;contagious&rdquo; the same way async is. Naturally, Rust has both!</p><p>Despite how obviously useful the borrow-checker is in writing correct code, in practice it is horrendous to work with. This is because the borrow checker cannot run until an entire function compiles. Sometimes it seems to refuse to run until my entire file compiles. Because an explicit lifetime must come from somewhere, they have a habit of &ldquo;floating up&rdquo; through the stack, from the point of usage to the point of origin, infecting everything in-between with another explicit generic lifetime parameter. If you end up not needing it, you need to go through and delete every instance of this lifetime, which can sometimes be 30 or more generic statements that end up needing to be modified.</p><p>In the worst cases, your entire architecture simply cannot work with the borrow checker, and at minimum you&rsquo;ll need to wrap things in an Rc&lt;>, which again will requiring upwards of 30 or more statements depending on the complexity of your architecture. Other times you realize you need a split borrow, and have to then modify <em>every single function under the split borrow check</em> to take specific field references instead of the original type. These constant refactors have been a major detractor for the language for years, although some improvements, like <code>impl</code>, have reduced the need for refactoring in some narrow cases.</p><p>This means, to be a highly productive Rust programmer, you basically have to memorize the borrow checker rules, so you get it right the first time. This is stupid, because <em>the whole point</em> of having a type system or a borrow checker is to tell you when you get it wrong, so you <em>don&rsquo;t</em> have to memorize how the borrow rules work. I don&rsquo;t need to memorize how all the types work, because these errors get caught almost immediately, and rarely require massive refactors because the whole architecture doesn&rsquo;t need to exist before it can identify problems.</p><p>This is painful because I am an experienced C++ programmer, and C++ has this exact problem except worse: undefined behavior. In the worst case, C++ simply doesn&rsquo;t check anything, compiles your code wrong, and then does inexplicable and impossible things at runtime for no discernable reason (or it just deletes your entire function). If you run <code>ubsan</code> (undefined behavior sanitizer), it will at least explode at runtime with an error message. Unfortunately, it can only catch undefined behavior that actually happens, so if your test suite doesn&rsquo;t cover all your code branches you might have undefined behavior lurking in the code somewhere. Even worse, the very existence of undefined behavior sometimes creates a new branch you couldn&rsquo;t possibly think of testing without knowing about the undefined behavior in the first place!</p><p>This means that in order to write C++, you effectively have to memorize the undefined behavior rules, which sucks. Sound familiar? This is both stupid and strictly worse than Rust, because there is no compile-time error at all, only a runtime error if you get it wrong (and you are running <code>ubsan</code>). However, because it&rsquo;s a runtime error, correcting it usually requires less total refactoring&mldr; usually.</p><p>At this point, C++ can&rsquo;t fix it&rsquo;s undefined behavior problem because C++ uses undefined behavior to drive optimization, so now it&rsquo;s just stuck like this forever. Rust can&rsquo;t really fix borrow checking either, because borrow checking is embedded so deeply into the compiler at this point. All Rust can do is make the borrow checker more powerful (probably by introducing partial borrows, which seems stuck in eternal bikeshedding hell) or introduce more powerful IDE tooling that can make refactors less painful and more automatic, like automatically removing a generic parameter from everywhere it was used.</p><p>Problems like these are unfortunate, because it drives people towards using C for it&rsquo;s &ldquo;simplicity&rdquo;, when in reality they are simply deferring logic errors until runtime. I think Rust manages to &ldquo;get away&rdquo; with it&rsquo;s excessive verbosity because &ldquo;safe C++&rdquo; is even more horrendously verbose and arcane, and safe C++ is what Rust is really competing against right now. I just think Rust needs more competition.</p><p>Any prospective Rust competitor, however, needs to be very cognizant of the tradeoffs they force programmers to make in exchange for correctness. It is not sufficient to invent a language that makes it possible to write provably correct kernel-level code, it has to be <em>easy to use</em> as well, and we really need to get away from indirectly forcing programmers to anticipate what the compiler will do simply to be productive. It&rsquo;s not the 1970s anymore, writing a program shouldn&rsquo;t feel like taking a stack of punchcards to the mainframe to see if it works or not. Rust is not the answer, it is simply a step towards the answer.</p></article></div></section><section><div class=dim><aside><h2>Rust Async Makes Me Want To Gouge My Eyes Out</h2><ul></ul></aside><article><p>One of the most fundamental problems with Rust is the design of <code>Result</code>. It is a lightweight, standardized error return value, similar to C-style error codes but implemented at a type system level that can contain arbitrary information. They are easy to use and very useful, and the ecosystem encourages you to use them over <code>panic!</code> whenever possible. Unfortunately, this ends up creating a problem. <code>Result</code> is not like a C++ exception because it doesn&rsquo;t contain a stacktrace by default, nor does the compiler have any idea where it was first constructed, unless the error type it contains decides to include that information upon construction by using <a href=https://doc.rust-lang.org/std/backtrace/index.html>backtrace</a>.</p><p><a href=https://doc.rust-lang.org/std/panic/fn.catch_unwind.html>You can catch</a> an unwinding <code>panic!</code>, which is implemented much more like a C++ exception, but a <code>panic!</code> that aborts cannot be caught and it&rsquo;s impossible to tell if a panic will unwind at compile-time because <a href=https://doc.rust-lang.org/std/panic/fn.always_abort.html>the behavior can be changed at runtime</a>. Another interesting difference is that the panic handler is invoked before the panic starts unwinding the stack. These implementation details push you towards using the panic handlers purely as uncaught exception handlers, when you can do nothing but perhaps save critical information and dump a stacktrace before exiting. As a result, <code>panic!</code> is designed for and used almost exclusively for unrecoverable errors.</p><p>If you want an error that is recoverable, you use <code>Result</code>&mldr; which doesn&rsquo;t have a backtrace, unless you and all your dependencies are using some variant of <code>anyhow</code> (or it&rsquo;s various forks), which allows you to <a href=https://stackoverflow.com/questions/42275777/how-to-trace-the-cause-of-an-error-result>add backtrace information</a>. If the error is coming from a dependency that doesn&rsquo;t use anyhow, you&rsquo;re screwed. There actually is <a href=https://github.com/rust-lang/rust/issues/53487>an RFC to fix this</a>, but it&rsquo;s been open for six years and shows no signs of being merged anytime soon.</p><p>&ldquo;But&rdquo;, I hear you ask, &ldquo;what does this have to do with Rust Async?&rdquo; Well, like most things, Rust Async makes this annoying part of Rust <em>twice as annoying</em> because the default behavior is to <em>silently eat the error and drop the future</em>, unless you have stored the join handle somewhere, <em>and</em> you are in a position where you can access that join handle to find out what the actual error was. The API for making tokio panic when an unhandled panic happens is <a href=https://github.com/tokio-rs/tokio/issues/4516>still unstable</a>, with the interesting comment of &ldquo;dropping errors silently is definitely the correct default behavior&rdquo;. Really? In <em><strong>debug mode?</strong></em> In release mode, fine, that&rsquo;s reasonable, but if I&rsquo;ve compiled my program <em>in debug mode</em> I&rsquo;m pretty sure I want to know if random errors are being thrown. Even with this API change, you&rsquo;ll have to manually opt-in to it, they won&rsquo;t helpfully default to this behavior when you compile in debug mode.</p><p>Until that feature gets stabilized, you basically have to throw all your <code>JoinHandle</code>&rsquo;s into a <code>JoinSet</code> blender so you can tell when something errored out, and unless you are <em>extremely sure</em> you didn&rsquo;t accidentally drop any <code>JoinHandle</code>&rsquo;s on the floor (because Rust <em>does not warn you if you do this</em>), you probably need a timeout function even after your main future has returned, in case there are zombie tasks that are still deadlocked.</p><p>Oh, have I mentioned deadlocks? Because that&rsquo;s what Rust async gives you instead of errors. Did you forget to await something? Deadlock. Did you await things in the wrong order? Deadlock. Did you forget to store the join handle and an error happened? Deadlock. Did you call a syncronous function that invokes the async runtime 5 layers deep in the callstack because it doesn&rsquo;t know it&rsquo;s already inside an async call and you forgot it tried to do that? Deadlock. Did you implement a poll() function incorrectly? Deadlock.</p><iframe width=560 height=315 src="https://www.youtube.com/embed/IPFiKEm-oNI?si=yW7sV5uo25Uu37o7" title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><p>For simple deadlocks, something like <a href=https://github.com/tokio-rs/console>tokio-console</a> <em><strong>might</strong></em> be able to tell you something useful (&ldquo;might&rdquo; is doing a lot of work here). However, any time you forget to await something, or don&rsquo;t call join on the localset, or add things to the wrong localset, or your poll function isn&rsquo;t returning the right value, or the async waker was called incorrectly, you&rsquo;re just screwed, because there is no list of &ldquo;pending futures that have not been awaited yet&rdquo; that you can look through, unless you get saved by your IDE noticing you didn&rsquo;t await the future, which it often doesn&rsquo;t. It definitely doesn&rsquo;t tell you about accidentally dropping a <code>JoinHandle</code>, which is one of the most common issues.</p><p>But why would you have to implement a poll function? That&rsquo;s reserved for advanced users&ndash; Nope, nope, actually you have to do that when implementing literally any <code>AsyncRead</code>/<code>AsyncWrite</code> trait. Oh, sorry, there&rsquo;s actually<a href=https://github.com/nrc/portable-interoperable/issues/5> 4 different possible <code>AsyncRead</code>/<code>AsyncWrite</code> implementations</a> and they&rsquo;re all slightly different and completely incompatible with each other, but they&rsquo;re all equally easy to fuck up. Everything in Rust Async is absurdly easy to fuck up, and your reward is always the same: <code>[your-program].exe has been running for over 60 seconds</code>.</p><p>I haven&rsquo;t even mentioned how the tokio and futures runtimes are almost the same but have subtle differences between them, and tokio relies on aspects of futures that have been factored out into <code>future-util</code>, which should be in the standard library but isn&rsquo;t because the literal only thing they actually standardized on was <code>std::future</code> itself. All this is ignoring the usual complaints about async function color-coding - I&rsquo;m complaining about obnoxious implementation footguns <em>on top of</em> all the usual annoyances involved with poll-based async. Trying to use async is like trying to use a hammer made out of hundreds of tiny footguns hot-glued together.</p><p>I wish async was just one cursed corner of Rust that had its warts relatively self-contained, but that isn&rsquo;t the case. Rust async is a microcosm of an endless stream of basic usability problems that the language simply doesn&rsquo;t fix, and might not ever fix. I&rsquo;m honestly not sure how they&rsquo;re going to fix the <a href=https://doc.rust-lang.org/nomicon/borrow-splitting.html>split-borrow problem</a> because the type system isn&rsquo;t powerful enough to encode where a particular borrow came from, which is required to implement spatially disjoint borrows, which ends up creating an endless cascade of <a href=https://users.rust-lang.org/t/split-borrow-of-struct-in-a-refcell/41115>subtle complications</a>.</p><p>For example, there are quite a few cases where serde_json errors <a href=https://github.com/serde-rs/serde/issues/2711>are not very helpful</a>. None of these situations would matter if you could open a debugger and go straight to what was throwing the error, but you can&rsquo;t because this is Rust and serde_json doesn&rsquo;t implement <code>anyhow</code> so you can&rsquo;t inject any errors. <a href=https://github.com/AlexanderThaller/format_serde_error>format_serde_error</a> was created to solve this exact problem, but it is no longer maintained and <a href=https://github.com/AlexanderThaller/format_serde_error/issues/20>is buggy</a>. Also, <a href=https://github.com/rust-lang/cargo/issues/9096>artifact dependencies still aren&rsquo;t stabilized</a>, despite the very obvious use-case of needing to test inter-process communication that comes up in basically any process management framework? So <a href=https://crates.io/crates/test-binary>this crazy hack</a> exists instead.</p><p>Rust&rsquo;s ecosystem <em>heavily</em> relies on two undebuggable-by-default constructions: macros and async, which makes actually learning how to debug production Rust code about as fun as pulling your own teeth out. I have legitimately had an easier time hunting down <em>memory corruption errors in C++</em> then trying to figure out where a particular error is being thrown when it is hidden inside a macro inside an error with no stacktrace information, because C++ has mature tooling for hunting down various kinds of memory errors.</p><p>Because of <a href=https://soasis.org/posts/statement-on-rustconf-compile-time-introspection/>last year&rsquo;s shenanigans</a>, I am no longer confident that any of these problems will be fixed anymore. Rust&rsquo;s development has slowed to a crawl, and it seems like it&rsquo;ll take years to stabilize features like <a href=https://internals.rust-lang.org/t/variadic-generics-design-sketch/18974/3>variadic generics</a>, which are currently still in the design phase despite <a href=https://equestria.social/@cloudhop/113080310647278940>all the problems</a> the ecosystem runs into without them. It is extremely frustrating to see comments saying &ldquo;<a href=https://www.reddit.com/r/rust/comments/etsnh8/comment/fflqhqe/>oh the ecosystem is just immature</a>&rdquo; when those comments are <em>5 years old</em>. On the other hand, I am tired of clueless C or C++ fans trying to throw bricks at Rust over these kinds of problems when C++ has <a href="https://learn.microsoft.com/en-us/cpp/build/reference/zc-cplusplus?view=msvc-170">far</a> <a href=https://stackoverflow.com/questions/10613126/what-are-the-differences-between-std-c11-and-std-gnu11>worse</a> <a href=https://stackoverflow.com/questions/17789928/whats-a-proper-way-of-type-punning-a-float-to-an-int-and-vice-versa>sins</a>. Because of this, I will continue building all future projects in Rust, at least until the dependently typed language I&rsquo;m working on has a real compiler instead of a half-broken interpreter.</p><p>Because hey, <a href=https://thephd.dev/finally-embed-in-c23>at least it isn&rsquo;t C</a>.</p></article></div></section><section><div class=dim><aside><h2>Engineers Only Get Paid If Something Is Broken</h2><ul></ul></aside><article><blockquote class=twitter-tweet data-media-max-width=560><p lang=en dir=ltr>*shaking voice*<br><br>"so, here's the thing — you're not gonna force all of us to learn rust"<br><br>the way this is said... you'd think they were being deported or something. incredibly childish people at the helm there.<a href=https://t.co/fcgA6m9fvq>https://t.co/fcgA6m9fvq</a></p>&mdash; fasterthanlime (@fasterthanlime) <a href="https://twitter.com/fasterthanlime/status/1829574164854030843?ref_src=twsrc%5Etfw">August 30, 2024</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>Recently, Rust people have been getting frustrated with C developers who seem to base their entire personal identity on being able to do unsafe memory things. The thing is, this is not a phenomenon unique to C developers, or even software developers in general, although the visibility of the kernel C developers is higher than the average programmer. Most people will be upset about technology that makes their particular skillset irrelevent. This is best summarized by a famous quote:</p><blockquote>"It is difficult to get a man to understand something, when his salary depends upon his not understanding it" — <a href=https://quoteinvestigator.com/2017/11/30/salary/>Upton Sinclair</a></blockquote><p>Professional C developers who dislike Rust&rsquo;s borrow checker are almost always extremely good at doing manual memory management while making very few detectable errors (I say detectable because they usually miss some edge-case that turns into a security nightmare 10 years later). They are either paid lots of money to do this specific task, or they have almost no experience doing anything else. They are one-trick ponies, and they are terrified of their one skill being made irrelevent, at which point they will either no longer make lots of money, or not have a job at all.</p><p>You could argue that they could simply <em>learn Rust</em>, but you must understand that they believe they are <em>singularly talented at C</em>, which in some cases may actually be true. If they start learning Rust now, they might end up just being an <em>average</em> Rust developer, and the thought of being <em>average</em> is <strong>absolutely terrifying to them</strong>. This is because it&rsquo;s percieved to be a <em>loss of social status</em>, which human brains are hardwired to avoid at all costs. It sounds like they&rsquo;re about to be &ldquo;deported&rdquo; because that is the exact psychological response that potentially losing social status provokes.</p><p>It&rsquo;s not just languages, either. When programmers ask &ldquo;why is this ecosystem such a disaster&rdquo;, half the time it&rsquo;s because somebody is getting paid to deal with it. Our industry is trapped in an endless loop of a startup building a new technology on top of some half-broken ecosystem, exploding in popularity, and then everyone using the startup&rsquo;s technology hires people to deal with the ecosystem it&rsquo;s built on top of, and those people don&rsquo;t actually want anyone to fix it or they&rsquo;ll be out of a job. There is no escaping the fact that, if someone was getting paid to deal with something that was broken, and you fix it, you just made them irrelevent.</p><p>In 30 years, when Rust is slowly being replaced by something better, Rust developers will behave the <em>exact</em> same way. Someone will invent a borrow checker that is much more powerful and capable of solving most of the annoying borrow situations that baffle the current Rust borrow checker. Their response will be that this language is for &ldquo;lazy&rdquo; programmers, who don&rsquo;t want to be as <em>precise</em> as a Real Rust Programmer. They&rsquo;ll complain about things that don&rsquo;t make any sense because they&rsquo;ve never actually used the language they&rsquo;re complaining about. The Rust programmers will sound just as dumb as C programmers do today.</p><p>I know this will happen because this already happens in literally every other field in existence. Musicians <em>still</em> sometimes claim that if you can&rsquo;t play an actual instrument you aren&rsquo;t a &ldquo;real&rdquo; musician, whatever that is. There was a big fight when Photoshop came out because artists complained that &ldquo;ctrl-Z&rdquo; was cheating and if you can&rsquo;t paint on a real canvas you aren&rsquo;t a Real Artist. It&rsquo;s everywhere, and it&rsquo;s universal.</p><p>This is not a programmer problem, it&rsquo;s a people problem. When you look at this through the lens of livelihoods being threatened, you can instantly see that this is all the exact same instinctual human reaction: they have a high status because they are incredibly skilled at a particular thing, and New Thing is threatening to make that skill either irrelevent, or less important, and they don&rsquo;t want to <em>lose status</em>.</p><p>The best defense against this behavior seems to be skill generalization and good self-esteem. If you are confident in your abilities as a musician, you don&rsquo;t need to worry about people who are good at using a sequencer, instead you might <a href="https://www.youtube.com/watch?v=aHjpOzsQ9YI">try to team up with them</a>. If you are confident in your general problem solving abilities as a programmer, then the language barely matters, what matters is which language is best suited for the problem at hand.</p><p>Software engineering in particular seems to suffer from hyper-specialization, with people having jobs working with extremely specific frameworks, like React, or Kubernetes, or whatever the newest Javascript framework is. It might be that the complexity of our problems are outstripping our tool&rsquo;s abstractions, but regardless of what the cause is, if we don&rsquo;t get things under control soon, this will just keep getting worse.</p></article></div></section><section><div class=dim><aside><h2>Measuring Competence Is Epistemic Hell</h2><ul></ul></aside><article><p><a href=https://en.wikipedia.org/wiki/Sturgeon%27s_law>Sturgeon&rsquo;s law</a> states that 90% of everything is crap. Combined with <a href=https://en.wikipedia.org/wiki/Hanlon%27s_razor>Hanlon&rsquo;s Razor</a>, we arrive at the inescapable conclusion that most problems are caused by incompetence. What&rsquo;s particularly interesting is that the number of incompetent people in a system tends to increase the higher up you go. Part of this is due to the <a href=https://en.wikipedia.org/wiki/Peter_principle>Peter Principle</a>, where organizations promote employees until they become incompetent, but this happens in the first place because it becomes <em>harder to measure competence&rsquo;</em> the longer it takes the effects of actions to be felt, and as a species we have no way of measuring long-term incompetence. Instead, we rely on social cues, and tend to use whatever our local culture determines is &ldquo;competent&rdquo;.</p><p>One way to try to address this is to teach better critical thinking, but this almost always runs into fierce objections from parents who don&rsquo;t want schools to &ldquo;undermine parental authority&rdquo;, which is what happened with <a href=https://www.washingtonpost.com/blogs/answer-sheet/post/texas-gop-rejects-critical-thinking-skills-really/2012/07/08/gJQAHNpFXW_blog.html>the 2012 Republican Party of Texas platform</a> (<a href=https://web.archive.org/web/20120630113751/http://s3.amazonaws.com/texasgop_pre/assets/original/2012-Platform-Final.pdf>original</a>). This kind of thinking is actually fairly common, and it is not a fluke of human nature - it is a <em>feature</em>.</p><p>To understand why humans can be inquisitive and intelligent on an individual level, but follow arbitrary and sometimes counterproductive rituals on a cultural level, you must understand that <a href=https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/>our ancestors lived in epistemic hell</a>. My favorite example is the tribe that had a very long and complicated ritual for preparing manioc, which contained dangerous amounts of cyanide:</p><blockquote>In the Americas, where manioc was first domesticated, societies who have relied on bitter varieties for thousands of years show no evidence of chronic cyanide poisoning. In the Colombian Amazon, for example, indigenous Tukanoans use a multistep, multiday processing technique that involves scraping, grating, and finally washing the roots in order to separate the fiber, starch, and liquid. Once separated, the liquid is boiled into a beverage, but the fiber and starch must then sit for two more days, when they can then be baked and eaten.<br><br>[..] even if the processing was ineffective, such that cases of goiter (swollen necks) or neurological problems were common, it would still be hard to recognize the link between these chronic health issues and eating manioc. Low cyanogenic varieties are typically boiled, but boiling alone is insufficient to prevent the chronic conditions for bitter varieties. Boiling does, however, remove or reduce the bitter taste and prevent the acute symptoms.<br><br>So, if one did the common-sense thing and just boiled the high-cyanogenic manioc, everything would seem fine. [..] Consider what might result if a self-reliant Tukanoan mother decided to drop any seemingly unnecessary steps from the processing of her bitter manioc. She might critically examine the procedure handed down to her from earlier generations and conclude that the goal of the procedure is to remove the bitter taste. She might then experiment with alternative procedures by dropping some of the more labor-intensive or time-consuming steps. She’d find that with a shorter and much less labor-intensive process, she could remove the bitter taste. Adopting this easier protocol, she would have more time for other activities, like caring for her children. Of course, years or decades later her family would begin to develop the symptoms of chronic cyanide poisoning.<br><br>Thus, the unwillingness of this mother to take on faith the practices handed down to her from earlier generations would result in sickness and early death for members of her family. Individual learning does not pay here, and intuitions are misleading. &mdash; <a href="https://www.amazon.com/Secret-Our-Success-Evolution-Domesticating-ebook/dp/B00WY4OXAS/ref=as_li_ss_tl?keywords=the+secret+of+our+success&qid=1559607052&s=gateway&sr=8-1&linkCode=ll1&tag=slatestarcode-20&linkId=761afd67f6541a6de5cebbd0127aa910&language=en_US">"The Secret Of Our Success" by Joseph Henrich</a></blockquote><p>Without modern tools, there is no possible way (other than <em>acquiring brain damage from chronic cyanide poisoning</em>), for an ancient human to realize that every step of the ritual is actually necessary, because without extensive experimentation over many human lifetimes, it isn&rsquo;t obvious what danger the ritual is guarding against, and if it&rsquo;s working as intended, no one will have seen the danger or be able to know about it in the first place! It seems that evolution always kept around enough sacrificial intelligent humans to tinker with new possible rituals, but always ensured that the majority of the population would obey the safe, known ways of doing things, <em><strong>without questioning them</strong></em>, because trying to rationally evaluate an opaque ritual meant death. <em>Not even the culture itself</em> knew what disaster or point of failure the ritual was actually preventing, only that it kept them alive. Religion is simply a convenient way of packaging rituals; if you look in the rules set out by many ancient religions, a lot of them start looking like &ldquo;how to run a functioning society&rdquo; and include things like <a href=https://en.wikipedia.org/wiki/Toilet_god>&ldquo;keep your toilet clean&rdquo;</a>. They got popular because they worked, we just had <em>no idea why</em> and in many cases <em>couldn&rsquo;t have possibly figured out why</em> with the technology at the time. Even worse, if you got it wrong, it could take you decades until you finally manifested an affliction that actually started causing problems.</p><p>This is the core evolutionary drive behind religion and conservative mindsets, where obeying authority is paramount to survival. In modern times, we could communicate to our children why doing a particular thing is bad, because we know the entire chain of cause and effect. Just a few hundred years ago, we couldn&rsquo;t even do that! A famous example is the effort to <a href=https://www.lrb.co.uk/the-paper/v45/n23/jonah-goodman/a-national-evil>get iodine added to salt</a>. Doctors didn&rsquo;t resist the idea of adding iodine to salt for no reason, they resisted it because at every dosage amount that seemed like it could have an effect, it made people sick! They had experiments on fish that showed that iodine seemed to make goiters go away, but giving people iodine supplements would always make them sick. At this point in time, nobody had <em>any evidence whatsoever</em> that micronutrients existed. Giving people just 150 micrograms of iodine a day, accomplished by evenly mixing tiny grains of potassium iodide into a kilogram of salt, seemed like homeopathic medicine. There was no known substance that had any effect at that little concentration. Only by taking a leap of faith could Otto Bayard theorize that perhaps we needed just a tiny amount of iodine, going against all known nutritional science at the time.</p><p>Humans likely evolved culture as an alternative to animal&rsquo;s reliance on old pack members to know what to do in case an extremely rare but devastating event happened every hundred-ish years. Rituals could seem completely nonsensical inside a single human lifespan, because they addressed problems at a societal level that only happened every 200 years, or slow acting chronic issues. In one case, <a href=https://global.wcs.org/Wildlife/Global-Priority-Species/African-Elephants/African-Savannah-Elephant.aspx>ancient elephant matriarchs</a> were the only ones capable of remembering waterholes far enough from a drought that only happened once every 35 years. The packs that lost their matriarchs all died because they had lost this knowledge.</p><p>We evolved logic to solve problems that had clear first-order effects, but we aren&rsquo;t very good at evaluating <a href=https://www.quora.com/What-are-second-order-effects-consequences-and-some-examples-of-them>second-order effects</a>. Long lived humans were capable of finding cause and effect links that happened over a human lifespan, but only human culture perpetuating strange and bizarre rituals created out of random experimentation could deal with problems that had very long, unknowable cause and effect chains. It is very hard to tell if the person building your house is competent if the house only collapses every 150 years when a massive earthquake hits. Various cultures have developed all sorts of indirect methods of measuring competence, but many of them emphasize students obeying their teachers, because the teachers are often perpetuating rituals that are critically important without actually understanding why the rituals are important or what they guard against. It is culture guarding against <a href=https://en.wiktionary.org/wiki/Chesterton%27s_fence>Chesterton&rsquo;s fence</a> over enormous timespans. Another good example of epistemic hell is how we cured scurvy by accident <a href=https://www.secretorum.life/p/epistemic-hell>and then ruined the cure</a>:</p><blockquote>Originally, the Royal Navy was given lemon juice, which works well because it contains a lot of vitamin C. But at some point between 1799 and 1870, someone switched out lemons for limes, which contain a lot less vitamin C. Worse, the lime juice was pumped through copper tubing as part of its processing, which destroyed the little vitamin C that it had to begin with.<br><br>This ended up being fine, because ships were so much faster at this point that no one had time to develop scurvy. So everything was all right until 1875, when a British arctic expedition set out on an attempt to reach the North Pole. They had plenty of lime juice and thought they were prepared — but they all got scurvy. The same thing happened a few more times on other polar voyages, and this was enough to convince everyone that citrus juice doesn’t cure scurvy.</blockquote><p>Our ancestors weren&rsquo;t stupid. They were trying to find some kind of logical progression of cause-and-effect, but they lived in epistemic hell. This is why cargo-cult programming exists. This is why urban legends persist. This is why parents simply want their children to do as they say. This is why we have youtubers chastising NASA for <a href="https://yt.drgnz.club/watch?v=OoJsPvmFixU">not reading their own Apollo 11 postmortem</a>. This is why corporate procedures emphasize checking boxes instead of critically examining the problem. When your cause-and-effect chain is a hundred steps long and caused by something 5 years ago, economic pressure incentivizes simply trying to avoid blame instead of trying to find the actual systemic problem. The farther up the chain of management a problem is, the longer it takes for the effects to be felt, and the worse we get at finding the root cause. Software engineering has the same issue, where incompetence may only cause performance issues years later, after the original coder has left, and the system has scaled up beyond a critical breaking point. This is <a href=https://erikmcclure.com/blog/factorio-is-best-interview-we-have/>why we still don&rsquo;t know how to hire programmers</a>.</p><p>Only in the modern era do we have the necessary technological progress <em>and the historical records</em> to be able to accurately evaluate the usefulness of our rituals. Only now can <a href=https://www.nature.com/articles/nature.2017.21573>we watch chemical reactions happen</a> at an atomic level. Only now can we have <a href=https://en.wikipedia.org/wiki/Just_culture>Just Culture</a> and <a href=https://www.etsy.com/codeascraft/blameless-postmortems>blameless post-mortems</a> that allows identifying actual systemic failures. Only now can I watch a YouTube video explaining how to go from <a href="https://www.youtube.com/watch?v=MXs_vkc8hpY">a quantum simulation of particle collisions to a dynamical fluid simulation</a>. Only now can I watch a slow-motion capture at 200000 frames per second to see exactly how a tiny filament explodes into hot globules that then fly into a nest of zirconium filings and set it aflame exactly where each one lands.</p><iframe width=560 height=315 src="https://www.youtube.com/embed/AEm-2giH_zw?si=PSbrydqNDeZ_9vqE&amp;clip=Ugkxa3VMb5AgdZ7g1NvWSHb4Aqm_L837ZgSk&amp;clipt=ELWDTBjSsE0" title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><p>The engineers who invented these flashbulbs couldn&rsquo;t see any of this. They had to infer it from experimentation, whereas I can just <em>watch it happen</em> and immediately understand exactly what is going on. We live in a pivotal moment of human history, on the cusp of being able to truly understand <em>the entire chain</em> of cause-and-effect for why things happen. We have the ability to measure events with unprecedented accuracy, to tease out tiny differences that catalyze huge reactions.</p><p>Unfortunately, the ability to merely see cause-and-effect is not sufficient when large systems tend to be <a href=https://en.wikipedia.org/wiki/Chaos_theory>chaotic</a>. We do not yet have good mathematical frameworks for predicting emergent behavior, and our ability to analyze complex chaotic systems is still in its infancy. We know that large groups of humans consistently display emergent behavior, such as crowd dynamics closely following the equations of fluid dynamics. Likewise, large human organizations are themselves largely emergent behavior, and we never really understood how they were working in the first place. Organizational competence, and <a href=https://erikmcclure.com/blog/we-could-fix-everything-we-just-dont/>coordination problems in general</a> are our modern epistemic hell, and it means there is no easy way for us to address the failure of our institutions, because we still have no holistic way to analyze the effectiveness of a given organization.</p><p>We are tantalizingly close to grasping the true nature of reality, to having the machinations of the universe laid out before us, but we are still missing the tools to fully analyze subtle patterns, to lift a whispered signal out of the thundering noise of spacetime. There is simply no escape from emergent behaviors evolving out of chaotic systems. Until we have the means to analyze these kinds of complex systems, we will forever be at odds with our nature, still tempted to cling on to superstitions of old, because long ago, that was the only thing that kept us alive.</p></article></div></section><section><div class=dim><aside><h2>We Could Fix Everything, We Just Don't</h2><ul></ul></aside><article><blockquote class=twitter-tweet><p lang=en dir=ltr>[programmers frantically pulling cables out of the wall]<br><br>AI: "Nuclear power. Double teachers' salaries. Build more houses. Distribute food more fairly. TRAINS—"</p>&mdash; qntmyrrh (@qntm) <a href="https://twitter.com/qntm/status/1728178227355763063?ref_src=twsrc%5Etfw">November 24, 2023</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>I remember growing up with that same old adage of how you could be the next scientist to invent a cure for cancer, or a solution to climate change, or whatever. What they don&rsquo;t tell you is that we already have solutions for a <strong>lot</strong> of problems, we just don&rsquo;t use them. Sometimes this is because the solution is too expensive, but usually it&rsquo;s because competing interests create a tragedy of the commons. Most problems in the modern age aren&rsquo;t complicated engineering problems, they&rsquo;re the same problem: coordination failure.</p><p>It was recently unveiled that basically every single UEFI SecureBoot implementation ever made <a href=https://binarly.io/posts/finding_logofail_the_dangers_of_image_parsing_during_system_boot/index.html>can be bypassed with a <em><strong>malicious image file</strong></em></a>. This means that any manufacturer that allows the user to customize the boot image is now vulnerable to a complete bypass of SecureBoot <em>and</em> Intel Boot Guard. Luckily, the fix for this is pretty simple: don&rsquo;t make the logo customizable. But how did something this absurd happen in the first place?</p><blockquote>The results from our fuzzing and subsequent bug triaging unequivocally say that none of these image parsers were ever tested by IBVs or OEMs. We can confidently say this because we found crashes in almost every parser we tested. Moreover, the fuzzer was able to find the first crashes after running just for a few seconds and, even worse, certain parsers were crashing on valid images found on the Internet. &mdash; <a href=https://binarly.io/posts/finding_logofail_the_dangers_of_image_parsing_during_system_boot/index.html>binarly.io</a></blockquote><p>It&rsquo;s pretty obvious what happened, actually. The image parsers were written with the assumption they&rsquo;d only ever need to load an image file provided by the manufacturer. When this assumption was violated, all hell broke loose, because <a href=https://davidkcaudill.medium.com/maybe-getting-rid-of-your-qa-team-was-bad-actually-52c408bd048b>we don&rsquo;t test software anymore</a>. None of this happened because engineering is hard. None of this happened because of some tricky, subtle bug. It happened because the people writing the image parsers made an incredibly stupid mistake and then didn&rsquo;t bother testing it, because the software industry doesn&rsquo;t bother with QA anymore. Thus, there was no <a href=https://en.wikipedia.org/wiki/Swiss_cheese_model>swiss cheese</a>. There was just one slice of cheese with a gaping hole in it, because it turns out that some manufacturers decided to let users customize their boot image, thinking it would be harmless, and that by itself was enough to wreak havoc.</p><p>Every layer of this problem is a different flavor of coordination failure. No one on the team who implemented this either thought that there might need to be a warning about untrusted images, or whoever did bring it up was ignored because it was supposed to be handled by another team. Except whoever was supposed to put in a warning about this either wasn&rsquo;t told, or buried it inside a technical document nobody ever reads. The vendors who decided to implement user-customizable boot logos didn&rsquo;t ask whether this would be a problem, or weren&rsquo;t told about it.</p><p>And <em>nobody</em>, not a single layer in this clown train, <a href=https://davidkcaudill.medium.com/maybe-getting-rid-of-your-qa-team-was-bad-actually-52c408bd048b#5a65>implemented a proper QA or pentesting process</a> that could have caught this bug, because we just don&rsquo;t bother testing anything anymore. Our economic incentives have somehow managed to incentivize building the worst possible piece of shit that still technically works. We know how to avoid this situation. We have decades of experience building in-depth QA processes that we are simply ignoring. <strong>We could fix this, we just don&rsquo;t</strong>.</p><p>This is not exclusive to software, as this <a href="https://www.youtube.com/watch?v=Limpr1L8Pss">fantastic video about the popcorn button</a> explains. Our economic race to the bottom has been sabotaging almost every aspect of engineering in our society. To save a few cents per microwave, the cheap microwaves don&rsquo;t include a humidity sensor and then lie about having a popcorn button when it can&rsquo;t actually work properly, which leads to everyone saying &ldquo;don&rsquo;t use the popcorn button&rdquo; and now nobody uses the popcorn buttons <em>even on microwaves that actually have a humidity sensor and a working popcorn button</em>. The cheapskates control the supply chain now. They have pissed in the proverbial pool, and if this sounds familiar, that&rsquo;s because it&rsquo;s a classic example of the <a href=https://en.wikipedia.org/wiki/Tragedy_of_the_commons>Tragedy of the Commons</a>.</p><p>Except, that&rsquo;s not an excuse. What&rsquo;s truly absurd is that the tragedy of the commons <em>isn&rsquo;t inevitable</em>. We <em>know</em> this because ancient human tribes managed to navigate responsible utilization of common resources <a href=https://en.wikipedia.org/wiki/Tragedy_of_the_commons#Non-governmental_solution>all the time</a>. It has no historical basis whatsoever. The tragedy of the commons only happens when you have a total failure of collective action. It is the original symptom of societal enshittification.</p><blockquote>[...] many nomadic pastoralist societies of Africa and the Middle East in fact "balanced local stocking ratios against seasonal rangeland conditions in ways that were ecologically sound", reflecting a desire for lower risk rather than higher profit...</blockquote><p>We actually have a cure for blood cancer now, by the way. Like, <a href=https://www.pennmedicine.org/news/news-releases/2023/august/an-immunotherapy-strategy-against-all-blood-cancers>we&rsquo;ve done it</a>. It&rsquo;s likely that a similar form of immunotherapy will generalize to most forms of cancer. Unfortunately, the only approved gene therapy we have is for sickle-cell disease and <a href=https://www.nature.com/articles/d41586-023-03590-6>costs $2 million per patient</a>, so most people in America simply assume they will never be able to afford any of these treatments, even if they were dying of cancer, because insurance will never cover it. This is actually really bad, because if nobody can afford the treatment, then biotech companies won&rsquo;t bother investing into it, because it&rsquo;s not profitable! We have built a society that <em>can&rsquo;t properly incentivize</em> <em><strong>CURING CANCER</strong></em>. This is despite the fact that socialized healthcare is a proven effective strategy (as long as the government <a href=https://www.independent.co.uk/voices/nhs-crisis-covid-privatisation-funding-b2255741.html>doesn&rsquo;t sabotage it</a>). <strong>We could fix this, we just don&rsquo;t</strong>.</p><p>Some people try to complain that this happens because democracy is hard, or whatever, and they&rsquo;re also wrong. <em>We know exactly what&rsquo;s wrong with our current voting systems</em> and CGP Grey even put out a video on it <a href="https://www.youtube.com/watch?v=s7tWHJfhiyo">13 fucking years ago</a>. It <em>inevitably</em> results in a two-party system, because strategic voting is rational behavior, and you can&rsquo;t break out of this two-party system because of the spoiler effect, and the solution is <a href="https://www.youtube.com/watch?v=3Y3jE3B8HsE">Ranked Choice Voting</a> (or the Alternative Vote). If you want to go further and address gerrymandering you can use the <a href="https://www.youtube.com/watch?v=l8XOZJkozfI">Single Transferable Vote</a>. All of these better systems were proposed decades ago. We have implemented exactly none of them for the presidential election (<a href=https://en.wikipedia.org/wiki/Ranked-choice_voting_in_the_United_States>except for Maine and Alaska</a>). In fact, America still uses the electoral vote system, which is strictly worse than the popular vote, we all know it&rsquo;s worse, and we even <a href="https://www.youtube.com/watch?v=tUX-frlNBJY">have a potential solution</a> but we still can&rsquo;t get rid of it due to counterproductive societal interests.</p><p>We <em>HAVE</em> solutions for these problems. We just don&rsquo;t use them. We <em>could</em> be running fiber-optic cable to every house in America, and we even know how much it would cost. We just don&rsquo;t because we gave the money to corporations who then <a href=https://www.reddit.com/r/explainlikeimfive/comments/6c5e97/comment/dhsxq6k/>used none of it and instead paid themselves huge bonuses</a>. We <em>know</em> that automation is chipping away at low-skill jobs, which means our workforce needs to be better educated, and that providing free college to everyone would be a good idea, we just don&rsquo;t. We know how to build interstate high-speed commuter rail, we just don&rsquo;t (although <a href=https://www.whitehouse.gov/briefing-room/statements-releases/2023/12/08/fact-sheet-president-biden-announces-billions-to-deliver-world-class-high-speed-rail-and-launch-new-passenger-rail-corridors-across-the-country/>Biden is trying</a>). <strong>We could fix everything, we just don&rsquo;t</strong>.</p><p>We have no excuses anymore. None of these are novel or difficult problems, not even the tragedy of the commons. We can do better. We don&rsquo;t need AI to fix things. We don&rsquo;t need new technology to solve these problems. We <em>already know how to do better</em>, we&rsquo;ve just dug ourselves into a cooperation slump that&rsquo;s so bad we can&rsquo;t even implement solutions <em>we already know about</em>, let alone invent new ones. We&rsquo;re in this hole simply because society is run by people who are incentivized to sabotage cooperation in the name of profits. That&rsquo;s it.</p><p>It&rsquo;s January 1st of the new year, and with all these people wishing each other a &ldquo;better year&rdquo;, I am here to remind you that <strong>it will only get worse</strong> unless we <em>do something</em>. Society getting worse is <a href="https://www.youtube.com/watch?v=q118B_QdP2k">not something you are hallucinating</a>. It cannot be fixed by you biking to work, or winning the lottery. We are running on the fumes of our wild technological progress of the past 100 years, and our inability to build social systems that can cooperate will destroy civilization as we know it, unless we <strong>do something about it</strong>.</p><p>We live in what is perhaps the most critical turning point in all of human history, and we&rsquo;re on a ship that has drifted far off course. The rapid current of technology means that we are swept along faster and faster, making it exponentially harder to steer away from the icebergs ahead of us. We <em><strong>must</strong></em> address our coordination failures. We <em><strong>must</strong></em> build systems that foster better cooperation, or this century won&rsquo;t be a turning point for humanity, it will be the end of humanity.</p><blockquote>"All that would remain of us would be a thin layer in some future rock face. This is the future we must avoid at all costs." &mdash; <a href="https://www.youtube.com/watch?v=o48X3_XQ9to&t=725s">John D. Boswell (melodysheep)</a></blockquote></article></div></section><section><div class=dim><aside><h2>People Can't Care About Everything</h2><ul></ul></aside><article><div class=imgwrap style=max-width:1172px><a href=/img/linux-tweet.png target=_blank><img src=/img/linux-tweet.png alt="Sorry, I need my computer to work" width=100%></a></div><p>I originally posted an even more snarky response to this, but later deleted it when I realized they were just a teenager. Kids do not have decades of experience with buggy drivers, infuriating edge-cases, and broken promises necessary to understand and contribute to the underlying debate here (nor do they have the social context to know that Xe and I were just joking with each other). Of course, they also don&rsquo;t know that it&rsquo;s generally considered poor taste to interject like this, as it tends to annoy everyone and <em>almost always</em> fails to take into consideration the greater context in which someone might be using Windows, or Mac, or TikTok, or Twitter, or whatever corporate hellscape they are trapped in. The thing is, <em>there&rsquo;s always a reason</em>. You might not <em>like</em> the reason, but there is usually a reason someone has locked themselves inside the Apple ecosystem, or subjected themselves to Twitter, or tried to eke a living from beneath the lovecraftian whims of YouTube&rsquo;s recommendation algorithm.</p><p><strong>People can only care about so much.</strong></p><p>They can&rsquo;t care about everything. You might think something is important, and it probably is, but&mldr; so is everything else. Everything matters to someone, and everything is important to <em>society in general</em> to some degree. Some people think that YouTube isn&rsquo;t very important, but they&rsquo;re objectively wrong, as YouTube creators reach <em>billions of people</em>. They <em>change people&rsquo;s lives</em> on a daily basis. We could argue about how important art and music and creativity is to society, yet observe that our capitalist hellhole treats creatives as little more than wage slaves, but then we&rsquo;d be here all day.</p><p>As this blog post bemoaning the loss of Bandcamp explains, <a href=https://www.welcometohellworld.com/they-can-and-will-ruin-everything-you-love/>They Can And Will Ruin Everything You Love</a>. The only thing that is important to the money vultures is&mldr; money. The only people who can build another Bandcamp are people who believe it&rsquo;s important. I particularly care about the Bandcamp debacle because one of my hobbies is writing music, and I prefer selling it <a href=https://erikmcclure.bandcamp.com/>on Bandcamp</a>. If Bandcamp dies, I will no longer have anywhere to offer downloadable lossless versions of my songs. Everything has devolved into shitty streaming services, and there&rsquo;s nothing I can do about it. I&rsquo;m too busy fixing everything else that&rsquo;s broken, there&rsquo;s no time for me to build a Bandcamp alternative and I&rsquo;m terrible at web development anyway. Don&rsquo;t get me started on whether the new solution should be FOSS, because some people believe FOSS is important, and they&rsquo;d be right! Just look at Cory Doctorow&rsquo;s <a href="https://www.youtube.com/watch?v=q118B_QdP2k">talk about enshittification</a> and how proprietary platforms are squeezing the life out of us.</p><p><em><strong>Everything is important!!!</strong></em></p><p>&mldr;But I can&rsquo;t care about everything. You can&rsquo;t care about everything either, you have to pick your battles. No, that&rsquo;s too many battles, put some back. That&rsquo;s still too many battles. You only have 16 waking hours every day to do anything. You have to <em>pick something</em>, and everything you care about has a cost. When everything is important, nothing happens. No websites are created. No projects are built. No progress is made. We simply sit around, bikeshedding over whose pet issue is the most important. There are always trade-offs, and sometimes you can make the wrong ones:</p><blockquote class=twitter-tweet><p lang=en dir=ltr>If I had a dollar every time I saw an "open source" project vaguely recreate a handful of surface-level features of some commercial solution then go "there you have it folks, YOU'RE FREE" while Nothing Actually Works, I would be able to, like, pay rent with it.</p>&mdash; fasterthanlime (@fasterthanlime) <a href="https://twitter.com/fasterthanlime/status/1714980045960159341?ref_src=twsrc%5Etfw">October 19, 2023</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><p>As the <a href=https://fasterthanli.me/articles/just-paying-figma-15-dollars>corresponding blog post</a> later elaborates on, when you are 19 / still a student / unemployed, time is all you have to spend. It can be easy to forget how valuable time is to some people. Even if I won&rsquo;t touch Apple devices with a 10-foot-pole, I can understand why people use them. If all your use cases fall inside Apple&rsquo;s supported list of behaviors, it can be great to have devices that just work (assuming you can afford them, of course). On the other hand, while I prefer Windows, I know many people who use Linux because Windows either won&rsquo;t let them do what they want, or literally just doesn&rsquo;t even work. They are willing to put in the time and effort to make their linux machines work just the way they want, and to maintain them, and occasionally do batshit insane source-code patches that I hopefully will never have to do in my life, because it&rsquo;s important to them.</p><p>Back when I was still writing fiction, I got a great comment from an editor who said something along the lines of &ldquo;writing should be fun, you should only pursue perfection as far as you enjoy.&rdquo; You can spend your entire life chasing perfection, but you&rsquo;ll never reach it, and at some point you have to ship something. I&rsquo;ve been trying to finish up some songs for an album recently and I&rsquo;ve had to rely on formulaic crutches more than I want to, because at the end of the day, it&rsquo;s just a hobby, and I simply don&rsquo;t have the time to be as experimental as I want. My choice is to either release an okay song, or none at all. You can tell where I was hopelessly chasing an unattainable goal for over two years when <a href=https://erikmcclure.newgrounds.com/audio/>my output completely stops</a>:</p><div class=imgwrap style=max-width:449px><a href=/img/song-output.png target=_blank><img src=/img/song-output.png alt="Song Output" width=100%></a></div><p>Everyone has to make trade-offs, and it can take time to figure out which ones are right for you. Not everyone can contribute to your particular social cause. When you ask someone to care about something, you are implicitly asking them to <em>stop</em> caring about something else, because they have a finite amount of time. They can&rsquo;t do everything. In order to help you, they must give up something else. Is it grocery shopping? Time to cook? Time to sleep? A social gathering? Playtime with their children?</p><p>By no means should you <em>stop</em> asking people to care about something, that part is kind of important. Raising awareness allows individuals to make informed decisions about what trade-offs they are making with their time. However, if someone says they aren&rsquo;t interested in something you care about&mldr; it&rsquo;s because they have different priorities, and the trade-offs didn&rsquo;t make sense. Maybe they care more about adding a feature to a 50 year old programming language, and thank goodness they did, because <a href=https://thephd.dev/finally-embed-in-c23>would you have cared enough to put up with this nonsense</a>?</p><p>Your time is precious. Other people&rsquo;s time, doubly so. Mind it well.</p></article></div></section><section><div class=dim><aside><h2>Discord Should Remove Usernames Entirely</h2><ul></ul></aside><article><p><a href=https://discord.com/blog/usernames>Discord&rsquo;s Recent Announcement</a> made a lot of people mad, mostly because of <a href=https://www.hyrumslaw.com/>Hyrum&rsquo;s Law</a> - users were relying on unintended observable behavior in the original username system, and are mad that their use-cases are being broken despite very good evidence that the current system is problematic. I think the major issue here is that Discord didn&rsquo;t go far enough, and as a result, it&rsquo;s confusing users who are unaware of the technical and practical reasons for the username change, or what a username is even for.</p><p><a href=https://twitter.com/TeqieLemon/status/1654014041373790208>There are several issues</a> being brought up with the username change. One is that users are very upset about usernames being ascii-only alphanumeric, presumably because they do not realize that Discord is only ever going to show their usernames for the purposes of adding friends. Their <em>Display Name</em> is what everyone will normally see, which can be any arbitrary unicode. Discord only spent a single sentence mentioning the problem with someone&rsquo;s username being written in 𝕨𝕚𝕕𝕖 𝕥𝕖𝕩𝕥 and I think a lot of users missed just how big of a problem this is. Any kind of strange character in a username would be liable to render it completely unsearchable, could easily get corrupted when sent over ascii-only text mediums, and essentially had to be copy+pasted verbatim or it wouldn&rsquo;t work.</p><p>However, some users <em>wanted</em> to be unsearchable, because they had stalkers or were very popular and didn&rsquo;t want random people finding their discord account. Discriminators and case-sensitivity essentially created a searchability problem which users were utilizing on purpose to make it harder for people to search them. The solution to this is extremely simple, and was in fact a feature of many early chat apps: let the user turn off the ability for people to search for their username. That&rsquo;s what people <em>actually want</em>.</p><p>What discord is trying to do, and communicating incredibly poorly, is <strong>transform usernames into friend codes</strong>. They say this in a very roundabout way for some reason, and they are also allowing people to essentially <em>reserve custom friend codes</em>. This is silly. Discord should instead <strong>replace usernames with friend codes</strong>, and provide an opt-in fuzzy search mechanism that tries to find someone based on their Display Name, if users want to be discoverable that way. Discord should let you either regenerate or completely disable your own friend code, if users don&rsquo;t want random people trying to friend them.</p><p>What makes this so silly is that nothing is preventing discord from doing this, because <em>you log in with your e-mail anyway!</em> By replacing usernames with display names, Discord has removed all functionality from them aside from friend codes, so they should just turn usernames into friend codes and stop confusing everyone so much. There is absolutely no reason a user should have to keep track of their username, display name, and server specific nicknames, and letting users reserve custom friend codes is never going to work, because everyone is going to fight over common friend codes. Force the friend codes to be random 10-digit alphanumeric strings. Stop pretending they should be anything else. Stop letting people reserve specific ones.</p><p>There is <em>one exception</em> to this that I would tolerate: a custom profile URL. If you wanted to allow people with nitro to, for whatever reason, pay to have a special URL that linked to their profile, this could be done on a first-come first-serve basis, and it would be pretty obvious to everyone why it had to be unique and an ascii-compatible URL.</p><p>I&rsquo;m really tired of companies making a decision for good engineering reasons, and then implementing that decision in the most confusing way possible and blaming anyone who complains as luddites who hate change. There are better ways to communicate these kinds of changes. If your users are confused and angry about it, then it&rsquo;s <em>your fault</em>, not theirs.</p></article></div></section><section><div class=dim><aside><h2>Money Is Fake. It's Not Real. It's Made Up.</h2><ul></ul></aside><article><blockquote><b>Death</b>: No. Humans need fantasy to be human. To be the place where the falling angel meets the rising ape.<br><b>Susan</b>: With tooth fairies? Hogfathers?<br><b>Death</b>: Yes. As practice, you have to start out learning to believe the little lies.<br><b>Susan</b>: So we can believe the big ones?<br><b>Death</b>: Yes. Justice, mercy, duty. That sort of thing.</blockquote><p>I want to start this by saying that I am in favor of a wealth tax. We should be increasing taxes on the wealthy and raising minimum wage, because we know that steadily increasing the relative buying power of the poor is the best way to improve an economy. However, none of this happens in a vacuum. When we talk about income equality, I have become distressed at the amount of ignorance on display about the economy, systemic societal problems, and even what money actually is.</p><p><a href=https://mkorostoff.github.io/1-pixel-wealth/>One Pixel Wealth</a> is a webpage from 2021 that helps visualize how truly insane the amount of wealth that the richest people have actually is. While the visualization is great at putting in perspective just how much Jeff Bezos&rsquo; wealth is <em>on paper</em>, it links to a refutation of the Paper Billionaire Argument to dispute the idea that Jeff Bezos doesn&rsquo;t <em>really</em> have that much money in liquid assets. The paper billionaire argument is that, because <a href=https://www.cnbc.com/2018/02/07/where-the-super-rich-keep-their-money.html>most wealth is in stocks or bonds</a>, selling it all at once would flood the market and crater the total value of those assets.</p><p><a href=https://github.com/MKorostoff/1-pixel-wealth/blob/master/THE_PAPER_BILLIONAIRE.md>The proposed counter-argument</a> is <strong>incredibly bad</strong>. It demonstrates a total lack of understanding about macro-economic forces. Ironically, this is because it cannot appreciate the scale of its own arguments, the exact issue that One Pixel Wealth is trying to address. Let me paraphrase the key points in this counter-argument:</p><ol><li>The Paper Billionaire argument doesn&rsquo;t work, because you can liquidate the wealth over time in a <a href=https://corpgov.law.harvard.edu/2016/03/24/a-guide-to-rule-10b5-1-plans/>controlled sell-off</a>, which <a href=https://www.cnbc.com/2020/02/11/jeff-bezos-sold-4point1-billion-worth-of-amazon-shares-in-past-week.html>executives do regularly</a>.</li><li>Given that <a href="https://www.nasdaqtrader.com/trader.aspx?id=FullVolumeSummary#">$122 trillion worth of stock</a> changes hands in the US every year, you could liquidate a trillion dollars over five years and only constitute 0.16% of all the trading.</li><li>Because 50% of all US households own stock, you will always be able find people to buy the stock the billionaires are selling, it&rsquo;s not just other billionaires that will buy it.</li><li>Even if the paper bilionaire argument was true, if selling all the stock would lose 80% of it&rsquo;s value, that would leave behind $700 billion.</li></ol><p>To start, #1 and #2 don&rsquo;t work for a very simple reason: A stock&rsquo;s value represents the market&rsquo;s <em>confidence</em> in the stock producing future value. Owning stock, in some circumstances, is interpreted as having confidence in that future value. If the market loses confidence in your company, it doesn&rsquo;t matter what assets you have, your stock price will crater if the market thinks you&rsquo;ll start losing money. If the <em><strong>CEO</strong></em> starts liquidating their position (which they must state their intention to sell stock ahead of time, years before it completes), the market will panic and the stock price will implode at the mere <em>announcement</em> of the liquidation, let alone actually selling any stock. Elon Musk right now should make it painfully obvious that he was only ever the richest man in the world on paper, because he just lost <a href="https://www.cbsnews.com/news/elon-musk-wealth-bernard-arnault-richest-person-forbes-bloomberg-gdp/#:~:text=Arnault%2C%2073%2C%20owns%2048%25,billion%20loss%20on%20the%20year">$107 billion dollars</a> this year! He only bought Twitter for $44 billion! You simply cannot make the combined GDPs of Bulgaria, Croatia, Iceland and Uruguay <em>evaporate</em> if that money was actually real in any sense.</p><p><strong>Money does not represent physical assets.</strong> Money is supposed to represent human labor, and there is a fixed amount of human labor available on the planet. When someone dies or is incapacitated, it goes down. When someone graduates into the labor force, or becomes more skilled, it goes up. In ancient times, &ldquo;human labor&rdquo; was heavily correlated to how much physical activity someone could do, like lifting things or harvesting food. However, our modern economy is dominated by specialist jobs done by highly skilled laborers. So for the sake of analysis, we can say that the GDP of the entire planet should <em>ideally</em> represent the maximum amount of labor the entire human race could do, if we assigned everyone to the job they are most qualified for. We could then increase the total amount of labor we can do by either building machines or improving our skills.</p><p>This leads into why point #3 is complete nonsense. It reminds me of when Ben Shapiro, when talking about climate change, asked &ldquo;you think that people aren&rsquo;t going to just sell their homes and move?&rdquo;</p><iframe width=560 height=315 src=https://www.youtube.com/embed/X9FGRkqUdf8 title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p>The <strong>entire point of wealth inequality</strong> is that the top 1% <a href="https://www.bloomberg.com/news/articles/2021-10-08/top-1-earners-hold-more-wealth-than-the-u-s-middle-class?leadSource=uverify+wall">holds more money than the <strong>entire middle-class</strong></a>. That&rsquo;s <em>literally</em> <em><strong>the problem!</strong></em> How can everyone else possibly buy all the stock the billionaires are selling if it would require all of their savings? Who are you selling the stocks to?! <strong>This isn&rsquo;t how money works!</strong> One Pixel Wealth even tries to claim that if we just gave all the poor people in america a bunch of money it would fix poverty, while <a href=https://www.givedirectly.org/research-on-cash-transfers/>linking to a study</a> that only applies to <strong>local</strong> economies. The <em>world&rsquo;s largest economy</em> is <strong>NOT</strong> a local economy! These measures only work when the global economy can absorb the difference, which means making changes gradually or in small, localized areas.</p><p>Of course, even if you somehow magically liquidated all your assets and acquired $700 billion dollars in real, liquid cash, it&rsquo;s not actually $700 billion dollars. It&rsquo;s like saying that there are gold asteroids worth <a href=https://tech.hindustantimes.com/photos/this-asteroid-is-worth-10-000-quadrillion-71659963582639.html>$10000 quadrillion dollars</a> - the value would plummet if you actually had that much gold. Since money represents human labor, which is a limited resource, simply having more money does not let you do more things. $700 billion dollars is enough to hire 12 billion people for 1 day working at minimum wage ($7.25), but you can&rsquo;t actually do that, because there&rsquo;s only 7.8 billion people in the entire world. Having $700 billion in liquid assets would decrease the value <em>of money itself</em>. That&rsquo;s what inflation <em>is</em>. People claim that some billions of dollars will be enough to eradicate malaria or provide drinking water to everyone, but it&rsquo;s <em>never that simple</em> because these are always <em>geopolitical issues</em>. Bill Gates has donated <a href=https://fortune.com/2018/04/18/bill-gates-foundation-malaria/>billions of dollars</a> since <a href=https://www.gatesfoundation.org/ideas/media-center/press-releases/2005/10/gates-foundation-commits-2583-million-for-malaria-research>2005</a> towards fighting malaria and we only got a vaccine <a href=https://www.aljazeera.com/news/2021/10/6/who-reccomends-rollout-of-malaria-vaccine-for-african-children>16 years later</a>. We&rsquo;re surrounded by so many dumb problems we can solve with more money that we&rsquo;ve forgetten that some problems are really, truly, fundamentally difficult problems that cannot be solved by throwing money at them. At some point there are just too many cooks in the kitchen.</p><p>Note that this labor distribution problem applies to <em>liquid assets</em>, which is one theory on why inflation had (until 2021), remained fairly low despite the amount of wealth increasing to ridiculous amounts. Wealthy people are acting as gigantic money sinks - by absorbing all of the &ldquo;money&rdquo;, the actual amount of real, liquid cash in the economy increased at a modest rate, so inflation remained stable. Now, inflation has started to skyrocket in 2022, and some people blame the stimulus payments, but the reality is that the low interest rates during the pandemic, combined with other complex macroeconomic forces, likely caused it, although nobody knows for sure. If wealthy people started actually spending all their money at once, as people seem to want them to do, the amount of liquid assets would skyrocket and so would inflation.</p><p>I keep saying that money is <em>supposed</em> to represent human labor, because it&rsquo;s really an approximation. Someone can be more productive at one job than another, so the amount of human labor is not a knowable value in the first place. Instead, it helps to think of money as representing <em>percieved power imbalance</em> (conservatives often make the mistake of thinking it represents <em>actual</em> power imbalance, which it does not). This power imbalance can come from economic, diplomatic, or military factors. Basically, money is just the current state of global geopolitics. You cannot fight wealth inequality by just redistributing money. Simply taking money from rich people does not fix the systemic issues that created the power imbalance in the first place, because it&rsquo;s not actually <em>wealth</em> inequality, it&rsquo;s <em>power</em> inequality, and that is a political issue, not economic. Money is simply our way of quantifying that imbalance. The government&rsquo;s unwillingness to tax rich people is <em>because</em> of the power imbalance, not the <em>cause</em> of it. If politicians are unwilling to go after rich people, it&rsquo;s because those rich people hold an alarming amount of sway over politicians, which makes them <a href="https://www.youtube.com/watch?v=rStL7niR7gs">keys to power</a>.</p><p>It means that we have allowed power to accumulate in dangerously high concentrations, and we need to deal with this at a political level before we get an economic solution. We must elect leaders that help tackle power inequality (like break up huge corporations) before we can make progress on wealth inequality. Basically, go vote.</p></article></div></section><section><div class=dim><aside><h2>We Need New Motherboards Before GPUs Collapse Under Their Own Gravity</h2><ul></ul></aside><article><p><strong>You can&rsquo;t have a 4-slot GPU. You just can&rsquo;t.</strong></p><p>We have finally left sanity behind, with nvidia&rsquo;s 4000 series cards yielding a &ldquo;clown car&rdquo; of absurd GPU designs, as <a href="https://www.youtube.com/watch?v=mGARjRBJRX8">GamersNexus put it</a>. These cards are so huge they need &ldquo;GPU Support Sticks&rdquo;, which are an <a href=https://www.pcworld.com/article/1072925/custom-nvidia-geforce-rtx-4090-4080s-graphics-cards-insane.html>actual real thing now</a>. The fact that we insist on relegating the GPU to interfacing with the system while hanging off of a single, increasingly absurd PCIe 6.0 x16 slot that can <a href=https://www.anandtech.com/show/17203/pcie-60-specification-finalized-x16-slots-to-reach-128gbps>push 128 GBps</a> is <strong>completely insane</strong>. There is no real ability to just pick the GPU you want and then pair it with a cooler that is <em>actually attached to the motherboard</em>. The entire cooling solution has to be in the card itself and we are fast reaching the practical limitations here due to gravity and the laws of physics. Top-heavy GPUs are now essentially giant levers pulling on the PCIe slot, with the only possible anchor point that is above the center of mass being the bracket on one side.</p><p>A 4090 series card will demand a whopping 450 W, which dwarfs the Ryzen 9 5900X peak power consumption of only 140 W. That&rsquo;s over 3 times as much power! The graphics card is now drawing more power than the <strong>entire rest of the computer!</strong> We&rsquo;ll have to wait for benchmarks to be sure, but the laws of thermodynamics suggest that the GPU will now also be producing more heat than every other component of the PC, <em>combined</em>. And this is the thing we have hanging off of a PCIe slot that doesn&rsquo;t have any other way of mounting a cooling solution to the motherboard?!</p><p><strong>What the FUCK are we doing?!</strong></p><p>Look, I&rsquo;m not a hardware guy. I just write all the shader code that makes GPUs cry. I don&rsquo;t actually know how we should fix this problem, because I don&rsquo;t know what designs are thermally efficient or not. I do know, however, that something has to change. Maybe we can make motherboards with a GPU slot next to the CPU slot and have a unified massive radiator sitting on top of them - or maybe it&rsquo;s a better idea to put the two processor units on opposite ends of the board. I don&rsquo;t know, <strong>just <em>do</em> something</strong> so I can use a cooling solution that is actually <em>screwed into the fucking motherboard</em> instead of requiring a &ldquo;GPU Support Stick&rdquo; so gravity doesn&rsquo;t rip it out of the PCIe slot.</p><p>As an example of alternative solutions, <a href="https://www.amazon.com/Original-Graphics-Alienware-N14E-GS-A1-Replacement/dp/B081L3KH3T/ref=sr_1_35?keywords=mxm+graphics+card&amp;qid=1663887964&amp;sr=8-35">here is an MXM form-factor</a> for laptops that allow them to provide custom cooling solutions appropriate for the laptop.</p><div class=imgwrap style=max-width:466px><a href=https://m.media-amazon.com/images/I/71QOqoCJ3WL._AC_SX466_.jpg target=_blank><img src=https://m.media-amazon.com/images/I/71QOqoCJ3WL._AC_SX466_.jpg width=100%></a></div><p>In fact, the PCIe spec itself actually contains a rear-bracket mount that, if anyone was paying attention, would help address this problem:</p><div class=imgwrap style=max-width:1440px><a href=/img/pci_standard.png target=_blank><img src=/img/pci_standard.png width=100%></a></div><p>See that funky looking metal thing labeled &ldquo;2&rdquo; on the diagram? That sure looks like a good alternative to a &ldquo;support stick&rdquo; if anyone ever actually paid attention to the spec. Or maybe this second bracket doesn&rsquo;t work very well and we need to rethink how motherboards work entirely. Should we have GPU VRAM slots alongside CPU RAM slots? Is that even possible? (<a href="https://news.ycombinator.com/item?id=32946816">Nope.</a>) Or maybe we can come up with an alternative form factor for GPU cards that you can actually attach to the motherboard with screws?</p><p>I have no idea what is or isn&rsquo;t practical, but please, just do something before the GPUs collapse under their own gravity and create strange new forms of matter inside my PC case.</p></article></div></section><section><div class=dim><aside><h2>C++ Constructors, Memory, and Lifetimes</h2><ul></ul></aside><article><div class=imgwrap style=max-width:400px><a href=/img/cpp_init_forest.gif target=_blank><img src=/img/cpp_init_forest.gif alt="C++ Initialization Hell" width=100%></a></div><p>What exactly happens when you write <code><code>Foo* foo = new Foo();</code></code>? A lot is packed into this one statement, so lets try to break it down. First, this example is allocating new memory on the heap, but in order to understand everything that&rsquo;s going on, we&rsquo;re going to have to explain what it means to declare a variable <em>on the stack</em>. If you already have a good understanding of how the stack works, and how functions do cleanup before returning, feel free to skip to the <a href=#new-statements>new statement</a>.</p><h3 id=stack-lifetimes>Stack Lifetimes</h3><p>Describing the stack is very often glossed over in many other imperative languages, despite the fact that those languages still have one (functional languages are an entirely different level of weird). Let&rsquo;s start with something very simple:<pre class=language-cpp><code>int foobar(int b)
{
  int a;
  a = b;
  return a;
}
</code></pre>Here, we are declaring a function <code>foobar</code> that takes an <code>int</code> and returns an <code>int</code>. The first line of the function declares a variable <code>a</code> of type <code>int</code>. This is all well and good, but <em>where is the integer?</em>. On most modern platforms, <code>int</code> resolves to a 32-bit integer that takes up 4 bytes of space. We haven&rsquo;t allocated any memory yet, because no <code>new</code> statement happened and no <code>malloc()</code> was called. Where is the integer?</p><p>The answer is that the integer was allocated on the <em>stack</em>. If you aren&rsquo;t familiar with the <a href=https://en.wikipedia.org/wiki/Stack_(abstract_data_type)>computer science data structure</a> of the same name, your program is given a chunk of memory by the operating system that is organized into a stack structure, hence the name. It&rsquo;s like a stack of plates - you can push items on top of the stack, or you can remove items from the top of the stack, but you can&rsquo;t remove things from the middle of the stack or all the plates will come crashing down. So if we push something on top of the stack, we&rsquo;re stuck with it until we get rid of everything on top of it.</p><p>When we called our function, the parameter <code>int b</code> was pushed on to the stack. Parameters take up memory, so on to the stack they go. Hence, before we ever reach the statement <code>int a</code>, 4 bytes of memory were already pushed onto our stack. Here&rsquo;s what our stack looks like at the beginning of the function if we call it with the number <code>90</code> (assuming little-endian):</p><div class=imgwrap style=max-width:450px><a href=/img/stack1.svg target=_blank><img src=/img/stack1.svg alt="Stack for b" width=100%></a></div><p><code>int a</code> tells the compiler to push another 4 bytes of memory on to the stack, but it has no initial value, so the contents are undefined:</p><div class=imgwrap style=max-width:450px><a href=/img/stack2.svg target=_blank><img src=/img/stack2.svg alt="Stack for a and b" width=100%></a></div><p><code>a = b</code> assigns b to a, so now our stack looks like this:</p><div class=imgwrap style=max-width:450px><a href=/img/stack3.svg target=_blank><img src=/img/stack3.svg alt="Stack for initialized a and b" width=100%></a></div><p>Finally, <code>return a</code> tells the compiler to evaluate the return expression (which in our case is just <code>a</code> so there&rsquo;s nothing to evaluate), then copy the result into a chunk of memory we reserved ahead of time for the return value. Some programmers may assume the function returns <em>immediately</em> once the <code>return</code> statement is executed - after all, that&rsquo;s what <code>return</code> means, right? However, the reality is that the function still has to clean things up before it can actually return. Specifically, we need to return our stack to the state it was before the function was called by removing everything we pushed on top of it <strong>in reverse order</strong>. So, after copying our return value <code>a</code>, our function pops the top of the stack off, which is the last thing we pushed. In our case, that&rsquo;s <code>int a</code>, so we pop it off the stack. Our stack now looks like this:</p><div class=imgwrap style=max-width:450px><a href=/img/stack1.svg target=_blank><img src=/img/stack1.svg alt="Stack without a" width=100%></a></div><p>The moment from which <code>int a</code> was pushed onto the stack to the moment it was popped off the stack is called the <strong>lifetime</strong> of <code>int a</code>. In this case, <code>int a</code> has a lifetime of the entire function. After the function returns, our caller has to pop off <code>int b</code>, the parameter we called the function with. Now our stack is empty, and the <strong>lifetime</strong> of <code>int b</code> is longer than the <strong>lifetime</strong> of <code>int a</code>, because it was pushed first (before the function was called) and popped afterwards (after the function returned). C++ builds it&rsquo;s entire concept of constructors and destructors on this concept of lifetimes, and they can get very complicated, but for now, we&rsquo;ll focus only on stack lifetimes.</p><p>Let&rsquo;s take a look at a more complex example:<pre class=language-cpp><code>int foobar(int b)
{
  int a;
  
  {
    int x;
    x = 3;
    
    {
      int z;
      int max;
      
      max = 999;
      z = x + b;
      
      if(z &gt; max)
      {
        return z - max;
      }
      
      x = x + z;
    }
    
    // a = z; // COMPILER ERROR!
    
    {
      int ten = 10;
      a = x + ten;
    }
  } 
  
  return a;
}
</code></pre>Let&rsquo;s look at the lifetimes of all our parameters and variables in this function. First, before calling the function, we push <code>int b</code> on to the stack with the value of whatever we&rsquo;re calling the function with - say, <code>900</code>. Then, we call the function, which immediately pushes <code>int a</code> on to the stack. Then, we <em>enter a new block</em> using the character <code>{</code>, which does not consume any memory, but instead acts as a marker for the compiler - we&rsquo;ll see what it&rsquo;s used for later. Then, we push <code>int x</code> on to the stack. We now have 3 integers on the stack. We set <code>int x</code> to <code>3</code>, but <code>int a</code> is still undefined. Then, we <em>enter another new block</em>. Nothing interesting has happened yet. We then push both <code>int z</code> and <code>int max</code> on to the stack. Then we assign <code>999</code> to <code>int max</code> and assign <code>int z</code> the value <code>x + b</code> - if we passed in <code>900</code>, this means <code>z</code> is now equal to <code>903</code>, which is less than the value of <code>int max</code> (<code>999</code>), so we skip the if statement for now. Then we assign <code>x</code> to <code>x + z</code>, which will be <code>906</code>.</p><p>Now things get interesting. Our topmost block <strong>ends</strong> with a <code>}</code> character. This tells the compiler to <em>pop all variables declared inside that block</em>. We pushed <code>int z</code> on to the stack inside this block, so it&rsquo;s gone now. We cannot refer to <code>int z</code> anymore, and doing so will be a compiler error. <code>int z</code> is said to have <em>gone out of scope</em>. However, we also pushed <code>int max</code> on to the stack, and we pushed it after <code>int z</code>. This means that the compiler will <strong>first pop <code>int max</code> off the stack</strong>, and only afterwards will it then pop <code>int z</code> off the stack. The order in which this happens will be critical for understanding how lifetimes work with constructors and destructors, so keep it in mind.</p><p>Then, we enter another new scope. This new scope is still inside the first scope we created that contains <code>int x</code>, so we can still access <code>x</code>. We define <code>int ten</code> and initialize it with <code>10</code>. Then we set <code>int a</code> equal to <code>x + ten</code>, which will be <code>916</code>. Then, our scope ends, and <code>int ten</code> goes out of scope, being popped off the stack. Immediately afterwards, we reach the end of our first scope, and <code>int x</code> is popped off the stack.</p><p>Finally, we reach <code>return a</code>, which copies <code>a</code> to our return value memory segment, pops <code>int a</code>, and returns to our caller, who then pops <code>int b</code>. That&rsquo;s what happens when we pass in <code>900</code>, but what happens if we pass in <code>9000</code>?</p><p>Everything is the same until we reach the <code>if</code> statement, whose condition is now satisfied, which results in the function terminating early and returning <code>z - max</code>. What happens to the stack?</p><p>When we reach <code>return z - max</code>, the compiler evaluates the statement and copies the result (<code>8004</code>) out. Then it starts popping everything off the stack (once again, in the reverse order that things were pushed). The last thing we pushed on to the stack was <code>int max</code>, so it gets popped first. Then <code>int z</code> is popped. Then <code>int x</code> is popped. Then <code>int a</code> is popped, the function returns, and finally <code>int b</code> is popped by the caller. This behavior is critical to how C++ uses lifetimes to implement things like smart pointers and automatic memory management. Rust actually uses a similar concept, but it uses it for a lot more than C++ does.</p><h3 id=new-statements><code>new</code> Statements</h3><p>Okay, now we know how lifetimes work and where variables live when they aren&rsquo;t allocated, but what happens when you <em>do</em> allocate memory? What&rsquo;s going on with the <code>new</code> statement? To look at this, let&rsquo;s use a simplified example:<pre class=language-cpp><code>int* foo = new int();
</code></pre>Here we have allocated a pointer to an integer on the stack (which will be 8 bytes if you&rsquo;re on a 64-bit system), and assigned the result of <code>new int()</code> to it. What happens when we call <code>new int()</code>? In C++, the <code>new</code> operator is an extension of <code>malloc()</code> from C. This means it allocates memory from the <em>heap</em>. When you allocate memory on the heap, it never goes out of scope. This is what most programmers are familiar with in other languages, except that most other languages handle figuring out when to deallocate it and C++ forces you to delete it yourself. Memory allocated on the heap is just there, floating around, forever, or until you deallocate it. So this function has a memory leak:<pre class=language-cpp><code>int bar(int b)
{
  int* a = new int();
  *a = b;
  return *a;
}
</code></pre>This is the same as our <a href=#stack-lifetimes>first example</a>, except now we allocate <code>a</code> on the heap instead of the stack. So, it never goes out of scope. It&rsquo;s just there, sitting in memory, forever, until the process is terminated. The <code>new</code> operator looks at the type we passed it (which is <code>int</code> in this case) and calls <code>malloc</code> for us with the appropriate number of bytes. Because <code>int</code> has no constructors or destructors, it&rsquo;s actually equivelent to this:<pre class=language-cpp><code>int bar(int b)
{
  int* a = (int*)malloc(sizeof(int));
  *a = b;
  return *a;
}
</code></pre>Now, people who are familiar with C will recognize that any call to <code>malloc</code> should come with a call to <code>free</code>, so how do we do that in C++? We use <code>delete</code>:<pre class=language-cpp><code>int bar(int b)
{
  int* a = new int();
  *a = b;
  int r = *a;
  delete a;
  return r;
}
</code></pre><strong>IMPORTANT:</strong> Never mix <code>new</code> and <code>free</code> or <code>malloc</code> and <code>delete</code>. The <code>new</code>/<code>delete</code> operators can use a different allocator than <code>malloc</code>/<code>free</code>, so things will violently explode if you treat them as interchangeable. Always <code>free</code> something from <code>malloc</code> and always <code>delete</code> something created with <code>new</code>.</p><p>Now we aren&rsquo;t leaking memory, but we also can&rsquo;t do <code>return *a</code> anymore, because it&rsquo;s impossible for us to do the necessary cleanup. If we were allocating on the stack, C++ would clean up our variable for us after the <code>return</code> statement, but we can&rsquo;t put anything after the return statement, so there&rsquo;s no way to tell C++ to copy the value of <code>*a</code> and <em>then</em> manually delete <code>a</code> without introducing a new variable <code>r</code>. Of course, if we could run arbitrary code when our variable went out of scope, we could solve this problem! This sounds like a job for constructors and destructors!</p><h3 id=constructors-and-delete>Constructors and <code>delete</code></h3><p>Okay, let&rsquo;s put everything together and return to our original statement in a more complete example:<pre class=language-cpp><code>struct Foo
{
  // Constructor for Foo
  Foo(int b)
  {
    a = b;
  }
  // Empty Destructor for Foo
  ~Foo() {}
  
  int a;
};

int bar(int b)
{
  // Create
  Foo* foo = new Foo(b);
  int a = foo-&gt;a;
  // Destroy
  delete foo;
  return a; // Still can&#39;t return foo-&gt;a
}
</code></pre>In this code, we still haven&rsquo;t solved the return problem, but we are now using constructors and destructors, so let&rsquo;s walk through what happens. First, <code>new</code> allocates memory on the heap for your type. <code>Foo</code> contains a 32-bit integer, so that&rsquo;s 4 bytes. Then, <em>after</em> the memory is allocated, <code>new</code> automatically calls the <em>constructor</em> that matches whatever parameters you pass to the type. Your constructor doesn&rsquo;t need to allocate any memory to contain your type, since <code>new</code> already did this for you. Then, this pointer is assigned to <code>foo</code>. Then we delete <code>foo</code>, which <strong>calls the destructor first</strong> (which does nothing), and <em>then</em> deallocates the memory. If you don&rsquo;t pass any parameters when calling <code>new Type()</code>, or you are creating an array, C++ will simply call the default constructor (a constructor that takes no parameters). This is all equivelent to:<pre class=language-cpp><code>int bar(int b)
{
  // Create
  Foo* foo = (Foo*)malloc(sizeof(Foo));
  new (foo) Foo(b); // Special new syntax that ONLY calls the constructor function (this is how you manually call constructors in C++)
  int a = foo-&gt;a; 
  // Destroy
  foo-&gt;~Foo(); // We can, however, call the destructor function directly
  free(foo);
  
  return a; // Still can&#39;t return foo-&gt;a
}
</code></pre>This uses a special new syntax that doesn&rsquo;t allocate anything and simply lets us call the constructor function directly on our already allocated memory. This is what the <code>new</code> operator is doing for you under the hood. We then call the destructor manually (which you <em>can</em> do) and free our memory. Of course, this is all still useless, because we can&rsquo;t return the integer we allocated on the heap!</p><h3 id=destructors-and-lifetimes>Destructors and lifetimes</h3><p>Now, the magical part of C++ is that constructors and destructors are run <em>when things are pushed or popped from the stack</em> <sup><a href=#f1>[1]</a></sup>. The fact that constructors and destructors respect variable lifetimes allows us to solve our problem of cleaning up a heap allocation upon returning from a function. Let&rsquo;s see how that works:<pre class=language-cpp><code>struct Foo
{
  // Default constructor for Foo
  Foo()
  {
    a = new int();
  }
  // Destructor frees memory we allocated using delete
  ~Foo()
  {
    delete a;
  }
  
  int* a;
};

int bar(int b)
{
  Foo foo;
  *foo.a = b;
  return *foo.a; // Doesn&#39;t leak memory!
}
</code></pre>How does this avoid leaking memory? Let&rsquo;s walk through what happens: First, we declare <code>Foo foo</code> on the stack, which pushes 4 bytes on to the stack, and then C++ calls our default constructor. Inside our default constructor, we use <code>new</code> to allocate a new integer and store it in <code>int* a</code>. Returning to our function, we then set our integer pointer <code>foo.a</code> to <code>b</code>. Then, we return the value stored in <code>foo.a</code> from the function<sup><a href=#f2>[2]</a></sup>. This copies the value out of <code>foo.a</code> first by dereferencing the pointer, and <em>then</em> C++ calls our destructor <code>~Foo</code> before <code>Foo foo</code> is popped off the stack. This destructor deletes <code>int* a</code>, ensuring we don&rsquo;t leak any memory. Then we pop off <code>int b</code> from the stack and the function returns. If we could somehow do this without constructors or destructors, it would look like this:<pre class=language-cpp><code>int bar(int b)
{
  Foo foo;
  foo.a = new int();
  *foo.a = b;
  int retval = *foo.b;
  delete a;
  return retval;
}
</code></pre>The ability to run a destructor when something goes out of scope is an incredibly important part of writing good C++ code, becuase when a function returns, <em>all</em> your variables go out of scope when the stack is popped. Thus, all cleanup that is done during destructors is gauranteed to run no matter when you return from a function. Destructors are gauranteed to run <strong>even when you throw an exception!</strong> This means that if you throw an exception that gets caught farther up in the program, you won&rsquo;t leak memory, because C++ ensures that everything on the stack is correctly destroyed when processing exception handling, so all destructors are run in the same order they normally are.</p><p>This is the core idea behind smart pointers - if a pointer is stored inside an object, and that object deletes the pointer in the destructor, then you will never leak the pointer because C++ ensures that the destructor will eventually get called when the object goes out of scope. Now, if implemented naively there is no way to pass the pointer into different functions, so the utility is limited, but C++11 introduced <strong>move semantics</strong> to help solve this issue. We&rsquo;ll talk about those later. For now, let&rsquo;s talk about different kinds of lifetimes and what they mean for when constructors and destructors are called.</p><h3 id=static-lifetimes>Static Lifetimes</h3><p>Because any struct or class in C++ can have constructors or destructors, and you can put structs or classes anywhere in a C++ program, this means that there are rules for how to safely invoke constructors and destructors in all possible cases. These different possible lifetimes have different names. Global variables, or static variables inside classes, have what&rsquo;s called &ldquo;static lifetime&rdquo;, which means their lifetime begins when the program starts and ends once the program exits. The exact order these constructors are called, however, is a bit tricky. Let&rsquo;s look at an example:<pre class=language-cpp><code>struct Foo
{
  // Default constructor for Foo
  Foo()
  {
    a = new int();
  }
  // Destructor frees memory we allocated using delete
  ~Foo()
  {
    delete a;
  }
  
  int* a;
  static Foo instance;
};

static Foo GlobalFoo;

int main()
{
  *GlobalFoo.a = 3;
  *Foo::instance.a = *GlobalFoo.a;
  return *Foo::instance.a;
}
</code></pre>When is <code>instance</code> constructed? When is <code>GlobalFoo</code> constructed? Can we safely assign to <code>GlobalFoo.a</code> immediately? The answer is that all static lifetimes are constructed <strong>before your program even starts</strong>, or more specifically, before <code>main()</code> is called. Thus, by the time your program has reached your entry point (<code>main()</code>), C++ gaurantees that all static lifetime objects have already been constructed. But what <em>order</em> are they constructed in? This gets complicated. Basically, static variables are constructed in the order they are declared in a single <code>.cpp</code> file. However, the order these <code>.cpp</code> files are constructed in is undefined. So, you can have static variables that rely on each other inside a single <code>.cpp</code> file, but never between different files.</p><p>Likewise, all static lifetime objects get deconstructed <em>after</em> your <code>main()</code> function returns, and once again, this order is random, although it <em>should</em> be in the reverse order they were constructed in. Technically this should be respected even if an exception occurs, but because the compiler can assume the process will terminate immediately after an unhandled exception occurs, this is unreliable.</p><p>Static lifetimes still apply for shared libraries, and are constructed the moment the library is loaded into memory - that&rsquo;s <code>LoadLibrary</code> on Windows and <code>dlopen</code> on Linux. Most kernels provide a custom function that fires when the shared library is loaded or unloaded, and these functions fall outside of the C++ standard, so there&rsquo;s no gaurantee about whether the static constructors have actually been called when you&rsquo;re inside the <code>DllLoad</code>, but almost nobody actually needs to worry about those edge cases, so for any normal code, by the time any function in your DLL can be called by another program, you can rest assured all static and global variables have had their constructors called. Likewise, they are destructed when the shared library is unloaded from memory.</p><p>While we&rsquo;re here, there are a few gotchas in the previous example that junior programmers should know about. You&rsquo;ll notice that I did not write <code><code>static Foo* = new GlobalFoo();</code></code> - <strong>this will leak memory!</strong>. In this case, C++ doesn&rsquo;t actually call the destructor because <code>Foo</code> doesn&rsquo;t have a static lifetime, <strong>the pointer it&rsquo;s stored in does!</strong>. So the <em>pointer</em> will get it&rsquo;s constructor called before the program starts (which does nothing, because it&rsquo;s a primitive), and then the pointer will have it&rsquo;s destructor called after <code>main()</code> returns, which also does nothing, which means <code>Foo</code> never actually gets deconstructed or deallocated. Always remember that C++ is <em>extremely</em> picky about what you do. C++ won&rsquo;t magically extend Foo&rsquo;s lifetime to the lifetime of the pointer, it will instead do <em>exactly</em> what you told it to do, which is to declare a global pointer primitive.</p><p>Another thing to avoid is to not accidentally write <code>Foo::instance.a = GlobalFoo.a;</code>, because this doesn&rsquo;t copy the integer, it copies <em>the pointer</em> from <code>GlobalFoo</code> to <code>Foo::instance</code>. This is extremely bad, because now <code>Foo::instance</code> will leak it&rsquo;s pointer and instead try to free <code>GlobalFoo</code>&rsquo;s pointer, which was already deleted by <code>GlobalFoo</code>, so the program will crash, but only AFTER successfully returning 3. In fact, it will crash outside of the <code>main()</code> function completely, which is going to look very weird if you don&rsquo;t know what&rsquo;s going on.</p><h3 id=implicit-constructors-and-temporary-lifetimes>Implicit Constructors and Temporary Lifetimes</h3><p>Lifetimes in C++ can get complicated, because they don&rsquo;t just apply to function blocks, but also function parameters, return values, and expressions. This means that, for example, if we are calling a function, and we construct a new object inside the function call, there is an implicit lifetime that exists for the duration of the function call, which is well-defined but very weird unless you&rsquo;re aware of exactly what&rsquo;s going on. Let&rsquo;s look at a simple example of a function call that constructs an object:<pre class=language-cpp><code>class Foo
{
  // Implicit constructor for Foo
  Foo(int b)
  {
    a = b;
  }
  // Empty Destructor for Foo
  ~Foo() {}
  
  int a;
}

int get(Foo foo)
{
  return foo.a;
}

int main()
{
  return get(3);
}
</code></pre>To understand what&rsquo;s going on here, we need to understand <em>implicit constructors</em>, which are a &ldquo;feature&rdquo; of C++ you never wanted but got anyway. In C++, all constructors that take exactly 1 argument are <em>implicit</em>, which means the compiler will attempt to use call them to satisfy a type transformation. In this case, we are trying to pass <code>3</code> into the <code>get()</code> function. <code>3</code> has the type <code>int</code>, but <code>get()</code> takes an argument of type <code>Foo</code>. Normally, this would just cause an error, because the types don&rsquo;t match. But because we have a constructor for <code>Foo</code> that takes an <code>int</code>, the compiler actually calls it for us, constructing an object of type <code>Foo</code> and passing it into the function! Here&rsquo;s what it looks like if we do this ourselves:<pre class=language-cpp><code>int main()
{
  return get(Foo(3));
}
</code></pre>C++ has &ldquo;helpfully&rdquo; inserted this constructor for us inside the function call. So, now that we know our <code>Foo</code> object is being constructed inside the function call, we can ask a different question: When does the constructor get called, exactly? When is it destructed? The answer is that all the expressions in your function call are evaluated first, from left-to-right. Our expression allocated a new temporary <code>Foo</code> object by pushing it onto the stack and then calling the constructor. However, do be aware that compilers <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=51253">aren&rsquo;t always so great about respecting initialization order</a> in function calls or other initialization lists. But, ostensibly, they&rsquo;re <em>supposed</em> to be evaluated from left-to-right.</p><p>So, once all expressions inside the parameteres have been evaluated, we then push the parameters on to the stack and copy the results of the expressions into them, allocate space on the stack for the return value, and then we enter the function. Our function executes, copies a return value into the space we reserved, finishes cleaning up, and returns. Then we do something with the return value and pop our parameters off the stack. Finally, after all the function parameter boilerplate has been finished, our expressions go out of scope in reverse order. This means that destructors are called from right-to-left after the function returns. This is all roughly equivilent to doing this:<pre class=language-cpp><code>int main()
{
  int b;
  {
    Foo a = Foo(3); // Construct Foo
    b = get(a); // Call function and copy result
  } // Deconstruct Foo
  return b;
}
</code></pre>This same logic works for all expressions - if you construct a temporary object inside an expression, it exists for the duration of the expression. However, the exact order that C++ evaluates expressions is <a href=https://en.cppreference.com/w/cpp/language/eval_order>extremely complicated and not always defined</a>, so this is a bit harder to nail down. Generally speaking, an object gets constructed right before it&rsquo;s needed to evaluate the expression, and gets deconstructed afterwards. These are &ldquo;temporary lifetimes&rdquo;, because the object only briefly exists inside the expression, and is deconstructed once the expression is evaluated. Because C++ expressions are not always ordered, you should not attempt to rely on any sort of constructor order for arbitrary expressions. As an example, we can inline our previous <code>get()</code> function:<pre class=language-cpp><code>int main()
{
  return Foo(3).a;
}
</code></pre>This will allocate a temporary object of type <code>Foo</code>, construct it with <code>3</code>, copy out the value from <code>a</code>, and then deconstruct the temporary object before the return statement is evaluated. For the most part, you can just assume your objects get constructed before the expression happens and get destructed after it happens - try not to rely on ordering more specific than that. The specific ordering rules are also changing in C++20 to make it more strict, which means how strict the ordering is will depend on what compiler you&rsquo;re using until everyone implements the standard properly.</p><p>For the record, if you don&rsquo;t want C++ &ldquo;helpfully&rdquo; turning your constructors into implicit ones, you can use the <code>explicit</code> keyword to disable that behavior:<pre class=language-cpp><code>struct Foo
{
  explicit Foo(int b)
  {
    a = b;
  }
  ~Foo() {}
  
  int a;
};
</code></pre></p><h3 id=static-variables-and-thread-local-lifetimes>Static Variables and Thread Local Lifetimes</h3><p>Static variables inside a function (not a struct!) operate by completely different rules, because this is C++ and consistency is for the weak.<pre class=language-cpp><code>struct Foo
{
  explicit Foo(int b)
  {
    a = b;
  }
  ~Foo() {}
  
  int a;
};

int get()
{
  static Foo foo(3);
  
  return foo.a;
}

int main()
{
  return get() + get();
}
</code></pre>When is <code>foo</code> constructed? It&rsquo;s not when the program starts - it&rsquo;s actually only constructed <em>the first time the function gets called</em>. C++ injects some magic code that stores a global flag saying whether or not the static variable has been initialized yet. The first time we call <code>get()</code>, it will be false, so the constructor is called and the flag is set to true. The second time, the flag is true, so the constructor isn&rsquo;t called. So when does it get destructed? After <code>main()</code> returns and the program is exiting, just like global variables!</p><p>Now, this static initialization <em>is</em> gauranteed to be thread-safe, but that&rsquo;s only useful if you intend to share the value through multiple threads, which usually doesn&rsquo;t work very well, because only the initialization is thread-safe, not accessing the variable. C++ has introduced a new lifetime called <code>thread_local</code> which is even weirder. Thread-local static variables only exist for the duration of the <em>thread</em> they belong to. So, if you have a thread-local static variable in a function, it&rsquo;s constructed the first time you call the function <em>on a per-thread basis</em>, and destroyed <em>when each thread exits</em>, not the program. This means you are gauranteed to have a unique instance of that static variable for each thread, which can be useful in certain concurrency situations.</p><p>I&rsquo;m not going to spend any more time on <code>thread_local</code> because to understand it you really need to know how C++ concurrency works, which is out of scope for this blog post. Instead, let&rsquo;s take a brief look at Move Semantics.</p><h3 id=move-semantics>Move Semantics</h3><p>Let&rsquo;s look at C++&rsquo;s smart pointer implementation, <code>unique_ptr&lt;></code>.<pre class=language-cpp><code>int get(int* b)
{
  return *b;
}

int main()
{
  std::unique_ptr&lt;int&gt; p(new int());
  *p = 3;
  int a = get(p.get());
  return a;
}
</code></pre>Here, we allocate a new integer on the heap by calling <code>new</code>, then store it in <code>unique_ptr</code>. This ensures that when our function returns, our integer gets freed and we don&rsquo;t leak memory. However, the lifetime of our pointer is actually excessively long - we don&rsquo;t need our integer pointer after we&rsquo;ve extracted the value inside <code>get()</code>. What if we could change the lifetime of our pointer? The actual lifetime that we want is this:<pre class=language-cpp><code>int get(int* b)
{
  return *b;
  // We want the lifetime to end here
}

int main()
{
  // Lifetime starts here
  std::unique_ptr&lt;int&gt; p(new int());
  *p = 3;
  int a = get(p.get());
  return a;
  // Lifetime ends here
}
</code></pre>We can accomplish this by using <strong>move semantics</strong>:<pre class=language-cpp><code>int get(std::unique_ptr&lt;int&gt;&amp;&amp; b)
{
  return *b;
  // Lifetime of our pointer ends here
}

int main()
{
  // Lifetime of our pointer starts here
  std::unique_ptr&lt;int&gt; p(new int());
  *p                = 3;
  int a             = get(std::move(p));
  return a;
  // Lifetime of p ends here, but p is now empty
}
</code></pre>By using <code>std::move</code>, we <em>transfer ownership</em> of our unique_ptr to the function parameter. Now the <code>get()</code> function owns our integer pointer, so as long as we don&rsquo;t move it around again, it will go out of scope once <code>get()</code> returns, which will delete it. Our previous <code>unique_ptr</code> variable <code>p</code> is now empty, and when it goes out of scope, nothing happens, because it gave up ownership of the pointer it contained. This is how you can implement automatic memory management in C++ without needing to use a garbage collector, and Rust actually uses a more sophisticated version of this built into the compiler.</p><p>Move semantics can get very complex and have a lot of rules surrounding how temporary values work, but we&rsquo;re not going to get into all that right now. I also haven&rsquo;t gone into the many different ways that constructors can be invoked, and how those constructors interact with the <a href=https://blog.tartanllama.xyz/initialization-is-bonkers/>different ways you can initialize objects</a>. Hopefully, however, you now have a grasp of what lifetimes are in C++, which is a good jumping off point for learning about more advanced concepts.</p><hr><p><sup><a name=f1>[1]</a></sup> Pedantic assembly-code analysts will remind us that the stack allocations usually happen exactly once, at the beginning of the function, and then are popped off at the very end of the function, but the standard technically doesn&rsquo;t even require a stack to exist in the first place, so we&rsquo;re really talking about pushing and popping off the abstract stack concept that the language uses, not what the actual compiled assembly code really does.</p><p><sup><a name=f2>[2]</a></sup> We&rsquo;re <em>dereferencing</em> the pointer here because we want to return the <em>value</em> of the pointer, not the pointer itself! If you tried to return the pointer itself from the function, it would point to freed memory and crash after the function returned. Trying to return pointers from functions is a common mistake, so be careful if you find yourself returning a pointer to something. It&rsquo;s better to use <code>unique_ptr</code> to manage lifetimes of pointers for you.</p></article></div></section><section><div class=dim><aside><h2>Factorio Is The Best Technical Interview We Have</h2><ul></ul></aside><article><p>There&rsquo;s been a lot of hand-wringing over The Technical Interview lately. Many people realize that inverting a binary tree on a whiteboard has basically zero correlation to whether or not someone is actually a good software developer. The most effective programming test anyone&rsquo;s come up with is still <a href=https://www.globalnerdy.com/2012/11/15/fizzbuzz-still-works/>Fizzbuzz</a>. One consequence of this has been an increased emphasis on Open Source Contributions, but it turns out <a href=https://blog.ploeh.dk/2021/03/22/the-dispassionate-developer/>these aren&rsquo;t a very good metric either</a>, because most people don&rsquo;t have that kind of time.</p><p>The most effective programming interview we have now is usually some kind of take-home project, where a candidate is asked to fix a bug or implement a small feature within a few days. This isn&rsquo;t great because it takes up a lot of time, and they could recieve outside help (or, if the feature is sufficiently common, google it). On the other hand, some large companies have instead doubled-down on whiteboard style interviews by subjecting prospective engineers to multiple hour-long online coding assessments, with varying levels of invasive surveillience.</p><p>All these interviewing methods pale in comparison to a very simple metric: <strong>playing Factorio with someone</strong>. Going through an entire run of Factorio is almost the best possible indication of how well someone deals with common technical problems. You can even tweak the playthrough based on the seniority of the position you&rsquo;re hiring for to get a better sense of how they&rsquo;ll function in that role.</p><h3 id=factorio>Factorio?</h3><p>Factorio is a game about automation. The best introduction is probably <a href="https://www.youtube.com/watch?v=KVvXv1Z6EY8">this trailer</a>, but in essence, your job is to build an automated factory capable of launching a rocket into space.</p><p>You begin with nothing. You mine stone manually to craft a smelter that can smelt iron ore you mined into iron plates, which you then use to build a coal-driven automatic miner. You could grab the iron ore from the miner and put it in the smelter yourself, but it&rsquo;s more efficient to use an inserter to do the inserting for you. Then you can use the iron this gives you to make another miner, which automates coal mining. Then you can use belts to take the coal and use an inserter to put it in the iron miner. Then you use the iron plates this tiny factory produces to make a third miner to start gathering copper, which then lets you craft copper wire, which lets you craft a circuit, which lets you build a water pump. Combined with a boiler and a steam engine, you can then build produce power, and use this power to run a research facility to unlock new technology, like assembly machines. Once you&rsquo;ve unlocked assembly machines, you can use your circuits to craft an assembly machine that can craft copper wire for you, and insert this into an assembly machine that crafts circuits for you.</p><p>Eventually you unlock trains and robots and logistic systems which help you deal with the increasing logistic complexity the game demands, until you finally manage to launch a rocket into space.</p><h3 id=self-direction>Self-Direction</h3><p>The beginning of the game starts with no goals and barely any direction. A senior developer should be able to explore the UI and figure out a goal, then establish a plan for accomplishing that goal. A junior developer should be able to perform a task that a senior developer has provided for them. An intern is expected to require quite a bit of mentoring, but a junior developer should be able to troubleshoot basic problems with their own code before requiring assistance from the senior developer. An intermediate developer should be able to operate independently once given a task, but is not expected to do any architecture design.</p><p>In more concrete terms, you might expect the following:</p><ul><li>An <strong>Intern</strong> is generally expected to be able to fill in a pre-placed blueprint, and use belts to hook up their blueprint with something else, like an ore patch.</li><li>A <strong>Junior Developer</strong> should be able to build a production line by themselves, although it probably won&rsquo;t be very optimal. They may need assistance from the senior developer on how to route the belts properly to all of the intermediate assembly machines.</li><li>An <strong>Intermediate Developer</strong> should be capable of designing a near-optimal production line (without beacons) once given direction, with minimal oversight.</li><li>The <strong>Senior Developer</strong> needs no direction, and is capable of determining what goals need to happen and designing a plan of action, then delegating these tasks to other coders.</li></ul><h3 id=teamwork>Teamwork</h3><p>A critical aspect of software development is the ability to work on a team. This means coordinating your efforts with other people, accomadating the needs of other people&rsquo;s designs and cooperating with the team, instead of simply running off on your own and refusing to adjust your design to help integrate it with someone else&rsquo;s work. This, naturally, arises all the time in Factorio, because base layout designs are limited by physical space. As a result, you need to carefully consider what other people are doing, and sometimes adjust your design to fit in size constraints or deal with someone else&rsquo;s design that took more room than anticipated.</p><p>Anyone who simply runs off and starts doing things themselves or fixing problems without telling people is going to quickly earn the ire of their fellow players, for the exact same reasons cowboy programmers do. Luckily, Factorio includes a built-in equivelent to <code>git blame</code>, by showing you the last player that modified any entity. Thus, when people duct tape temporary solutions and don&rsquo;t inform the team about the problem they were fixing, when their temporary solution finally blows up, people will find out. If people want to win they game, they&rsquo;ll have to learn to cooperate well with their teammates.</p><h3 id=debugging>Debugging</h3><p>One of the most important skills for any programmer is their ability to debug problems. This is perhaps the most obvious parallel between Factorio and real software engineering. Something can go wrong very far away from the actual source of the problem. Being able to rapidly hone in on the real problem is a critical skill, and the thinking process is almost identical to tracing the cause of a crash in an actual program. If an assembly machine has stopped working, first you have to see if there are multiple outputs that got backed up. Then you have to check what ingredient it&rsquo;s missing. Then you have to trace the ingredient back through your factory to find out where you&rsquo;re making it, and repeat ad nauseum.</p><p>Factorio&rsquo;s debugging gets fairly complicated quite quickly. As soon as you start working on oil processing you&rsquo;ll be dealing with cracking, where you&rsquo;re dealing with 3 different outputs and if any of them get backed up for any reason, the entire thing stops. There are cases where your entire factory can grind to a halt because you started researching something that doesn&rsquo;t require yellow science, which stopped using up robot frames, which stopped using up electric engines, which stopped using lubricant, which stopped consuming heavy oil, which backed up and stopped oil production, which made you run out of petroleum, which broke plastic, which broke red circuits, which broke the rest of the factory. Seasoned players will anticipate scenarios like this and use circuits to construct self-balancing oil cracking to ensure the system is balanced and will only back up if petroleum backs up. A new player who is a good programmer, when presented with a factory that has collapsed, will usually be able to trace the issue back to the source, realize what&rsquo;s happened, and promptly attempt to figure out a solution. On the other hand, if someone simply plops down a few storage tanks, unless they can provide a good reason (they are very confident we will never stop consuming lubricant in the future), then this is a red flag for how they approach problem solving in their programs.</p><p>Situations like these allow Factorio to closely mimic the complex interdependencies that programmers routinely deal with, and the complexity simply increases the more gameplay concepts are added. This closely follows the increased complexity that additional layers of abstraction introduce when attempting to debug a crash that could have potentially occured deep inside one of the frameworks you use.</p><h3 id=code-reviews>Code Reviews</h3><p>Often, initial designs need to be tweaked for performance or throughput. Good programmers will not only accept critique of their designs, but incorporate that feedback into their future work. If they disagree with a proposed change, they will provide a concrete reason for why they disagree so that the team can more accurately weigh the pros and cons of the proposed change.</p><p>Resisting feedback without providing good reasons is a well-known red flag, but what can also be problematic is a programmer who begrudgingly accepts proposed changes, but refuses to adjust future designs accordingly. They end up requiring constant reminders to adhere to some standard way of solving a problem while giving no reason for why they don&rsquo;t seem to like the way the team is doing things. These can be ticking time-bombs in organizations, because when left unsupervised they can rapidly accumulate technical debt for other team members. This kind of problem is almost impossible to catch in a traditional interview, unless it&rsquo;s an internship.</p><h3 id=code-style-and-frameworks>Code Style and Frameworks</h3><p>Refusing to incorporate feedback is often just a slice of a much larger problem, where someone is unable to integrate properly into an existing framework being used. There are many ways to build a factory in Factorio, and each one requires standard methods of building pieces. Failing to adhere to standards can very quickly jam up an entire factory, often in subtle ways that aren&rsquo;t necessarily obvious to a careless developer.</p><p>In the Main Belt design, a set of 4-8 chunk of belts, divided by 2 spaces to allow for underground belts, are placed in the center of the factory, and all production happens perpendicular to the belt. This design relies on several rules that can wreck havoc if not followed correctly. One, players must always use a splitter to pull items off of a belt, never redirecting the entire belt, otherwise using the empty space for a different belt of items means you&rsquo;ll have permanently lost one entire belt of resources, even after upgrading belts. Two, all factories must be scalable in a direction perpendicular to the main belt. Failing to do this will rapidly result in either a massive waste of space, or a production line that cannot be scaled up because it&rsquo;s surrounded by other production lines.</p><p>There are also different ways of building logistic networks. The simplest method is with passive provider chests, but another method uses a storage chest with a filter, which is used to solve the trashed item problem. Both of these methods require properly setting limiters in the right location. Passive provider chests generally are limited by chest space. Storage chests require hooking the inserter for the chest up to the logistics network and ensuring that less than N of an item exists before inserting it. Forgetting to perform these limiting steps is a massive waste of resources. <em>Consistently</em> forgetting to put limiters on outputs is a red flag for someone who is careless about performance in real-world applications.</p><p>In other cases, the team may be using some pre-designed blueprints, like a nuclear reactor design, or a bot factory. These can be extremely complex, but as long as people are willing to learn how to use them, they can be huge time-savers. Beware of candidates who don&rsquo;t want to learn how to set up a new item in the bot factory simply because they can&rsquo;t debug the complex logic that drives it, or ones that get frustrated learning how to use a bot factory despite the clear and obvious benefits.</p><h3 id=multithreading>Multithreading</h3><p>Trains in Factorio are a direct analogue to multithreading: one train is one thread of execution, and each train intersection or train stop is a place in memory where two threads could potentially write at the same time. Train signals are locks, or mutexes. All bugs in train networks manifest in exactly the same way software race conditions do, because they&rsquo;re literally physical race conditions. All of the tradeoffs apply here as well - if you make a lock too large, it slows down your throughput, because now the intersection is blocked for a longer period of time. Incorrectly signaled tracks routinely cause train deadlocks that are exactly the same as a software deadlock, because you end up with a circular lock dependency. The most common deadlock is when a train is too long and unexpectedly blocks a second intersection while waiting to enter one. This second intersection then prevents another train from leaving, preventing the first intersection from ever being unblocked.</p><p>The number of lanes of track in your network is equivilent to the number of cores available in your CPU. A single rail line is difficult to scale beyond a few threads because the entire system gets throughput limited very quickly, even with wait areas. The most common design is a two-lane design where each lane is a single direction, but this will eventually suffer from throughput issues when you need trains constantly being unloaded. Thus, large bases tend to have at least 4 lanes, with two outer lanes acting as bypasses to avoid the intersection whenever possible.</p><p>Missing signal problems in these systems can take a ridiculous amount of time to actually show up. A single missing signal in one rail network once caused a deadlock after functioning correctly for <em>two weeks</em>. This is remniscient of difficult to pin down race conditions in software that only occur once a month or so when under high contention.</p><h3 id=scaling>Scaling</h3><p>Just like in software, scaling up production in Factorio introduces new problems with initial designs, and often require complete redesigns that can pipe resources into factories as fast as possible, while taking advange of production modules and speed module beacons. Belt limits become problematic even at the fastest belt speed, forcing players to find ways to split designs up so that more belts can be put in later down the line, or split up their factories into modules.</p><p>Handling your logistics network itself becomes a logistics problem in the late game because of how problematic expansive bot networks are. You generally need to start segmenting the logistics network and either using trains to transport items between them, or build a requester chest/provider chest that propagates items across bounderies.</p><p>Managing trains in the late game necessitates switching to a pull architecture from a push architecture, because the push architecture can&rsquo;t function in high throughput. This inevitably requires taking advantage of the Train Limits feature and learning how circuit networks can be used to encode basic logic, such that a station only requests a train when it is actually ready to completely fill the train with resources, instead of the common early game tactic of simply telling a bunch of trains to go to stations named &ldquo;Iron Pickup&rdquo;. This minimizes the number of trains you need while making sure all stops are served on the network.</p><p>Often times, limitations in the number of possible inputs to an assembly machine and inserter speed require redesigning factories around them, just like how high-speed computing requires being aware of subtle bottlenecks in how your CPU works. These bottlenecks are almost never a problem until you reach a certain scale, at which point they begin to dominate your efficiency.</p><h3 id=microservices-and-plugin-architectures>Microservices and Plugin Architectures</h3><p>Eventually, factories get so enormous they must abandon a simple main belt or spaghetti design and use a more scalable framework instead. To reach Megabase-scale, factories generally either use a train system or a module system, which corresponds roughly to microservices or a plugin-architecture.</p><p>A train-based megabase is sometimes referred to a &ldquo;city-block&rdquo; design, where trains surrounded factory blocks and control all input and output. Thus, each individual city-block is isolated from all other city-blocks, since all their input is &ldquo;pure&rdquo; in that it comes from the train network. This is almost identical to a micro-services architecture (over HTTP) or a multi-process design (using IPC), and has similar potential issues with input and output latency, because results cannot be continually provided, they must be emitted in &ldquo;packets&rdquo;, or trains along the network.</p><p>The plugin architecture seeks to maintain some semblence of a main-belt, but instead splits belts off through the factory and uses modular blocks that take standard inputs and standard outputs. Sometimes this can be achieved entirely through bots, but materials usually need to be belted over long distances. This closely resembles a plugin system for a monolithic application, and has similar tradeoffs.</p><p>These megabases mark the extreme upper end of a vanilla Factorio server. However, there are plenty of mods to make things much more complex.</p><h3 id=distributed-systems>Distributed Systems</h3><p><a href=https://mods.factorio.com/mod/space-exploration>Space Exploration</a> is an overhaul of Factorio that adds an entire space segment of the game, and makes planets have limited resources, requiring players to colonize other planets and use rockets to transfer resources between planets. Because of the enormous latency involved with shipping materials between planets, coordinating these different bases winds up having similar problems to a globally distributed database system. Even the circuit network has to contend with latency, because an automatic request system loses track of items that have been launched but haven&rsquo;t yet reached the target planet. Not accounting for this will result in a <em>double-request</em> for all the items you wanted, which is the <em>exact same problem</em> that distributed systems have when trying to ensure consistency.</p><h3 id=conclusion>Conclusion</h3><p>Collectively, the software industry simply has no idea how to hire software developers. Factorio is probably the best technical interview we have right now, and <em>that&rsquo;s embarassing</em>. It is also wildly impractical, taking over 20 hours in an initial multiplayer playthrough, or 8 hours if you have a lot of people and know what you&rsquo;re doing. What&rsquo;s the takeaway from this? I don&rsquo;t know. We certainly can&rsquo;t switch to using Factorio as an interviewing method - you might as well just give a candidate a take-home assignment.</p><p>At the very least, we can do better than whiteboard interviews.</p></article></div></section><section><div class=dim><aside><h2>Why You Can't Use Prebuilt LLVM 10.0 with C++17</h2><ul></ul></aside><article><p>C++17 introduced <a href=https://en.cppreference.com/w/cpp/memory/new/operator_new>an alignment argument</a> to <code>::operator new()</code>. It&rsquo;s important to note that if you allocate something using aligned new, you <strong>absolutely must</strong> deallocate it <a href=https://en.cppreference.com/w/cpp/language/delete>using aligned delete</a>, or the behavior is undefined. LLVM 10.x takes advantage of this alignment parameter, if the compiler supports it. That means if you are compiling on Windows with MSVC set to C++14, <code>__cpp_aligned_new</code> is not defined and the extra argument isn&rsquo;t passed. Otherwise, if it&rsquo;s compiled with MSVC set to C++17, <a href=https://en.cppreference.com/w/User:D41D8CD98F/feature_testing_macros><code>__cpp_aligned_new</code> is defined</a> and the extra argument is passed.</p><p>There&rsquo;s just one teeny tiny little problem - <a href=https://github.com/llvm/llvm-project/blob/6d2f73f821ed5ea584869924b150ac2e6e65c12e/llvm/include/llvm/Support/Compiler.h#L568>the check was in a header file</a>.</p><p>Now, if this header file were completely internal and LLVM itself only ever called <code>::operator new()</code> from inside it&rsquo;s libraries, this wouldn&rsquo;t be a problem. As you might have guessed, this was not the case. LLVM was calling <code>allocate_buffer()</code> from inside header files. The corresponding <code>deallocate_buffer</code> call <em>was</em> inside the same header file, but sadly, it was called from a destructor, and <em>that</em> destructor had been called from inside one of LLVM&rsquo;s libraries, which meant it didn&rsquo;t know about aligned allocations&mldr; <strong>Oops!</strong></p><p>This means, if your program uses C++17, but LLVM was compiled with C++14, your program will happily pass LLVM <strong>aligned memory</strong>, which LLVM will then pass into <strong>the wrong delete operator</strong>, because it&rsquo;s calling the unaligned delete function from C++14 instead of the aligned delete function from C++17. This results in strange and bizarre heap corruption errors because of the mismatched <code>::operator new</code> and <code>::operator delete</code>. Arguably, the real bug here is LLVM calling any allocation function from a header file in the first place, as this is just begging for ABI problems.</p><p>Of course, one might ask why a C++17 application would be linking against LLVM compiled with C++14. Well, you see, the <a href=https://github.com/llvm/llvm-project/releases/tag/llvmorg-10.0.0>prebuilt LLVM binaries</a> were compiled against C++14&mldr; <strong>OOPS!</strong> Suddenly if you wanted to avoid compiling LLVM yourself, you couldn&rsquo;t use C++17 anymore!</p><p>Luckily <a href=https://github.com/llvm/llvm-project/commit/7aaff8fd2da2812a2b3cbc8a41af29774b10a7d6><strong>this has now been fixed</strong></a> after a bunch of people complained about the extremely unintuitive errors resulting from this mismatch. Unfortunately, as of this writing, <a href=https://github.com/llvm/llvm-project/releases/>LLVM still hasn&rsquo;t provided a new release</a>, which means you will still encounter this problem using the latest precompiled binaries of LLVM. Personally, I recompile LLVM using C++17 for my own projects, just to be safe, since LLVM does not gaurantee ABI compatibility in these situations.</p><p>Still, this is a particularly nasty case of an unintentional ABI break between C++ versions, which is easy to miss because most people assume C++ versions are backwards compatible. Be careful, and stop allocating memory in header files that might be deallocated inside your library!</p></article></div></section><section><div class=dim><aside><h2>Pressure Based Anti-Spam for Discord Bots</h2><ul></ul></aside><article><p>Back when Discord was a wee little chat platform with no rate limiting whatsoever, it&rsquo;s API had already been reverse-engineered by a bunch of bot developers who then went around spamming random servers with so many messages it would crash the client. My friends and I were determined to keep our discord server public, so I went about creating the first anti-spam bot for Discord.</p><p>I am no longer maintaining this bot beyond simple bugfixes because I have better things to do with my time, and I&rsquo;m tired of people trying to use it because &ldquo;it&rsquo;s the best anti-spam bot&rdquo; even after I deprecated it. This post is an effort to educate all the other anti-spam bots on how to ascend beyond a simple &ldquo;mute someone if they send more than N messages in M seconds&rdquo; filter and hopefully make better anti-spam bots so I don&rsquo;t have to.</p><h2 id=architecture>Architecture</h2><p>There are disagreements over exactly how an anti-spam bot should work, with a wide-range of preferred behaviors that differ between servers, depending on the community, size of the server, rate of growth of the server, and the administrators personal preferences. Many anti-spam solutions lock down the server by forcing new members to go through an airlock channel, but I consider this overkill and the result of poor anti-spam bots that are bad at actually stopping trolls or responding to raids.</p><p>My moderation architecture goes like this:<br><pre><code>-- Channels --  
#Moderation Channel  
#Raid Containment  
#Silence Containment  
#Log Channel

-- Roles --  
@Member  
@Silence  
@New</code></pre></p><p>When first-time setup is run on a server, the bot queries and stores all the permissions given to the <code>@everyone</code> role. It adds all these permissions to the <code>@Member</code> role, then goes through all existing users and adds the <code>@Member</code> role to them. It then <strong>removes all permissions from <code>@everyone</code></strong>, but adds an override for <code>#Raid Containment</code> so that anyone without the <code>@Member</code> role can only speak in <code>#Raid Containment</code>.</p><p>Then, it creates the <code>@Silence</code> role by adding permission overrides to every single channel on the server that prevent you from <em>sending</em> messages on that channel, except for the <code>#Silence Containment</code> channel. This ensures that, no matter what new roles might be added in the future, <code>@Silence</code> will always prevent you from sending messages on any channel other than <code>#Silence Containment</code>. The admins of the server can configure the visibility of the containment channels. Most servers make it so <code>#Raid Containment</code> is only visible to anyone without the <code>@Member</code> role, ensure <code>#Silence Containment</code> is only visible to anyone with the <code>@Silence</code> role, and hide <code>#Log Channel</code> from everyone except admins.</p><p>This architecture was chosen to allow the bot to survive a mega-raid, so that even if 500+ bot accounts storm the server all at once, if the bot goes down or is rate-limited, they can&rsquo;t do anything because they haven&rsquo;t been given the <code>@Member</code> role, and so the server is protected by default. This is essentially an automated airlock that only engages if the bot detects a raid, instead of forcing all new members to go through the airlock. Some admins do not like this approach because they prefer to personally audit every new member, but this is not viable for larger servers.</p><p>The <code>@Member</code> role being added can also short-circuit Discord&rsquo;s own verification rules, which is an unfortunate consequence, but in practice it usually isn&rsquo;t a problem. However, to help compensate for this, the bot can also be configured to add a temporary <code>@New</code> role to newer users that expires after a configurable amount of time. What this role actually does is up to the administrators - usually it simply disables sharing images or embeds.</p><h2 id=operation>Operation</h2><p>The bot has two distinct modes of operation. Under normal operation, when a new member joins the server, they are automatically given <code>@Member</code> (and <code>@New</code> if it&rsquo;s been enabled) and are immediately allowed to speak. If they trigger the anti-spam, or a moderator uses the <code>!silence</code> command, they will have the <code>@Silence</code> role added, any messages sent in the last 5 seconds (by default) will be deleted, and they&rsquo;ll be restricted to speaking in <code>#Silence Containment</code>. By default, silences that happen automatically are never rescinded until a moderator manually unsilences someone. However, some server admins are lazy, so an automatic expiration can be added. A manual silence can also be configured to expire after a set period of time. If a user triggers the anti-spam filter again <em>while they are in the containment channel</em>, then the bot will automatically ban them.</p><p>It&rsquo;s important to note that when silenced, users have <strong>both</strong> the <code>@Member</code> and the <code>@Silence</code> role. The reason is because many servers have opt-in channels that are only accessible if you add a role to yourself. Simply removing <code>@Member</code> would not suffice to keep them from talking in these channels, but <code>@Silence</code> can override these permissions and ensure they can&rsquo;t speak in any channel without having to mess up anyone&rsquo;s assigned roles.</p><p>The bot detects a raid if <code>N</code> joins happen within <code>M</code> seconds. It&rsquo;s usually configured to be something like <code>3</code> joins in a <code>90</code> second interval for an active server. When this happens, the server enters <strong>raid mode</strong>, which automatically removes <code>@Member</code> from all the users that just triggered the raid alert. It sends a message pinging the moderators in <code>#Moderator Channel</code> and stops adding <code>@Member</code> to anyone who joins while it is still in raid mode. It also changes the server verification level, which actually does have an effect for the newly joined members, because they haven&rsquo;t had any roles assigned to them yet. Moderators can either cancel the raid alert, which automatically adds <code>@Member</code> to everyone who was detected as part of the raid alert, or they can individually add <code>@Member</code> to people they have vetted. The raid mode automatically ends after <code>M*2</code> seconds, which would be <code>180</code> in this example. A moderator can use a command to ban everyone who joined during the raid alert, or they can selectively add <code>@Member</code> to users who should be let in.</p><p>This method of dealing with raids ensures that the bot cannot be overwhelmed by the sheer number of joins. If it crashes or is taken down, the server stays in raid mode until the bot can recover, and the potential raiders will not be allowed in.</p><h2 id=pressure-system>Pressure System</h2><p>All the anti-spam logic in the bot is done by triggering an automatic silence, which gives someone the <code>@Silence</code> role. The bot determines when to silence someone by analyzing all the messages being sent using a pressure system. In essence, the pressure system is analogous to the gravity function used for calculating the &ldquo;hotness&rdquo; of a given post on a site like <a href=https://medium.com/hacking-and-gonzo/how-hacker-news-ranking-algorithm-works-1d9b0cf2c08d>Hacker News</a>.</p><p>Each message a user sends is assigned a &ldquo;pressure score&rdquo;. This is calculated from the message length, contents, whether it matches a filter, how many newlines it has, etc. This &ldquo;pressure&rdquo; is designed to simulate how <em>disruptive</em> the message is to the current chat. A wall of text is extremely disruptive, whereas saying &ldquo;hi&rdquo; probably isn&rsquo;t. Attaching 3 images to your message is very disruptive, but embedding a single link isn&rsquo;t that bad. Sending a blank message with 2000 newlines is <em>incredibly</em> disruptive, whereas sending a message with two paragraphs is probably fine. In addition, all messages are assigned a <em>base pressure</em>, the minimum amount of pressure that any message will have, even if it&rsquo;s only a single letter.</p><p>My bot looks at the following values:</p><ul><li><strong>Max Pressure</strong>: The default maximum pressure is 60.</li><li><strong>Base Pressure</strong>: All messages generate 10 pressure by default, so at most 6 messages can be sent at once.</li><li><strong>Embed Pressure</strong>: Each image, link, or attachment in a message generates 8.3 additional pressure, which limits users to 5 images or links in a single message.</li><li><strong>Length Pressure</strong>: Each individual character in a message generates 0.00625 additional pressure, which limits you to sending 2 maximum length messages at once.</li><li><strong>Line Pressure</strong>: Each newline in a message generates 0.714 additional pressure, which limits you to 70 newlines in a single message.</li><li><strong>Ping Pressure</strong>: Every single <em>unique</em> ping in a message generates 2.5 additional pressure, which limits users to 20 pings in a single message.</li><li><strong>Repeat Pressure</strong>: If the message being sent is the <em>exact</em> same as the previous message sent by this user (copy+pasted), it generates 10 additional pressure, effectively doubling the base pressure cost for the message. This means a user copy and pasting the same message more than twice in rapid succession will be silenced.</li><li><strong>Filter Pressure</strong>: Any filter set by the admins can add an arbitrary amount of additional pressure if a message triggers that filter regex. The amount is user-configurable.</li></ul><p>And here a simplified version of my implementation. Note that I am adding the pressure piecewise, so that I can alert the moderators and tell them exactly which pressure trigger broke the limit, which helps moderators tell if someone just posted too many pictures at once, or was spamming the same message over and over:<pre class=language-go><code>if w.AddPressure(info, m, track, info.Config.Spam.BasePressure) {
	return true
}
if w.AddPressure(info, m, track, info.Config.Spam.EmbedPressure*float32(len(m.Attachments))) {
	return true
}
if w.AddPressure(info, m, track, info.Config.Spam.EmbedPressure*float32(len(m.Embeds))) {
	return true
}
if w.AddPressure(info, m, track, info.Config.Spam.LengthPressure*float32(len(m.Content))) {
	return true
}
if w.AddPressure(info, m, track, info.Config.Spam.LinePressure*float32(strings.Count(m.Content, &#34;\n&#34;))) {
	return true
}
if w.AddPressure(info, m, track, info.Config.Spam.PingPressure*float32(len(m.Mentions))) {
	return true
}
if len(m.Content) &gt; 0 &amp;&amp; m.Content == track.lastcache {
	if w.AddPressure(info, m, track, info.Config.Spam.RepeatPressure) {
		return true
	}
}</code></pre>Once we&rsquo;ve calculated how disruptive a given message is, we can add it to the user&rsquo;s total pressure score, which is a measure of how disruptive a user is currently being. However, we need to recognize that sending a wall of text every 10 minutes probably isn&rsquo;t an issue, but sending 10 short messages saying &ldquo;ROFLCOPTER&rdquo; in 10 seconds is definitely spamming. So, before we add the message pressure to the user&rsquo;s pressure, we check how long it&rsquo;s been since that user last sent a message, and <em>decrease their pressure accordingly</em>. If it&rsquo;s only been a second, the pressure shouldn&rsquo;t decrease very much, but if it&rsquo;s been 3 minutes or so, any pressure they used to have should probably be gone by now. Most heat algorithms do this non-linearly, but my experiments showed that a linear drop-off tends to produce better results (and is simpler to implement).</p><p>My bot implements this by simply decreasing the amount of pressure a user has by a set amount for every <code>N</code> seconds. So if it&rsquo;s been 2 seconds, they will lose <code>4</code> pressure, until it reaches zero. Here is the implementation for my bot, which is done before any pressure is added:<br><pre class=language-go><code>timestamp := bot.GetTimestamp(m)
track := w.TrackUser(author, timestamp)
last := track.lastmessage
track.lastmessage = timestamp.Unix()*1000 + int64(timestamp.Nanosecond()/1000000)
if track.lastmessage &lt; last { // This can happen because discord has a bad habit of re-sending timestamps if anything so much as touches a message
	track.lastmessage = last
	return false // An invalid timestamp is never spam
}
interval := track.lastmessage - last

track.pressure -= info.Config.Spam.BasePressure * (float32(interval) / (info.Config.Spam.PressureDecay * 1000.0))
if track.pressure &lt; 0 {
	track.pressure = 0
}</code></pre>In essence, this entire system is a superset of the more simplistic &ldquo;<code>N</code> messages in <code>M</code> seconds&rdquo;. If you only use base pressure and maximum pressure, then these determine the absolute upper limit of how many messages can be send in a given time period, regardless of their content. You then tweak the rest of the pressure values to more quickly catch obvious instances of spamming. The maximum pressure can be altered on a per-channel basis, which allows meme channels to spam messages without triggering anti-spam. Here&rsquo;s what a user&rsquo;s pressure value looks like over time:</p><div class=imgwrap style=max-width:481px><a href=/img/pressure.png target=_blank><img src=/img/pressure.png alt="Pressure Graph" width=100%></a></div><p>Because this whole pressure system is integrated into the Regex filtering module, servers can essentially create their own bad-word filters by assigning a huge pressure value to a certain match, which instantly creates a silence. These regexes can be used to look for other things that the bot doesn&rsquo;t currently track, like all-caps messages, or for links to specific websites. The pressure system allows integrating all of these checks in a unified way that represents the total &ldquo;disruptiveness&rdquo; of an individual user&rsquo;s behavior.</p><h2 id=worst-case-scenario>Worst Case Scenario</h2><p>A special mention should go to the uncommon but possible worst-case scenario for spamming. This happens when a dedicated attacker really, really wants to fuck up your server for some reason. They can do this by creating 1000+ fake accounts with randomized names, and wait until they&rsquo;ve all been on discord for long enough that the &ldquo;new to discord&rdquo; message doesn&rsquo;t show up. Then, they have the bots join the server <em>extremely slowly</em>, at the pace of maybe 1 per hour. Then they wait another week or so, before they unleash a spam attack that has each account send exactly 1 message, followed by a different account sending another message.</p><p>If they do this fast enough, you can detect it simply by the sheer volume of messages being sent in the channel, and automatically put the channel in slow mode. However, in principle, it is completely impossible to automatically deal with this attack. Banning everyone who happens to be sending messages during the spam event will ban innocent bystanders, and if the individual accounts aren&rsquo;t sending messages that fast (maybe one per second), this is indistinguishable from a normal rapid-fire conversation.</p><p>My bot has an emergency method for dealing with this where it tracks when a given user sent their first message in the server. You can then use a command to ban everyone who sent their first message in the past <code>N</code> seconds. This is very effective when it works, but it is trivial for spammers to get past once they realize what&rsquo;s going on - all they need to do is have their fake accounts say &ldquo;hi&rdquo; once while they&rsquo;re trickling in. Of course, a bunch of accounts all saying &ldquo;hi&rdquo; and then nothing else may raise enough suspicion to catch the attack before it happens.</p><h2 id=conclusion>Conclusion</h2><p>Hopefully this article demonstrates how to make a reliable, extensible anti-spam bot that can deal with pretty much any imaginable spam attack. The code for my deprecated spam bot is <a href=https://github.com/ErikMcClure/sweetiebot>available here</a> for reference, but the code is terrible and most of it was a bad idea. You cannot add the bot itself to a server anymore, and the support channel is no longer accessible.</p><p>My hope is that someone can use these guidelines to build a much more effective anti-spam bot, and release me from my torment. I hate web development and I&rsquo;d rather spend my time building a native webassembly compiler instead of worrying about <a href=https://github.com/bwmarrin/discordgo/issues/659>weird intermittent cloudflare errors</a> that temporarily break everything, or <a href=https://github.com/bwmarrin/discordgo/issues/513>badly designed lock systems</a> that deadlock under rare edge cases. Godspeed, bot developers.</p></article></div></section><section><div class=dim><aside><h2>Name Shadowing Should Be An Operator</h2><ul></ul></aside><article><p>I recently discovered that in Rust, this is a relatively common operation:<pre class=language-rust><code>let foo = String::from(&#34;foo&#34;);
// stuff that needs ownership
let foo = &amp;foo;  
</code></pre>Or this:<pre class=language-rust><code>let mut vec = Vec::new();
vec.push(&#34;a&#34;);
vec.push(&#34;b&#34;);
let vec = vec; /* vec is immutable now */  
</code></pre>This is a particularly permissive form of <strong>name-shadowing</strong>, which allows you to re-declare a variable in an inner scope that <em>shadows</em> the name of a variable in an outer scope, making the outer variable inaccessible. Almost every single programming language in common use allows you to do this in some form or another. Rust goes a step further and lets you re-declare a variable <em>inside the same scope</em> as another variable.</p><p>This, to me, is pretty terrifying, because name-shadowing itself is often a dangerous operation. 90% of the time, name-shadowing causes problems with either temporary or index variables, such as <code>i</code>. These are all cases where almost you never want to name-shadow anything and doing so is probably a mistake.<pre class=language-cpp><code>for(int i = 0; i &lt; width; ++i)
{
  for(int j = 0; j &lt; height; ++j)
  {
    //
    // lots of unrelated code
    //
    
    float things[4] = {1,2,3,4};
    for(int i = 0; i &lt; 4; ++i)
    	box[i][j] += things[i] // oops!
    //      ^ that is the wrong variable!
  }
}
</code></pre>These errors almost always crop up in complex for loop scenarios, which can obviously be avoided with iterators, but this isn&rsquo;t always an option, espiecally in a lower-level systems programming language like Rust. Even newer languages like Go make it alarmingly easy to name-shadow, although they make it a lot harder to accidentally shoot yourself because unused variables are a compiler error:<pre class=language-rust><code>foo, err := func()
if err != nil {
	err := bar(foo) // error: err is not used
}
println(err)
</code></pre>Unfortunately, Rust doesn&rsquo;t have this error, only an <code>unused-variables</code> linter warning. If you want, you can add <code>#![deny(clippy::shadow_unrelated)]</code> to be warned about name-shadowing. However, a lot of Rust idioms <em>depend</em> on the ability to name-shadow, because Rust does a lot of type-reassignment, where the contents of a variable don&rsquo;t change, but in a particular scope, the known type of the variable has changed to something more specific.<pre class=language-rust><code>let foo = Some(5);
match foo {
    Some(foo) =&gt; println!(&#34;{}&#34;, foo),
    None =&gt; {},
}
</code></pre>Normally, in C++, I avoid name-shadowing at all costs, but this is partially because C++ shadowing rules are unpredictable or counter-intuitive. Rust, however, seems to have legitimate use cases for name-shadowing, which would be reasonable if name-shadowing could be made more explicit.</p><p>I doubt Rust would want to change it&rsquo;s syntax, but if I were to work on a new language, I would define an explicit <strong>name-shadowing operator</strong>, either as a keyword or as an operator. This operator would redefine a variable as a new type and throw a compiler error if there is no existing variable to redefine. Attempting to redefine a variable without this operator would also throw a compiler error, so you&rsquo;d have something like:<pre class=language-rust><code>let foo = String::from(&#34;foo&#34;);
// stuff that needs ownership
foo := &amp;foo;
</code></pre>Or alternatively:<pre class=language-rust><code>let mut vec = Vec::new();
vec.push(&#34;a&#34;);
vec.push(&#34;b&#34;);
shadow vec = vec; /* vec is immutable now */
</code></pre>While the <code>:=</code> operator is cleaner, the <code>shadow</code> keyword would be easier to embed in other constructions:<pre class=language-rust><code>let foo = Some(5);
match foo {
    Some(shadow foo) =&gt; println!(&#34;{}&#34;, foo),
    None =&gt; {},
}
</code></pre>Which method would depend on the idiomatic constructions in the language syntax, but by making name-shadowing an explicit, rather than an implicit action, this allows you to get the benefits of name-shadowing while eliminating most of the dangerous situations it can create.</p><p>Unfortunately, most modern language design seems hostile to any feature that even slightly inconveniences a developer for the sake of code safety and reliability. Perhaps a new language in the future will take these lessons to heart, but in the meantime, people will continue complaining about unstable software, at least until we put the &ldquo;engineer&rdquo; back in &ldquo;software engineering&rdquo;.</p></article></div></section><section><div class=dim><aside><h2>A Rant On Terra</h2><ul></ul></aside><article><p>Metaprogramming, or the ability to inspect, modify and generate code at compile-time (as opposed to reflection, which is runtime introspection of code), has slowly been gaining momentum. Programmers are finally admitting that, after <a href=https://en.wikipedia.org/wiki/Template_metaprogramming>accidentally inventing turing complete template systems</a>, maybe we should just have proper first-class support for generating code. <a href=https://doc.rust-lang.org/1.2.0/book/macros.html>Rust has macros</a>, Zig has <a href=https://ziglang.org/documentation/master/#Compile-Time-Expressions>built-in compile time expressions</a>, Nim lets you <a href=https://nim-lang.org/docs/macros.html>rewrite the AST</a> however you please, and <a href=https://en.wikipedia.org/wiki/Dependent_type>dependent types</a> have been cropping up all over the place. However, with great power comes <del>great responsibility</del> undecidable type systems, whose undefined behavior may involve summoning eldritch abominations from the Black Abyss of Rěgne Ūt.</p><p>One particular place where metaprogramming is particularly useful is low-level, high-performance code, which is what <a href=http://terralang.org/>Terra</a> was created for. The idea behind Terra is that, instead of crafting ancient runes inscribed with infinitely nested variadic templates, just replace the whole thing with an actual turing-complete language, like say, <a href=https://www.lua.org/>Lua</a> (technically including <a href=http://luajit.org/>LuaJIT</a> extensions for FFI). This all sounds nice, and no longer requires a circle of salt to ward off demonic syntax, which Terra is quick to point out. They espouse the magical wonders of replacing your metaprogramming system with an actual scripting language:</p><blockquote><p>In Terra, we just gave in to the trend of making the meta-language of C/C++ more powerful and replaced it with a real programming language, Lua.</p><p>The combination of a low-level language meta-programmed by a high-level scripting language allows many behaviors that are not possible in other systems. Unlike C/C++, Terra code can be JIT-compiled and run interleaved with Lua evaluation, making it easy to write software libraries that depend on runtime code generation.</p><p>Features of other languages such as conditional compilation and templating simply fall out of the combination of using Lua to meta-program Terra</p></blockquote><p>Terra even claims you can implement Java-like OOP inheritance models as libraries and drop them into your program. It may also cure cancer (the instructions were unclear).</p><blockquote><p>As shown in the templating example, Terra allows you to define methods on struct types but does not provide any built-in mechanism for inheritance or polymorphism. Instead, normal class systems can be written as libraries. More information is available in our PLDI Paper.</p><p>The file lib/javalike.t has one possible implementation of a Java-like class system, while the file lib/golike.t is more similar to Google’s Go language.</p></blockquote><p>I am here to warn you, traveler, that <strong>Terra sits on a throne of lies</strong>. I was foolish. I was taken in by their audacious claims and fake jewels. It is only when I finally sat down to dine with them that I realized I was surrounded by nothing but cheap plastic and slightly burnt toast.</p><h2 id=the-bracket-syntax-problem>The Bracket Syntax Problem</h2><p>Terra exists as a syntax extension to Lua. This means it adds additional keywords on top of Lua&rsquo;s existing grammar. Most languages, when extending a syntax, would go to great lengths to ensure the new grammar does not create any ambiguities or otherwise interfere with the original syntax, treating it like a delicate flower that mustn&rsquo;t be disturbed, lest it lose a single petal.</p><p>Terra takes the flower, gently places it on the ground, and then stomps on it, repeatedly, until the flower is nothing but a pile of rubbish, as dead as the dirt it grew from. Then it sets the remains of the flower on fire, collects the ashes that once knew beauty, drives to a nearby cliffside, and throws them into the uncaring ocean. It probably took a piss too, but I can&rsquo;t prove that.</p><p>To understand why, one must understand what the escape operator is. It allows you to splice an abstract AST generated from a Lua expression directly into Terra code. Here is an example from Terra&rsquo;s website:<pre class=language-lua><code>function get5()
  return 5
end
terra foobar()
  return [ get5() + 1 ]
end
foobar:printpretty()
&gt; output:
&gt; foobar0 = terra() : {int32}
&gt; 	return 6
&gt; end</code></pre>But, wait, that means it&rsquo;s&mldr; the same as the array indexing operator? You don&rsquo;t mean you just put it inside like&ndash;</p><pre class=language-lua><code>local rest = {symbol(int),symbol(int)}

terra doit(first : int, [rest])
  return first + [rest[1]] + [rest[2]]
end</code></pre><p>What.</p><p><strong><em>WHAT?!</em></strong></p><p>You were supposed to banish the syntax demons, not join them! This <em>abomination</em> is an insult to Nine Kingdoms of Asgard! It is the very foundation that Satan himself would use to unleash Evil upon the world. Behold, mortals, for I come as the harbinger of <em>despair</em>:</p><pre class=language-lua><code>function idx(x) return `x end
function gen(a, b) return `array(a, b) end

terra test()
  -- Intended to evaluate to array(1, 2) 0
  return [gen(1, 2)][idx(0)]
end</code></pre><p>For those of you joining us (probably because you heard a blood-curdling scream from down the hall), this syntax is exactly as ambiguous as you might think. Is it two splice statements put next to each other, or is a splice statement with an array index? You no longer know if a splice operator is supposed to index the array or act as a splice operator, as <a href=https://github.com/StanfordLegion/legion/issues/522>mentioned in this issue</a>. Terra &ldquo;resolves this&rdquo; by just assuming that any two bracketed expressions put next to each other are <em>always</em> an array indexing operation, which is a lot like fixing your server overheating issue by running the fire suppression system all day. However, because this is Lua, whose syntax is very much like a delicate flower that cannot be disturbed, a much worse ambiguity comes up when we try to fix this.</p><pre class=language-lua><code>function idx(x) return `x end
function gen(a, b) return `array(a, b) end

terra test()
  -- This is required to make it evaluate to array(1,2)[0]
  -- return [gen(1, 2)][ [idx(0)] ]
  -- This doesn&#39;t work:
  return [gen(1, 2)][[idx(0)]]
  -- This is equivalent to:
  -- return [gen(1, 2)] &#34;idx(0)&#34;
end</code></pre><p>We want to use a spliced Lua expression as the array index, but if we don&rsquo;t use any spaces, <em>it turns into a string</em> because <code>[[string]]</code> is the Lua syntax for an unescaped string! Now, those of you who still possess functioning brains may believe that this would always result in a syntax error, as we have now placed a string next to a variable. <strong>Not so!</strong> Lua, in it&rsquo;s infinite wisdom, converts anything of the form <code>symbol"string"</code> or <code>symbol[[string]]</code> into a <strong>function call</strong> with the string as the only parameter. That means that, in certain circumstances, we literally attempt to <em>call our variable as a function with our expression as a string</em>:</p><pre class=language-lua><code>local lookups = {x = 0, y = 1, z = 2, w = 3 };
  vec.metamethods.__entrymissing = macro(function(entryname, expr)
    if lookups[entryname] then
      -- This doesn&#39;t work
      return `expr.v[[lookups[entryname]]]
      -- This is equivalent to
      -- return `expr.v &#34;lookups[entryname]&#34;
      -- But it doesn&#39;t result in a syntax error, becase it&#39;s equivalent to:
      -- return `extr.v(&#34;lookups[entryname]&#34;)
    else
      error &#34;That is not a valid field.&#34;
    end
  end)</code></pre><p>As a result, you get a <em>type error</em>, not a syntax error, and a very bizarre one too, because it&rsquo;s going to complain that <code>v</code> isn&rsquo;t a function. This is like trying to bake pancakes for breakfast and accidentally going scuba diving instead. It&rsquo;s not a sequence of events that should ever be related in any universe that obeys causality.</p><p>It should be noted that, after a friend of mine heard my screams of agony, <a href=https://github.com/zdevito/terra/issues/385>an issue was raised</a> to change the syntax to a summoning ritual that involves less self-mutilation. Unfortunately, this is a breaking change, and will probably require an exorcism.</p><h2 id=the-documentation-is-wrong>The Documentation Is Wrong</h2><p>Terra&rsquo;s documentation is so wrong that it somehow manages to be wrong in both directions. That is, some of the documentation is out-of-date, while some of it refers to concepts that never made it into <code>master</code>. I can only assume that a time-traveling gremlin was hired to write the documentation, who promptly got lost amidst the diverging timelines. It is a quantum document, both right and wrong at the same time, yet somehow always useless, a puzzle beyond the grasp of modern physics.</p><ul><li>The first thing talked about in the <a href=http://terralang.org/api.html#lua-api>API Reference</a> is a List object. It does not actually exist. A <a href=https://github.com/zdevito/terra/blob/aa19f1ab6e70f9710a3d140219f100cc75e59462/src/asdl.lua#L1>primitive incarnation of it</a> does exist, but it only implements <code>map()</code> and <code>insertall()</code>. Almost the entire section is completely wrong for the <code>1.0.0-beta1</code> release. The actual List object being described <a href=https://github.com/zdevito/terra/blob/develop/src/terralist.lua>sits alone and forgotten</a> in the develop branch, dust already beginning to collect on it&rsquo;s API calls, despite those API calls being the ones in the documentation&mldr; somehow.</li><li><code>:printpretty()</code> is a function that prints out a pretty string representation of a given piece of Terra code, by parsing the AST representation. On it&rsquo;s face, it does do exactly what is advertised: it prints a string. However, one might assume that it <em>returns</em> the string, or otherwise allows you to do something with it. This doesn&rsquo;t happen. It literally calls the <code>print()</code> function, throwing the string out the window and straight into the <code>stdout</code> buffer without a care in the world. If you want the <em>actual string</em>, you must call either <code>layoutstring()</code> (for types) or <code>prettystring()</code> (for quotes). Neither function is documented, anywhere.</li><li>Macros can only be called from inside Terra code. Unless you give the constructor two parameters, where the second parameter is a function called from inside a Lua context. This behavior is not mentioned in any documentation, anywhere, which makes it even more confusing when someone defines a macro as <code>macro(myfunction, myfunction)</code> and then calls it from a Lua context, which, according to the documentation, should be impossible.</li><li>Struct fields are not specified by their name, but rather just held in a numbered list of {name, type} pairs. This <em>is</em> documented, but a consequence of this system is not: Struct field names do not have to be unique. They can all be the same thing. Terra doesn&rsquo;t actually care. You can&rsquo;t actually be sure that any given field name lookup will result in, y&rsquo;know, <em>one field</em>. Nothing mentions this.</li><li>The documentation for saveobj is a special kind of infuriating, because everything is <em>technically</em> correct, yet it does not give you any examples and instead simply lists a function with 2 arguments and 4 interwoven optional arguments. In reality it&rsquo;s absolutely trivial to use because you can ignore almost all the parameters. Just write <code>terralib.saveobj("blah", {main = main})</code> and you&rsquo;re done. But there isn&rsquo;t a <em>single example of this</em> anywhere on the entire website. Only a paragraph and two sentences explaining in the briefest way possible how to use the function, followed by a highly technical example of how to initialize a custom target parameter, which <em>doesn&rsquo;t actually compile</em> because it has errant semicolons. This is literally <em>the most important function</em> in the entire language, because it&rsquo;s what actually compiles an executable!</li><li>The <code>defer</code> keyword is critical to being able to do proper error cleanup, because it functions similar to Go&rsquo;s defer by performing a function call at the end of a lexical scope. It is not documented, anywhere, or even mentioned at all on the website. How Terra manages to implement new functionality it forgets to document while, <em>at the same time</em>, documenting functionality <em>that doesn&rsquo;t exist yet</em> is a 4-dimensional puzzle fit for an extra-dimensional hyperintelligent race of aliens particularly fond of BDSM.</li><li>You&rsquo;d think that compiling Terra on Linux would be a lot simpler, but <a href=https://github.com/zdevito/terra/issues/393>you&rsquo;d be wrong</a>. Not only are the makefiles unreliable, but cmake itself doesn&rsquo;t seem to work with LLVM 7 unless you pass in <a href=https://github.com/zdevito/terra/blob/master/.travis.yml#L22>a very specific set of flags</a>, none of which are documented, because compiling via cmake <strong>isn&rsquo;t documented at all</strong>, and this is the <em>only way</em> to compile with LLVM 7 or above on the latest Ubuntu release!</li></ul><p>Perhaps there are more tragedies hidden inside this baleful document, but I cannot know, as I have yet to unearth the true depths of the madness lurking within. I am, at most, on the third or fourth circle of hell.</p><h2 id=terra-doesnt-actually-work-on-windows>Terra Doesn&rsquo;t Actually Work On Windows</h2><p>Saying that Terra supports Windows is a statement fraught with danger. It is a statement so full of holes that an entire screen door could try to sell you car insurance and it&rsquo;d still be a safer bet than running Terra on Windows. Attempting to use Terra on Windows will work if you have Visual Studio 2015 installed. It <em>might</em> work if you have Visual Studio 2013 installed. No other scenarios are supported, especially not ones that involve being productive. Actually <em>compiling</em> Terra on Windows is a hellish endeavor comparable to climbing Mount Everest in a bathing suit, which requires either having Visual Studio 2015 installed <em>to the default location</em>, or manually modifying a Makefile with the exact absolute paths of all the relevant dependencies. At least up until last week, when I <a href=https://github.com/zdevito/terra/pull/400>submitted a pull request</a> to minimize the amount of mountain climbing required.</p><p>The problem Terra runs into is that it tries to use a registry value to find the location of Visual Studio and then work out where <code>link.exe</code> is from there, then finds the include directories for the C runtime. This hasn&rsquo;t worked since Visual Studio 2017 and also requires custom handling for each version because compiling an iteration of Visual Studio apparently involves throwing the directory structure into the air, watching it land on the floor in a disorganized mess, and drawing lines between vaguely related concepts. Good for divining the true nature of the C library, bad for building directory structures. Unfortunately, should you somehow manage to compile Terra, it will abruptly stop working the moment you try to call <code>printf</code>, claiming that <code>printf</code> does not actually exist, even after importing <code>stdio.h</code>.</p><p>Many Terra tests assume that <code>printf</code> actually resolves to a concrete symbol. <a href="https://social.msdn.microsoft.com/Forums/vstudio/en-US/5150eeec-4427-440f-ab19-aecb26113d31/updated-to-vs-2015-and-now-get-unresolved-external-errors?forum=vcgeneral">This is not true</a> and hasn&rsquo;t been true since Visual Studio 2015, which turned several <code>stdio.h</code> functions into inline-only implementations. In general, the C standard library is under no obligation to produce an actual concrete symbol for any function - or to make sense to a mere mortal, for that matter. In fact, it might be more productive to assume that the C standard was wrought from the unholy, broiling chaos of the void by Cthulhu himself, who saw fit to punish any being foolish enough to make reasonable assumptions about how C works.</p><p>Unfortunately, importing <code>stdio.h</code> does not fix this problem, for two reasons. One, Terra did not understand inline functions on Windows. They were ephemeral wisps, <a href=https://github.com/zdevito/terra/issues/401>vanishing like a mote of dust on the wind</a> the moment a C module was optimized. A <a href=https://github.com/zdevito/terra/pull/405>pull request fixed this</a>, but it can&rsquo;t fix the fact that the Windows SDK was wrought from the innocent blood of a thousand vivisected COMDAT objects. Microsoft&rsquo;s version of <code>stdio.h</code> can only be described as an extra-dimensional object, a meta-stable fragment of a past universe that can only be seen in brief slivers, never all at once.</p><p>Luckily for the Terra project, I am the demonic presence they need, for I was once a Microsoftie. Long ago, I walked the halls of the Operating Systems Group and helped craft black magic to sate the monster&rsquo;s unending hunger. I saw True Evil blossom in those dark rooms, like having only three flavors of sparkling water and a pasta station only open on Tuesdays.</p><p>I know the words of Black Speech that must be spoken to reveal the true nature of Windows. I know how to bend the rules of our prison, to craft a mighty workspace from the bowels within. After <a href=https://github.com/zdevito/terra/pull/322>fixing the cmake implementation</a> to function correctly on Windows, I intend to perform <a href=https://gist.github.com/tonetheman/522b623d00e64cb5feda5d68252fa68a>the unholy incantations</a> required to invoke the almighty powers of COM, so that it may find on which fifth-dimensional hyperplane Visual Studio exists. Only then can I disassociate myself from the mortal plane for long enough to <a href=https://github.com/zdevito/terra/issues/401#issuecomment-537245442>tackle the <code>stdio.h</code> problem</a>. You see, children, programming for Windows is easy! All you have to do is <strong>s͏̷E͏l͏̢҉l̷ ̸̕͡Y͏o҉u͝R̨͘ ̶͝sơ̷͟Ul̴</strong></p><p>For those of you who actually wish to try Terra, but don&rsquo;t want to wait for <del>me to fix everything</del> a new release, you can embed the following code at the top of your root Terra script:</p><pre class=language-lua><code>if os.getenv(&#34;VCINSTALLDIR&#34;) ~= nil then
  terralib.vshome = os.getenv(&#34;VCToolsInstallDir&#34;)
  if not terralib.vshome then
    terralib.vshome = os.getenv(&#34;VCINSTALLDIR&#34;)
    terralib.vclinker = terralib.vshome..[[BIN\x86_amd64\link.exe]]
  else
    terralib.vclinker = ([[%sbin\Host%s\%s\link.exe]]):format(terralib.vshome, os.getenv(&#34;VSCMD_ARG_HOST_ARCH&#34;), os.getenv(&#34;VSCMD_ARG_TGT_ARCH&#34;))
  end
  terralib.includepath = os.getenv(&#34;INCLUDE&#34;)

  function terralib.getvclinker()
    local vclib = os.getenv(&#34;LIB&#34;)
    local vcpath = terralib.vcpath or os.getenv(&#34;Path&#34;)
    vclib,vcpath = &#34;LIB=&#34;..vclib,&#34;Path=&#34;..vcpath
    return terralib.vclinker,vclib,vcpath
  end
end</code></pre><p>Yes, we are literally overwriting parts of the compiler itself, at runtime, from our script. <strong>Welcome to Lua!</strong> Enjoy your stay, and don&rsquo;t let the fact that any script you run could completely rewrite the compiler keep you up at night!</p><h2 id=the-existential-horror-of-terra-symbols>The Existential Horror of Terra Symbols</h2><p>Symbols are one of the most slippery concepts introduced in Terra, despite their relative simplicity. When encountering a Terra Symbol, one usually finds it in a function that looks like this:</p><pre class=language-lua><code>TkImpl.generate = function(skip, finish) return quote
    if [TkImpl.selfsym].count == 0 then goto [finish] end 
    [TkImpl.selfsym].count = [TkImpl.selfsym].count - 1
    [stype.generate(skip, finish)]
end end</code></pre><p>Where <code>selfsym</code> is a symbol that was set elsewhere.</p><p>&ldquo;Aha!&rdquo; says our observant student, &ldquo;a reference to a variable from an outside context!&rdquo; This construct <em>does</em> let you access a variable from another area of the same function, and using it to accomplish that will generally work as you expect, but what it&rsquo;s actually doing is much <del>worse</del> more subtle. You see, grasshopper, a symbol is not a reference to a variable node in the AST, it is a reference to an <em>identifier</em>.</p><pre class=language-lua><code>local sym = symbol(int)
local inc = quote [sym] = [sym] + 1 end

terra foo()
  var [sym] = 0
  inc
  inc
  return [sym]
end

terra bar()
  var[sym] = 0
  inc
  inc
  inc
  return [sym]
end</code></pre><p>Yes, that is valid Terra, and yes, the people who built this language did this on purpose. Why any human being still capable of love would ever design such a catastrophe is simply beyond me. Each symbol literally represents not a reference to a variable, but a <em>unique variable name</em> that will refer to any variable that has been initialized in the current Terra scope with that particular identifier. You aren&rsquo;t passing around variable <em>references</em>, you&rsquo;re passing around variable <em>names</em>.</p><p>These aren&rsquo;t just symbols, they&rsquo;re <strong>typed preprocessor macros</strong>. They are literally C preprocessor macros, capable of causing just as much woe and suffering as one, except that they are typed and they can&rsquo;t redefine existing terms. This is, admittedly, <em>slightly</em> better than a normal C macro. However, seeing as there have been entire books written about humanity&rsquo;s collective hatred of C macros, this is equivalent to being a slightly more usable programming language than Brainfuck. This is such a low bar it&rsquo;s probably buried somewhere in the Mariana Trench.</p><h2 id=terra-is-c-but-the-preprocessor-is-lua>Terra is C but the Preprocessor is Lua</h2><p>You realize now, the monstrosity we have unleashed upon the world? The sin Terra has committed now lies naked before us.</p><p><strong>Terra is C if you replaced the preprocessor with Lua.</strong></p><p>Remember how Terra says you can implement Java-like and Go-like class systems? You can&rsquo;t. Or rather, you will end up with a pathetic imitation, a facsimile of a real class system, striped down to the bone and bereft of any useful mechanisms. It is nothing more than an implementation of vtables, just like you would make in C. Because Terra is C. It&rsquo;s metaprogrammable C.</p><p>There can be no constructors, or destructors, or automatic initialization, or any sort of borrow checking analysis, because Terra has no scoping mechanisms. The only thing it provides is <code>defer</code>, which only operates inside Lua lexical blocks (<code>do</code> and <code>end</code>)&mldr; sometimes, if you get lucky. The exact behavior is a bit confusing, and of course can only be divined by random experimentation because it <strong>isn&rsquo;t documented anywhere!</strong> Terra&rsquo;s only saving grace, the <em>singular keyword</em> that allows you to attempt to build some sort of pretend object system, <strong>isn&rsquo;t actually mentioned anywhere</strong>.</p><p>Of course, Terra&rsquo;s metaprogramming <em>is</em> turing complete, and it is <em>technically possible</em> to implement some of these mechanisms, but only if you either wrap absolutely every single variable declaration in a function, or you introspect the AST and annotate every single variable with initialization statuses and then run a metaprogram over it to figure out when constructors or destructors or assignment operators need to be called. Except, this might not work, because the (undocumented, of course) <code>__update</code> metamethod that is supposed to trigger when you assign something to a variable has a bug where it&rsquo;s not always called in all situations. This turns catching assignments and finding the l-value or r-value status from a mind-bogglingly difficult, herculean task, to a near-impossible trial of cosmic proportions that probably requires the help of at least two Avengers.</p><h2 id=there-is-no-type-system>There Is No Type System</h2><p>If Terra was actually trying to build a metaprogramming equivalent to templates, it would have an actual type system. These languages already exist - <a href=https://www.idris-lang.org/>Idris</a>, <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.121.4251">Omega</a>, <a href=https://www.fstar-lang.org/>F*</a>, <a href=https://en.wikipedia.org/wiki/Ada_(programming_language)>Ada</a>, <a href=https://sage.soe.ucsc.edu/>Sage</a>, etc. but none of them are interested in using their dependent type systems to actually metaprogram low-level code (although <a href=https://github.com/FStarLang/kremlin/blob/master/src/CFlatToWasm.ml>F* can produce it</a>). The problem is that building a recursively metaprogrammable type system requires building a proof assistant, and everyone is so proud of the fact they built a proof assistant they forget that dependent type systems can do other things too, like build really fast memcpy implementations.</p><p>Terra, on the other hand, provides only the briefest glimpse of a type system. Terra functions enjoy what is essentially a slightly more complex C type system. However, the higher-level Lua context is, well, Lua, which has five basic types: Tables, Functions, Strings, Booleans and Numbers (it also has Thread, Nil, Userdata and CData for certain edge cases). That&rsquo;s it. Also, it&rsquo;s dynamic, not static, so everything is a syntax or a runtime error, because it&rsquo;s a scripting language. This means all your metaprogramming is sprinkled with type-verification calls like <code>:istype()</code> or <code>:isstruct()</code>, except the top came off the shaker and now the entire program is just sprinkles, everywhere. This is fine for when your metaprograms are, themselves, relatively simple. It is not fine when you are returning meta-programs out of meta-meta-functions.</p><p>This is the impasse I find myself at, and it is the answer to the question I know everyone wants to know the answer to. For the love of heaven and earth and all that lies between, <strong>why am I still using Terra?</strong></p><p>The truth is that the project I&rsquo;m working on requires highly complex metaprogramming techniques in order to properly generate type-safe mappings for arbitrary data structures. Explaining <em>why</em> would be an entire blog post on it&rsquo;s own, but suffice to say, it&rsquo;s a complex user interface library that&rsquo;s intended to run on tiny embedded devices, which means I can&rsquo;t simply give up and use Idris, or indeed anything that involves garbage collection.</p><p>What I really want is a low-level, recursively metaprogrammable language that is also recursively type-safe, in that any type strata can safely manipulate the code of any layer beneath it, preferably via <a href=https://www.cl.cam.ac.uk/~sd601/thesis.pdf>algebriac subtyping</a> that ensures all types are recursively a subset of types that contain them, ad nauseam. This would then allow you to move from a &ldquo;low-level&rdquo; language to a &ldquo;high-level&rdquo; language by simply walking up the tower of abstraction, building meta-meta-programs that manipulate meta-programs that generate low-level programs.</p><p>Alas, such beauty can only exist in the minds of mathematicians and small kittens. While I may one day attempt to build such a language, it will be nothing more than a poor imitation, forever striving for an ideal it cannot reach, cursed with a vision from the gods of a pristine language no mortal can ever possess.</p><p>I wish to forge galaxies, to wield the power of computation and sail the cosmos upon an infinite wave of creativity. Instead, I spend untold hours toiling inside LLVM, wondering why it won&rsquo;t print &ldquo;Hello World&rdquo;.</p><p>In conclusion, everything is terrible and the universe is on fire.</p></article></div></section><section><div class=dim><aside><h2>RISC Is Fundamentally Unscalable</h2><ul></ul></aside><article><p>Today, there was an announcement about <a href=https://twitter.com/Calista_Redmond/status/1154278392344305664>a new RISC-V chip</a>, which has got a lot of people excited. I wish I could also be excited, but to me, this is just a reminder that RISC architectures are fundamentally unscalable, and inevitably stop being RISC as soon as they need to be fast. People still call ARM a &ldquo;RISC&rdquo; architecture despite <a href=https://en.wikipedia.org/wiki/ARM_architecture#ARMv8.3-A>ARMv8.3-A adding a <code>FJCVTZS</code> instruction</a>, which is &ldquo;Floating-point Javascript Convert to Signed fixed-point, rounding toward Zero&rdquo;. Reduced instruction set, my ass.</p><p>The reason this keeps happening is because <strong>the laws of physics</strong> ensure that no RISC architecture can scale under load. The problem is that a modern CPU is so fast that just accessing the L1 cache takes anywhere from <a href=https://www.7-cpu.com/cpu/Skylake_X.html>3-5 cycles</a>. This is part of the reason modern CPUs rely so much on <a href=https://en.wikipedia.org/wiki/Register_renaming>register renaming</a>, allowing them to have hundreds of internal registers that are used to make things go fast, as opposed to the paltry <a href=https://en.wikipedia.org/wiki/X86#/media/File:Table_of_x86_Registers_svg.svg>90 registers actually exposed</a>, 40 of which are just floating point registers for vector operations. The fundamental issue that CPU architects run into is that <strong>the speed of light isn&rsquo;t getting any faster</strong>. Even getting an electrical signal from one end of a CPU to the other now takes more than one cycle, which means the physical layout of your CPU now has a significant impact on how fast operations take. Worse, the faster the CPU gets, the more this lag becomes a problem, so unless you shrink the entire CPU or redesign it so your L1 and L2 caches are <em>physically closer to the transistors that need them</em>, the latency from accessing those caches can only go up, not down. The CPU might be getting faster, but the speed of light isn&rsquo;t.</p><p>Now, obviously RISC CPUs are very complicated architectures that do <a href=https://en.wikipedia.org/wiki/Classic_RISC_pipeline#Hazard>all sorts of insane pipelining</a> to try and execute as many instructions at the same time as possible. This is necessary because, unless your data is already loaded into registers, you might spend more cycles <em>loading data from the L1 cache</em> than doing the actual operation! If you hit the L2 cache, that will cost you 13-20 cycles by itself, and L3 cache hits are 60-100 cycles. This is made worse by the fact that complex floating-point operations can almost always be performed faster by encoding the operation in hardware, often in just one or two cycles, when manually implementing the same operation would&rsquo;ve taken 8 or more cycles. The <code>FJCVTZS</code> instruction mentioned above even <a href=https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/armv8-a-architecture-2016-additions>sets a specific flag based on certain edge-cases</a> to allow an immediate jump instruction to be done afterwards, again to minimize hitting the cache.</p><p>All of this leads us to <strong>single instruction multiple data (SIMD)</strong> vector instructions common to almost all modern CPUs. Instead of doing a complex operation on a single float, they do a simple operation to many floats at once. The CPU can perform operations on 4, 8, or even 16 floating point numbers at the same time, in just 3 or 4 cycles, even though doing this for an individual float would have cost 2 or 3 cycles <em>each</em>. Even loading an array of floats into a large register will be faster than loading each float individually. There is no escaping the fact that attempting to run instructions one by one, even with fancy pipelining, will usually result in a CPU that&rsquo;s simply not doing anything most of the time. In order to make things go fast, you have to do things in bulk. This means having instructions that do as many things as possible, which is the exact opposite of how RISC works.</p><p>Now, this does not mean CISC is the future. We already invented a solution to this problem, which is <a href=https://en.wikipedia.org/wiki/Very_long_instruction_word>VLIW - Very Large Instruction Word</a>. This is what Itanium was, because researchers at HP anticipated this problem <a href=https://en.wikipedia.org/wiki/Itanium#Development:_1989%E2%80%932000>30 years ago</a> and teamed up with Intel to create what eventually became <a href=https://en.wikipedia.org/wiki/Itanium>Itanium</a>. In Itanium, or any VLIW architecture, you can tell the CPU to do <strong>many things at once</strong>. This means that, instead of having to build massive vector processing instructions or other complex specialized instructions, you can build your own mega-instructions out of a much simpler instruction set. This is great, because it simplifies the CPU design enormously while sidestepping the pipelining issues of RISC. The problem is that <em>this is really fucking hard to compile</em>, and that&rsquo;s what Intel screwed up. Intel assumed that compilers in 2001 could extract the instruction-level parallelism necessary to make VLIW work, but in reality we&rsquo;ve only <a href=https://arxiv.org/pdf/1902.02816.pdf>very recently figured out how to reliably do that</a>. 20 years ago, we weren&rsquo;t even close, so nobody could compile fast code for Itanium, and now Itanium is dead, even though it was specifically designed to solve our current predicament.</p><p>With that said, the <a href=https://en.wikipedia.org/wiki/Mill_architecture>MILL instruction set</a> uses VLIW along with several other innovations designed to compensate for a lot of the problems discussed here, like having <a href="https://www.youtube.com/watch?v=8E4qs2irmpc&amp;t=26m45s">deferred load instructions</a> to account for the lag time between requesting a piece of data and actually being able to use it (which, incidentally, also makes MILL immune to Spectre because it doesn&rsquo;t need to speculate). Sadly, MILL is currently still vaporware, having not materialized any actual hardware despite it&rsquo;s promising performance gains. One reason for this might be that any VLIW architecture has a highly unique instruction set. We&rsquo;re used to x86, which is so high-level it has almost nothing to do with the underlying CPU implementation. This is nice, because everyone implements the same instruction set and your programs all work on it, but it means the way instructions interact is hard to predict, much to the frustration of compiler optimizers. With VLIW, you would very likely have to recompile your program for <strong>every single unique CPU</strong>, which is a problem <a href=https://en.wikipedia.org/wiki/Mill_architecture#Family_traits>MILL has spent quite a bit of time on</a>.</p><p>MILL, and perhaps VLIW in general, may have a saving grace with <a href=https://webassembly.org/>WebAssembly</a>, precisely because it is a low-level assembly language that can be efficiently compiled to any architecture. It wouldn&rsquo;t be a problem to have unique instruction sets for every single type of CPU, because if you ship WebAssembly, you can simply compile the program for whatever CPU it happens to be running on. A lot of people miss this benefit of WebAssembly, even though I think it will be critical in allowing VLIW instruction sets to eventually proliferate. Perhaps MILL will see the light of day after all, or maybe someone else can come up with a VLIW version of RISC-V that&rsquo;s open-source. Either way, we need to stop pretending that pipelining RISC is going to work. It hasn&rsquo;t ever worked and it&rsquo;s not going to work, it&rsquo;ll just turn into another CISC with a javascript floating point conversion instruction.</p><p>Every. Single. Time.</p></article></div></section><section><div class=dim><aside><h2>Migrating To A Static Blog</h2><ul></ul></aside><article><p>I&rsquo;ve finished constructing a new personal website for myself using <a href=https://gohugo.io/>hugo</a>, and I&rsquo;m moving my blog over there so I have more control over what gets loaded, and more importantly, so the page doesn&rsquo;t attempt to load Blogger&rsquo;s 5 MB worth of bloated javascript nonsense just to read some text. It also fixes math and code highlighting while reading on mobile. If you reached this post using Blogger, you&rsquo;ll be redirected or will soon be redirected to the corresponding post on my new website.</p><p>All comments have been preserved from the original posts, but making new comments is currently disabled - I haven&rsquo;t decided if I want to use Disqus or attempt something else. An <a href=https://erikmcclure.com/index.xml><strong>RSS feed</strong></a> is available on the bottom of the page for tracking new posts that should mimic the Blogger RSS feed, if you were using that. If something doesn&rsquo;t work, <a href=https://twitter.com/erikmcclure0173><strong>poke me on twitter</strong></a> and I&rsquo;ll try to fix it.</p><p>I implemented share buttons with simple links, without embedding any crazy javascript bullshit. In fact, the only external resource loaded is a Google tracking ID for pageviews. Cloudflare is used to enforce an HTTPS connection over the custom domain even though the website is hosted on <a href=https://github.com/erikmcclure/erikmcclure.github.io>Github Pages</a>.</p><p>Hopefully, the new font and layout is easier to read than Blogger&rsquo;s tiny text and bullshit theme nonsense.</p></article></div></section><section><div class=dim><aside><h2>How To Avoid Memorizing Times Tables</h2><ul></ul></aside><article><p>I was recently told that my niece was trying to memorize her times tables. As an applied mathematician whose coding involves plenty of multiplication, I was not happy to hear this. Nobody who does math actually memorizes times tables, and furthermore, forcing a child to memorize <em>anything</em> is probably the worst possible thing you can do in modern society. No one should <em>memorize</em> their times tables, they should learn how to <em>calculate them</em>. Forcing children to memorize useless equations for no reason is a great way to either ensure they hate math, teach them they should blindly memorize and believe anything adults tell them, or both. So for any parents who wish to teach their children how to be critical thinkers and give them an advantage on their next math test, I am going to describe how to derive the entire times tables with only 12 rules.</p><ol><li><p>Anything multiplied by 1 is itself. Note that I said <em>anything</em>, that includes fractions, pies, cars, the moon, or anything else you can think of. Multiplying it by 1 just gives you back the same result.</p></li><li><p>Any number multiplied by 10 has a zero added on the end. 1 becomes 10, 2 becomes 20, 72 becomes 720, 9999 becomes 99990, etc.</p></li><li><p>Any single digit multiplied by 11 simply adds itself on the end instead of 0. 1 becomes 11, 2 becomes 22, 5 becomes 55, etc. This is because you never need to multiply something by eleven. Instead, multiply it by 10 (add a zero to it) then add itself.<div class=math>\[ \begin{aligned} 11*11 = 11*(10 + 1) = 11*10 + 11 = 110 + 11 = 121\\ 12*11 = 12*(10 + 1) = 12*10 + 12 = 120 + 12 = 132 \end{aligned} \]</div></p></li><li><p>You can always reverse the numbers being multiplied and the same result comes out. <span class=math>$$ 12*2 = 2*12 $$</span>, <span class=math>$$ 8*7 = 7*8 $$</span>, etc. This is a simple rule, but it&rsquo;s very easy to forget, so keep it in mind.</p></li><li><p>Anything multiplied by 2 is doubled, or added to itself, but you only need to do this up to 9. For example, <span class=math>$$ 4*2 = 4 + 4 = 8 $$</span>. Alternatively, you can count up by 2 that many times:<div class=math>\[ 4*2 = 2 + 2 + 2 + 2 = 4 + 2 + 2 = 6 + 2 = 8 \]</div>To multiply any large number by two, double each individual digit and <em>carry the result</em>. Because you multiply each digit by 2 separately, the highest result you can get from this is 18, so you will only ever carry a 1, just like in addition.<div class=math>\[ \begin{aligned} \begin{matrix} 3 & 6\\ & 2\\ \hline & \\ & \\ \hline &  \end{matrix}\quad \begin{matrix} 3 & 6\\ & 2\\ \hline 1 & 2\\ & \\ \hline &  \end{matrix}\quad \begin{matrix} 3 & 6\\ & 2\\ \hline 1 & 2\\ 6 & \\ \hline &  \end{matrix}\quad \begin{matrix} 3 & 6\\ & 2\\ \hline 1 & 2\\ 6 & \\ \hline 7 & 2 \end{matrix} \end{aligned}  \]</div>This method is why multiplying anything by 2 is one of the easiest operations in math, and as a result the rest of our times table rules are going to rely heavily on it. Don&rsquo;t worry about memorizing these results - you&rsquo;ll memorize them whether you want to or not simply because of how often you use them.</p></li><li><p>Any number multiplied by 3 is multiplied by 2 and then added to itself. For example:<div class=math>\[ 6*3 = 6*(2 + 1) = 6*2 + 6 = 12 + 6 = 18 \]</div>Alternatively, you can add the number to itself 3 times: <span class=math>$$ 3*3 = 3 + 3 + 3 = 6 + 3 = 9 $$</span></p></li><li><p>Any number multiplied by 4 is simply multiplied by 2 twice. For example: <span class=math>$$ 7*4 = 7*2*2 = 14*2 = 28 $$</span></p></li><li><p>Any number multiplied by 5 is the same number multiplied by 4 and then added to itself.<div class=math>\[ 6*5 = 6*(4 + 1) = 6*4 + 6 = 6*2*2 + 6 = 12*2 + 6 = 24 + 6 = 30 \]</div>Note that I used our rule for 4 here to break it up and calculate it using only 2. Once kids learn division, they will notice that it is often easier to calculate 5 by multiplying by 10 and halving the result, but we assume no knowledge of division.</p></li><li><p>Any number multiplied by 8 is multiplied by 4 and then by 2, which means it&rsquo;s actually just multiplied by 2 three times. For example: <span class=math>$$ 7*8 = 7*4*2 = 7*2*2*2 = 14*2*2 = 28*2 = 56 $$</span></p></li><li><p>Never multiply anything by 12. Instead, multiply it by 10, then add itself multiplied by 2. For example: <span class=math>$$ 12*12 = 12*(10 + 2) = 12*10 + 12*2 = 120 + 24 = 144 $$</span></p></li><li><p>Multiplying any single digit number by 9 results in a number whose digits always add up to nine, and whose digits decrease in the right column while increasing in the left column.<div class=math>\[ \begin{aligned} 9 * 1 = 09\\ 9 * 2 = 18\\ 9 * 3 = 27\\ 9 * 4 = 36\\ 9 * 5 = 45\\ 9 * 6 = 54\\ 9 * 7 = 63\\ 9 * 8 = 72\\ 9 * 9 = 81 \end{aligned} \]</div>10, 11, and 12 can be calculated using rules for those numbers.</p></li><li><p>For both 6 and 7, we already have rules for all the other numbers, so you just need to memorize 3 results:<div class=math>\[ \begin{aligned} 6*6 = 36\\ 6*7 = 42\\ 7*7 = 49 \end{aligned} \]</div>Note that <span class=math>$$ 7*6 = 6*7 = 42 $$</span>. This is where people often forget about being able to reverse the numbers. Every single other multiplication involving 7 or 6 can be calculated using a rule for another number.</p></li></ol><p>And there you have it. Instead of trying to memorize a bunch of numbers, kids can learn <em>rules</em> that build on top of each other, each taking advantage of the rules established before it. It&rsquo;s much more engaging then trying to memorize a giant table of meaningless numbers, a task that&rsquo;s so mind-numbingly boring I can&rsquo;t imagine forcing an adult to do it, let alone a small child. More importantly, this task teaches you what math is <em>really</em> about. It&rsquo;s not about numbers, or adding things together, or memorizing a bunch of formulas. It&rsquo;s establishing simple rules, and then combining those rules together into more complex rules you can use to solve more complex problems.</p><p>This also establishes a fundamental connection to computer science that is often glossed over. Both math and programming are <strong>repeated abstraction and generalization</strong>. It&rsquo;s about combining simple rules into a more generalized rule, which can then be abstracted into a simpler form and combined to create even more complex rules. Programs start with machine instructions, while math starts with propositions. Programs have functions, and math has theorems. Both build on top of previous results to create more powerful and expressive tools. Both require a spark of creativity to recognize similarities between seemingly unrelated concepts and unite them in a more generalized framework.</p><p>We can demonstrate all of this simply by refusing to memorize our times tables.</p></article></div></section><section><div class=dim><aside><h2>Ignoring Outliers Creates Racist Algorithms</h2><ul></ul></aside><article><p>Have you built an algorithm that <em>mostly</em> works? Does it account for <em>almost</em> everyone&rsquo;s needs, save for a few weird outliers that you ignore because they make up 0.0001% of the population? Congratulations, your algorithm is racist! To illustrate how this happens, let&rsquo;s take a recent example from Facebook. My friend&rsquo;s message was removed for &ldquo;violating community standards&rdquo;. Now, my friend has had all sorts of ridiculous problems with Facebook, so to test my theory, I posted the exact same message on my page, and then had him report it.</p><p><div class=imgwrap style=max-width:637px><a href=/img/fb1.png target=_blank><img src=/img/fb1.png width=100%></a></div><div class=imgwrap style=max-width:512px><a href=/img/fb2.png target=_blank><img src=/img/fb2.png width=100%></a></div><div class=imgwrap style=max-width:689px><a href=/img/fb3.png target=_blank><img src=/img/fb3.png width=100%></a></div></p><p>Golly gee, look at that, Facebook confirmed the message I sent does <strong>not</strong> violate community guidelines, but he&rsquo;s still banned for 12 hours for posting <em>the exact same thing</em>. What I suspect happened is this: Facebook has gotten mad at my friend for having a weird name multiple times, but he can&rsquo;t prove what his name is because he doesn&rsquo;t have access to his birth certificate because of family problems, <em>and</em> he thinks someone&rsquo;s been falsely reporting a bunch of his messages. The algorithm for determining whether or not something is &ldquo;bad&rdquo; probably took these misleading inputs, combined it with a short list of so-called &ldquo;dangerous&rdquo; topics like &ldquo;terrorism&rdquo;, and then decided that if anyone reported one of his messages, it was <em>probably</em> bad. On the other hand, I have a very western name and nobody reports anything I post, so either the report actually made it to a human being, or the algorithm simply decided it was probably fine.</p><p>Of course, the algorithm was wrong about my friend&rsquo;s message. But Facebook doesn&rsquo;t care. I&rsquo;m sure a bunch of self-important programmers are just itching to tell me we can&rsquo;t deal with all the edge-cases in a commercial algorithm because it&rsquo;s infeasible to account for all of them. What I want to know is, have any of these engineers ever thought about <strong>who the edge-cases are?</strong> Have they ever thought about the kind of people who can&rsquo;t produce birth certificates, or don&rsquo;t have a driver&rsquo;s license, or have strange names <a href=https://modelviewculture.com/pieces/i-can-text-you-a-pile-of-poo-but-i-cant-write-my-name>that don&rsquo;t map to unicode properly</a> because they aren&rsquo;t western enough?</p><p>Poor people. Minorities. Immigrants. Disabled people. All these people they claim to care about, all this talk of diversity and equal opportunity and inclusive policies, and they&rsquo;re building algorithms that <em>by their very nature</em> will exclude those less fortunate than them. Facebook&rsquo;s algorithm probably doesn&rsquo;t even know that my friend is asian, yet it&rsquo;s still discriminating against him. Do you know who can follow all those rules and assumptions they make about normal people? Rich people. White people. Privileged people. These algorithms benefit those who don&rsquo;t need help, and disproportionately punish those who don&rsquo;t need any more problems.</p><p>What&rsquo;s truly terrifying is that <a href=https://www.nytimes.com/2017/10/18/upshot/taxibots-sensors-and-self-driving-shuttles-a-glimpse-at-an-internet-city-in-toronto.html>Silicon Valley wants to run the world</a>, and it wants to automate everything using a bunch of inherently flawed algorithms. Algorithms that might be <em>impossible</em> to perfect, given the almost unlimited number of edge-cases that reality can come up with. In fact, as I am writing this article, Chrome doesn&rsquo;t recognize &ldquo;outlier&rdquo; as a word, even though <a href="https://www.google.com/search?q=outlier">Google itself does</a>.</p><p>Of course, despite this, Facebook already built an algorithm that tries to detect &ldquo;toxicity&rdquo; and silences &ldquo;unacceptable&rdquo; opinions. Even if they could build a perfect algorithm for detecting &ldquo;bad speech&rdquo;, do these companies really think forcibly restricting free speech will accomplish anything other than improving their own self-image? A deeply cynical part of me thinks the only thing these companies actually care about is looking good. A slightly more optimistic part of me thinks a bunch of well-meaning engineers are simply being stupid.</p><p>You can&rsquo;t change someone&rsquo;s mind by punching them in the face. Punching people in the face may shut them up, but it does not change their opinion. It doesn&rsquo;t <em>fix</em> anything. <em>Talking</em> to them does. I&rsquo;m tired of this industry hiding problems behind shiny exteriors instead of fixing them. That&rsquo;s what used car salesmen do, not engineers. Programming has devolved into an art of deceit, where coders hide behind pretty animations and huge frameworks that sweep all their problems under the rug, while simultaneously screwing over the people who were supposed to benefit from an &ldquo;egalitarian&rdquo; industry that seems less and less egalitarian by the day.</p><p>Either silicon valley needs to start dealing with people that don&rsquo;t fit in neat little boxes, or it will no longer be able to push humanity forward. If we&rsquo;re going to move forward as a species, we have to do it <strong>together</strong>. Launching a bunch of rich people into space doesn&rsquo;t accomplish anything. Curing cancer for rich people doesn&rsquo;t accomplish anything. Inventing immortality for rich people doesn&rsquo;t accomplish anything. If we&rsquo;re going to push humanity forward, we have to push <em>everyone</em> forward, and that means dealing with all 7 billion outliers.</p><p>I hope silicon valley doesn&rsquo;t drag us back to the feudal age, but I&rsquo;m beginning to think <a href=https://medium.com/@ebonstorm/feudalism-and-the-algorithmic-economy-62d6c5d90646>it already has</a>.</p></article></div></section><section><div class=dim><aside><h2>I Used To Want To Work For Google</h2><ul></ul></aside><article><p>A long time ago I thought Google was this magical company that truly cared about engineering and solving problems instead of maximizing shareholder value. Then Larry Page became CEO and <a href=https://erikmcclure.com/blog/googles-decline-really-bugs-me/>I realized they were not a magical unicorn</a> and lamented the fact that they had been transformed into &ldquo;just another large company&rdquo;. Several important things happened between that post and now: Microsoft got a new CEO, so I decided to give them a shot and got hired there. I quit right before Windows 10 came out because I knew it was going to be a disaster. More recently, it&rsquo;s become apparent that Google had gone far past simply being a behemoth unconcerned with the cries of the helpless and transformed into something outright malevolent. It&rsquo;s <a href=http://gizmodo.com/yes-google-uses-its-power-to-quash-ideas-it-doesn-t-li-1798646437>silenced multiple reporters</a>, <a href=https://thenextweb.com/insider/2013/01/05/calling-shenanigans-on-googles-windows-phone-8-maps-narrative/>blocked windows phone from accessing youtube out of spite</a>, and <a href="https://www.nytimes.com/2017/08/30/us/politics/eric-schmidt-google-new-america.html?_r=0">successfully gotten an entire group of researchers fired by threatening to pull funding</a> (<a href=https://citizensagainstmonopoly.org/>but that didn&rsquo;t stop them</a>).</p><p>This is <strong>evil</strong>. This is <em>horrifying</em>. This is the kind of stuff Microsoft did in the 90s that made everyone hate it so much they still have to fight against the repercussions of decisions made two decades ago because of the sheer amount of damage they did and lives they ruined. I&rsquo;m at the point where I&rsquo;d rather go back to Microsoft, whose primary sin at this point is mostly just being incompetent instead of outright evil, rather than Google, who is actually doing things that are fundamentally morally wrong. These are the kinds of decisions that are bald-faced abuses of power, without any possible &ldquo;good intention&rdquo; driving them. It&rsquo;s <strong>vile</strong>. There is no excuse.</p><p>As an ex-Microsoft employee, I can assure you that at no point did I think Microsoft was doing something <em>evil</em> while I was there. I haven&rsquo;t seen Microsoft do anything outright <em>evil</em> since I left, either. The few times they came close they backed off and apologized later. Microsoft didn&rsquo;t piss people off by being evil, it pissed people off by being <em>dumb</em>. I was approached by a Google recruiter shortly after I left and I briefly considered going to Google because I considered them vastly more competent, and I still do. However, no amount of engineering competency can make me want to work for a company that actively does things I consider morally reprehensible. This is the same reason I will never work for Facebook. I&rsquo;ve drawn a line in the sand, and I find myself in the surprising situation of being on the opposite side of Google, and discovering that <em>Microsoft</em>, of all companies, isn&rsquo;t with them.</p><p>I always thought I&rsquo;d be able to mostly disregard the questionable things that Google and Microsoft were doing and compare them purely on the competency of their engineers. However, it seems that Google has every intention of driving me away by doing things so utterly disgusting I could <em>never</em> work there and still be able to sleep at night. This worries me deeply, because as these companies get larger and larger, they eat up all the other sources of employment. Working at a startup that isn&rsquo;t one of the big 5 won&rsquo;t help if it gets bought out next month. One friend of mine with whom I shared many horror stories with worked at LinkedIn. He was not happy when he woke up one day to discover he now worked for the very company he had heard me complaining about. Even now, he&rsquo;s thinking of quitting, and not because Microsoft is evil - they&rsquo;re just so goddamn <em>dumb</em>.</p><p>The problem is that there aren&rsquo;t many other options, short of starting your own company. Google is evil, Facebook is evil, Apple is evil if you care about open hardware, Microsoft is too stupid to be evil but might at some point become evil again, and Amazon is probably evil and may or may not treat it&rsquo;s employees like shit depending on who you ask. Even if you don&rsquo;t work directly for them, you&rsquo;re probably using their products or services. At some point, you have to put food on the table. This is why I generally refuse to blame someone for working for an evil company because the economy punishes you for trying to stand up for your morals. It&rsquo;s not the workers fault, here, it&rsquo;s Wall Street incentivizing rotten behavior by rewarding short-term profits instead of long-term growth. A free market optimizes to a monopoly. Monopolies are bad. I don&rsquo;t know what people don&rsquo;t get about this. We&rsquo;re fighting over stupid shit like transgender troops or gay rights instead of just treating other human beings with decency, all the while letting rich people rob us blind as they decimate the economy. This is stupid. I would daresay it&rsquo;s almost more stupid than the guy at Microsoft who decided to fire all the testers.</p><p>But I guess I&rsquo;ll take unrelenting stupidity over <strong>retaliating against researchers for criticizing you</strong>. At least until Microsoft remembers how to be evil. Then I don&rsquo;t know what I&rsquo;ll do.</p><p>I don&rsquo;t know what anyone will do.</p></article></div></section><section><div class=dim><aside><h2>Integrating LuaJIT and Autogenerating C Bindings In Visual Studio</h2><ul></ul></aside><article><p><a href=https://www.lua.org/>Lua</a> is a popular scripting language due to its tight integration with C. <a href=http://luajit.org/>LuaJIT</a> is an extremely fast JIT compiler for Lua that can be integrated into your game, which also provides an <a href=http://luajit.org/ext_ffi.html>FFI Library</a> that directly interfaces with C functions, eliminating most overhead. However, the FFI library only accepts a subset of the C standard. Specifically, <strong>&ldquo;C declarations are not passed through a C pre-processor, yet. No pre-processor tokens are allowed, except for #pragma pack.&rdquo;</strong> The website suggests running the header file through a preprocesser stage, but I have yet to find a LuaJIT tutorial that actually explains how to do this. Instead, all the examples simply copy+paste the function prototype into the Lua file itself. Doing this with makefiles and GCC is trivial, because you just have to add a compile step using <a href=https://gcc.gnu.org/onlinedocs/gcc-4.9.1/gcc/Preprocessor-Options.html>the <code>-E</code> option</a>, but integrating this with Visual Studio is more difficult. In addition, I&rsquo;ll show you how to properly load scripts and modify the PATH lookup variable so your game can have a proper <code>scripts</code> folder instead of dumping everything in <code>bin</code>.</p><h5 id=compilation>Compilation</h5><p>To begin, we need to <a href=http://luajit.org/download.html>download LuaJIT</a> and get it to actually compile. Doing this manually isn&rsquo;t too difficult, simply open an x64 Native Tools Command Prompt (or x86 Native Tools if you want 32-bit), navigate to <code>src/msvcbuild.bat</code> and run <code>msvcbuild.bat</code>. The default options will build an x64 or x86 dll with dynamic linking to the CRT. If you want a static lib file, you need to run it with the <code>static</code> option. If you want static linking to the CRT so you don&rsquo;t have to deal with that annoying Visual Studio Runtime Library crap, you&rsquo;ll have to modify the .bat file directly. Specifically, you need to find <code>%LJCOMPILE% /MD</code> and change it to <code>%LJCOMPILE% /MT</code>. This will then compile the static lib or dll with static CRT linking to match your other projects.</p><p>This is a bit of a pain, and recently I&rsquo;ve been trying to automate my build process and dependencies using <a href=https://github.com/Microsoft/vcpkg>vcpkg</a> to act as a C++ package manager. A port of LuaJIT is included in the latest update of vcpkg, but if you want one that always statically links to the CRT, you can <a href=https://github.com/Black-Sphere-Studios/vcpkg/tree/master/ports/luajit>get it here</a>.</p><p>An important note: the build instructions for LuaJIT state that you should copy the lua scripts contained in <code>src/jit</code> to your application folder. What it doesn&rsquo;t mention is that this is <em>optional</em> - those scripts contain debugging instructions for the JIT engine, which you probably don&rsquo;t need. It will work just fine without them.</p><p>Once you have LuaJIT built, you should add it&rsquo;s library file to your project. This library file is called <strong>lua51.lib</strong> (and the dll is <strong>lua51.dll</strong>), because LuaJIT is designed as a drop-in replacement for the default Lua runtime. Now we need to actually load Lua in our program and integrate it with our code. To do this, use <code>lua_open()</code>, which returns a <code>lua_State*</code> pointer. You will need that <code>lua_State*</code> pointer for everything else you do, so store it somewhere easy to get to. If you are building a game using an Entity Component System, it makes sense to build a <code>LuaSystem</code> that stores your <code>lua_State*</code> pointer.</p><h5 id=initialization>Initialization</h5><p>The next step is to load in all the standard Lua libraries using <code>luaL_openlibs(L)</code>. Normally, you shouldn&rsquo;t do this if you need script sandboxing for player-created scripts. However, <strong>LuaJIT&rsquo;s FFI library is inherently unsafe</strong>. Any script with access to the FFI library can call any kernel API it wants, so you should be extremely careful about using LuaJIT if this is a use-case for your game. We can also register any C functions we want to the old-fashioned way via <code>lua_register</code>, but this is only useful for functions that don&rsquo;t have C analogues (due to having multiple return values, etc).</p><p>There is one function in particular that you probably want to overload, and that is the <code>print()</code> function. By default, Lua will simply print to standard out, but if you aren&rsquo;t redirecting standard out to your in-game console, you probably have your own <code>std::ostream</code> (or even a custom stream class) that is sent all log messages. By overloading <code>print()</code>, we can have our Lua scripts automatically write to both our log file and our in-game console, which is extremely useful. Here is a complete re-implementation of <code>print</code> that outputs to an arbitrary <code>std::ostream&</code> object:<pre class=language-cpp><code>int PrintOut(lua_State *L, std::ostream&amp; out)
{
  int n = lua_gettop(L);  /* number of arguments */
  if(!n)
    return 0;
  int i;
  lua_getglobal(L, &#34;tostring&#34;);
  for(i = 1; i &lt;= n; i++)
  {
    const char *s;
    lua_pushvalue(L, -1);  /* function to be called */
    lua_pushvalue(L, i);   /* value to print */
    lua_call(L, 1, 1);
    s = lua_tostring(L, -1);  /* get result */
    if(s == NULL)
      return luaL_error(L, LUA_QL(&#34;tostring&#34;) &#34; must return a string to &#34;
        LUA_QL(&#34;print&#34;));
    if(i &gt; 1) out &lt;&lt; &#34;\t&#34;;
    out &lt;&lt; s;
    lua_pop(L, 1);  /* pop result */
  }
  out &lt;&lt; std::endl;
  return 0;
}
</code></pre>To overwrite the existing <code>print</code> function, we need to first define a Lua compatible shim function. In this example, I pass <code>std::cout</code> as the target stream:</p><p><pre class=language-cpp><code>int lua_Print(lua_State *L)
{
  return PrintOut(L, std::cout);
}
</code></pre>Now we simply register our <code>lua_Print</code> function using <code>lua_register(L, "print", &amp;lua_Print)</code>. If we were doing this in a <code>LuaSystem</code> object, our constructor would look like this:</p><p><pre class=language-cpp><code>LuaSystem::LuaSystem()
{
  L = lua_open();
  luaL_openlibs(L);
  lua_register(L, &#34;print&#34;, &amp;lua_Print);
}
</code></pre>To clean up our Lua instance, we need to both trigger a final GC iteration to clean up any dangling memory, and then we call <code>lua_close(L)</code>, so our destructor would look like this:<pre class=language-cpp><code>LuaSystem::~LuaSystem()
{
  lua_gc(L, LUA_GCCOLLECT, 0);
  lua_close(L);
  L = 0;
}
</code></pre></p><h5 id=loadings-scripts-via-require>Loadings Scripts via Require</h5><p>At this point most tutorials skip to the part where you load a Lua script and write &ldquo;Hello World&rdquo;, but we aren&rsquo;t done yet. Integrating Lua into your game means loading scripts and/or arbitrary strings as Lua code while properly resolving dependencies. If you don&rsquo;t do this, any one of your scripts that relies on another script will have to do <code>require("full/path/to/script.lua")</code>. We also face another problem - if we want to have a <code>scripts</code> folder where we simply automatically load every single script into our workspace, simply loading them all can cause duplicated code, because <code>luaL_loadfile</code> does <em>not</em> have any knowledge of <code>require</code>. You can solve this by simply loading a single <code>bootstrap.lua</code> script which then loads all your game&rsquo;s scripts via <code>require</code>, but we&rsquo;re going to build a much more robust solution.</p><p>First, we need to modify Lua&rsquo;s <code>PATH</code> variable, or the variable that controls where it looks up scripts relative to our current directory. This function will append a path (which should be of the form <code>"path/to/scripts/?.lua"</code>) to the beginning of the <code>PATH</code> variable, giving it highest priority, which you can then use to add as many <code>script</code> directories as you want in your game, and any lua script from any of those folders will then be able to <code>require()</code> a script from any other folder in <code>PATH</code> without a problem. Obviously, you should probably only add one or two folders, because you don&rsquo;t want to deal with potential name conflicts in your script files.<pre class=language-cpp><code>int AppendPath(lua_State *L, const char* path)
{
  lua_getglobal(L, &#34;package&#34;);
  lua_getfield(L, -1, &#34;path&#34;); // get field &#34;path&#34; from table at top of stack (-1)
  std::string npath = path;
  npath.append(&#34;;&#34;);
  npath.append(lua_tostring(L, -1)); // grab path string from top of stack
  lua_pop(L, 1);
  lua_pushstring(L, npath.c_str());
  lua_setfield(L, -2, &#34;path&#34;); // set the field &#34;path&#34; in table at -2 with value at top of stack
  lua_pop(L, 1); // get rid of package table from top of stack
  return 0;
}
</code></pre>Next, we need a way to load all of our scripts using <code>require()</code> so that Lua properly resolves the dependencies. To do this, we create a function in C that literally calls the <code>require()</code> function for us:<pre class=language-cpp><code>int Require(lua_State *L,const char *name)
{
  lua_getglobal(L, &#34;require&#34;);
  lua_pushstring(L, name);
  int r = lua_pcall(L, 1, 1, 0);
  if(!r)
    lua_pop(L, 1);
  WriteError(L, r, std::cout);
  return r;
}
</code></pre>By using this to load all our scripts, we don&rsquo;t have to worry about loading them in any particular order - <code>require</code> will ensure everything gets loaded correctly. An important note here is <code>WriteError()</code>, which is a generic error handling function that processes Lua errors and writes them to a log. All errors in lua will return a nonzero error code, and will <em>usually</em> push a string containing the error message to the stack, which must then be popped off, or it&rsquo;ll mess things up later.<pre class=language-cpp><code>void WriteError(lua_State *L, int r, std::ostream&amp; out)
{
  if(!r)
    return;
  if(!lua_isnil(L, -1)) // Check if a string was pushed
  {
    const char* m = lua_tostring(L, -1);
    out &lt;&lt; &#34;Error &#34; &lt;&lt; r &lt;&lt; &#34;: &#34; &lt;&lt; m &lt;&lt; std::endl;
    lua_pop(L, 1);
  }
  else
    out &lt;&lt; &#34;Error &#34; &lt;&lt; r &lt;&lt; std::endl;
}
</code></pre></p><h5 id=automatic-c-binding-generation>Automatic C Binding Generation</h5><p>Fantastic, now we&rsquo;re all set to load up our scripts, but we still need to somehow define a header file and also load that header file into LuaJIT&rsquo;s FFI library so our scripts have direct access to our program&rsquo;s exposed C functions. One way to do this is to just copy+paste your C function definitions into a Lua file in your scripts folder that is then automatically loaded. This, however, is a pain in the butt and is error-prone. We want to have a <em>single source of truth</em> for our function definitions, which means defining our entire LuaJIT C API in a single header file, which is then loaded directly into LuaJIT. Predictably, we will accomplish this by abusing the C preprocessor:</p><p><pre class=language-cpp><code>#ifndef __LUA_API_H__
#define __LUA_API_H__

#ifndef LUA_EXPORTS
#define LUAFUNC(ret, name, ...) ffi.cdef[[ ret lua_##name(__VA_ARGS__); ]]; name = ffi.C.lua_##name
local ffi = require(&#34;ffi&#34;)
ffi.cdef[[ // Initial struct definitions
#else
#define LUAFUNC(ret, name, ...) ret __declspec(dllexport) lua_##name(__VA_ARGS__)
extern &#34;C&#34; { // Ensure C linkage is being used
#endif

struct GameInfo
{
  uint64_t DashTail;
  uint64_t MaxDash;
};

typedef const char* CSTRING; // VC++ complains about having const char* in macros, so we typedef it here

#ifndef LUA_EXPORTS
]] // End struct definitions
#endif

  LUAFUNC(CSTRING, GetGameName);
  LUAFUNC(CSTRING, IntToString, int);
  LUAFUNC(void, setdeadzone, float);

#ifdef Everglade_EXPORTS
}
#endif

#endif
</code></pre>The key idea here is to use macros such that, when we pass this through the preprocessor <em>without</em> any predefined constants, it will magically turn into a valid Lua script. However, when we compile it in our C++ project, our project defines <code>LUA_EXPORTS</code>, and the result is a valid C header. Our C <code>LUAFUNC</code> is set up so that we&rsquo;re using C linkage for our structs and functions, and that we&rsquo;re <em>exporting the function</em> via <code>__declspec(dllexport)</code>. This obviously only works for Visual Studio so you&rsquo;ll want to set up a macro for the GCC version, but I will warn you that VC++ got really cranky when i tried to use a macro for that in my code, so you may end up having to redefine the entire <code>LUAFUNC</code> macro for each compiler.</p><p>At this point, we have a bit of a choice to make. It&rsquo;s more convenient to have the C functions available in the global namespace, which is what this script does, because this simplifies calling them from an interactive console. However, <strong>using <code>ffi.C.FunctionName</code> is significantly faster</strong>. Technically the fastest way is declaring <code>local C = ffi.C</code> at the top of a file and then calling the functions via <code>C.FunctionName</code>. Luckily, importing the functions into the global namespace does not preclude us from using the &ldquo;fast&rdquo; way of calling them, so our script here imports them into the global namespace for ease of use, but in our scripts we can use the <code>C.FunctionName</code> method instead. Thus, when outputting our Lua script, our <code>LUAFUNC</code> macro wraps our function definition in a LuaJIT <code>ffi.cdef</code> block, and then runs a <em>second</em> Lua statement that brings the function into the global namespace. This is why we have an initial <code>ffi.cdef</code> code block for the structs up top, so we can include that second lua statement after each function definition.</p><p>Now we need to set up our compilation so that Visual Studio generates this file without any predefined constants and outputs the resulting lua script to our <code>scripts</code> folder, where our other in-game scripts can automatically load it from. We can accomplish this using a Post-Build Event (under Configuration Properties -> Build Events -> Post-Build Event), which then runs the following code:<pre class=language-cil><code>CL LuaAPI.h /P /EP /u
COPY &#34;LuaAPI.i&#34; &#34;../bin/your/script/folder/LuaAPI.lua&#34; /Y
</code></pre>Visual Studio can sometimes be finicky about that newline, but if you put in two statements on two separate lines, it should run both commands sequentially. You may have to edit the project file directly to convince it to actually do this. The key line here is <code>CL LuaAPI.h /P /EP /u</code>, which tells the compiler to preprocess the file and output it to a <code>*.i</code> file. There is no option to configure the output file, it will always be the exact same file but with a <code>.i</code> extension, so we have to copy and rename it ourselves to our scripts folder using the <code>COPY</code> command.</p><h5 id=loading-and-calling-lua-code>Loading and Calling Lua Code</h5><p>We are now set to load all our lua scripts in our script folder via <code>Require</code>, but what if we want an interactive Lua console? There are lua functions that read strings, but to make this simpler, I will provide a function that loads a lua script from an arbitrary <code>std::istream</code> and outputs to an arbitrary <code>std::ostream</code>:<pre class=language-cpp><code>const char* _luaStreamReader(lua_State *L, void *data, size_t *size)
{
  static char buf[CHUNKSIZE];
  reinterpret_cast&lt;std::istream*&gt;(data)-&gt;read(buf, CHUNKSIZE);
  *size = reinterpret_cast&lt;std::istream*&gt;(data)-&gt;gcount();
  return buf;
}

int Load(lua_State *L, std::istream&amp; s, std::ostream&amp; out)
{
  int r = lua_load(L, &amp;_luaStreamReader, &amp;s, 0);

  if(!r)
  {
    r = lua_pcall(L, 0, LUA_MULTRET, 0);
    if(!r)
      PrintOut(L, out);
  }

  WriteError(L, r, out);
  return r;
}
</code></pre>Of course, the other question is how to call Lua functions from our C++ code directly. There are many, <em>many</em> different implementations of this available, of varying amounts of safety and completeness, but to get you started, here is a very simple implementation in C++ using templates. Note that this does not handle errors - you can change it to use <code>lua_pcall</code> and check the return code, but <a href=http://www.knightsgame.org.uk/blog/2012/09/03/notes-on-luac-error-handling/>handling arbitrary Lua errors is nontrivial</a>.<pre class=language-cpp><code>template&lt;class T, int N&gt;
struct LuaStack;

template&lt;class T&gt; // Integers
struct LuaStack&lt;T, 1&gt;
{
  static inline void Push(lua_State *L, T i) { lua_pushinteger(L, static_cast&lt;lua_Integer&gt;(i)); }
  static inline T Pop(lua_State *L) { T r = (T)lua_tointeger(L, -1); lua_pop(L, 1); return r; }
};
template&lt;class T&gt; // Pointers
struct LuaStack&lt;T, 2&gt;
{
  static inline void Push(lua_State *L, T p) { lua_pushlightuserdata(L, (void*)p); }
  static inline T Pop(lua_State *L) { T r = (T)lua_touserdata(L, -1); lua_pop(L, 1); return r; }
};
template&lt;class T&gt; // Floats
struct LuaStack&lt;T, 3&gt;
{
  static inline void Push(lua_State *L, T n) { lua_pushnumber(L, static_cast&lt;lua_Number&gt;(n)); }
  static inline T Pop(lua_State *L) { T r = static_cast&lt;T&gt;(lua_touserdata(L, -1)); lua_pop(L, 1); return r; }
};
template&lt;&gt; // Strings
struct LuaStack&lt;std::string, 0&gt;
{
  static inline void Push(lua_State *L, std::string s) { lua_pushlstring(L, s.c_str(), s.size()); }
  static inline std::string Pop(lua_State *L) { size_t sz; const char* s = lua_tolstring(L, -1, &amp;sz); std::string r(s, sz); lua_pop(L, 1); return r; }
};
template&lt;&gt; // Boolean
struct LuaStack&lt;bool, 1&gt;
{
  static inline void Push(lua_State *L, bool b) { lua_pushboolean(L, b); }
  static inline bool Pop(lua_State *L) { bool r = lua_toboolean(L, -1); lua_pop(L, 1); return r; }
};
template&lt;&gt; // Void return type
struct LuaStack&lt;void, 0&gt; { static inline void Pop(lua_State *L) { } };

template&lt;typename T&gt;
struct LS : std::integral_constant&lt;int, 
  std::is_integral&lt;T&gt;::value + 
  (std::is_pointer&lt;T&gt;::value * 2) + 
  (std::is_floating_point&lt;T&gt;::value * 3)&gt;
{};

template&lt;typename R, int N, typename Arg, typename... Args&gt;
inline R _callLua(const char* function, Arg arg, Args... args)
{
  LuaStack&lt;Arg, LS&lt;Arg&gt;::value&gt;::Push(_l, arg);
  return _callLua&lt;R, N, Args...&gt;(function, args...);
}
template&lt;typename R, int N&gt;
inline R _callLua(const char* function)
{
  lua_call(_l, N, std::is_void&lt;R&gt;::value ? 0 : 1);
  return LuaStack&lt;R, LS&lt;R&gt;::value&gt;::Pop(_l);
}

template&lt;typename R, typename... Args&gt;
inline R CallLua(lua_State *L, const char* function, Args... args)
{
  lua_getglobal(L, function);
  return _callLua&lt;R, sizeof...(Args), Args...&gt;(L, function, args...);
}
</code></pre>Now you have everything you need for an extensible Lua scripting implementation for your game engine, and even an interactive Lua console, all using LuaJIT. Good Luck!</p></article></div></section><section><div class=dim><aside><h2>Discord: Rise Of The Bot Wars</h2><ul></ul></aside><article><p>The most surreal experience I ever had on discord was when someone PMed me to complain that my anti-spam bot wasn&rsquo;t working against a 200+ bot raid. I pointed out that it was never designed for large-scale attacks, and that discord&rsquo;s own rate-limiting would likely make it useless. He revealed he was selling spambot accounts at a rate of about $1 for 100 unique accounts and that he was being attacked by a rival spammer. My anti-spam bot had been dragged into a turf war between two spambot networks. We discussed possible mitigation strategies for worst-case scenarios, but agreed that most of them would involve false-positives and that discord showed no interest in fixing how exploitable their API was. I hoped that I would never have to implement such extreme measures into my bot.</p><p>Yesterday, our server was attacked by over 40 spambots, and after discord&rsquo;s astonishingly useless &ldquo;customer service&rdquo; response, I was forced to do exactly that.</p><h5 id=a-brief-history-of-discord-bots>A Brief History of Discord Bots</h5><p>Discord is built on a REST API, which was reverse engineered by late 2015 and used to make unofficial bots. To test out their bots, they would hunt for servers to &ldquo;raid&rdquo;, invite their bots to the server, then spam so many messages it would softlock the client, because discord still didn&rsquo;t have any rate limiting. Naturally, as the designated punching bags of the internet, furries/bronies/Twilight fans/slash fiction writers/etc. were among the first targets. The attack on our server was so severe it took us almost 5 minutes of wrestling with an unresponsive client to ban them. Ironically, a few of the more popular bots today, such as &ldquo;BooBot&rdquo;, are banned as a result of that attack, because the first thing the bot creator did was use it to raid our server.</p><p>I immediately went to work building an anti-spam bot that muted anyone sending more than 4 messages per second. Building a program in a hostile environment like this is much different from writing a desktop app or a game, because the bot had to be bulletproof - it had to rate-limit itself and could not be allowed to crash, <em>ever</em>. Any bug that allowed a user to crash the bot was treated as P0, because it could be used by an attacker to cripple the server. Despite using a very simplistic spam detection algorithm, this turned out to be highly effective. Of course, back then, discord didn&rsquo;t have rate limiting, or verification, or role hierarchies, or searching chat logs, or even a way to look up where your last ping was, so most spammers were probably not accustomed to having to deal with <em>any</em> kind of anti-spam system.</p><p>I added raid detection, autosilence, an isolation channel, and join alerts, but eventually we were targeted by a group from 4chan&rsquo;s /pol/ board. Because this was a sustained attack, they began crafting spam attacks timed just below the anti-spam threshold. This forced me to implement a much more sophisticated anti-spam system, using a heat algorithm with a linear decay rate, which is still in use today. This improved anti-spam system eventually made the /pol/ group give up entirely. I&rsquo;m honestly amazed the simplistic &ldquo;X messages in Y seconds&rdquo; approach worked as long as it did.</p><p>Of course, none of this can defend against a large scale attack. As I learned by my chance encounter with an actual spammer, it was getting easier and easier to amass an army of spambots to assault a channel instead of just using one or two.</p><h5 id=anatomy-of-a-modern-spambot-attack>Anatomy Of A Modern Spambot Attack</h5><p>At peak times (usually during summer break), our server gets raided 1-2 times <em>per day</em>. These minor raids are often just 2-3 tweens who either attempt to troll the chat, or use a basic user script to spam an offensive message. Roughly 60-70% of these raids are either painfully obvious or immediately trigger the anti-spam bot. About 20% of the raids involve slightly intelligent attempts to troll the chat by being annoying without breaking the rules, which usually take about 5-10 minutes to be &ldquo;exposed&rdquo;. About 5-10% of the raids are large, involving 8 or more people, but they are also very obvious and can be easily confined to an isolation channel. Problems arise, however, with large <em>spambot</em> raids. Below is a timeline of the recent spambot attack on our server:</p><div style=width:100%;height:150px;background:#dfdfdf;border-radius:3px><div style=padding:10px;height:125px;position:relative><div style=transform:rotate(90deg);position:absolute;left:-1em;top:65px>messages</div><div class=datachart style=display:flex;width:320px;height:100%;align-items:flex-end><div class=ychart style="flex-shrink:0;font-size:.8em;width:3em;height:100%;padding-right:.2em;border-right:solid 1px #999;display:flex;justify-content:space-between;flex-direction:column;align-items:flex-end"></div></div><div style=width:100%><div style="border-top:solid 1px #999;font-size:.8em;position:relative;margin-left:3.2em"><div style=position:absolute;left:0>19:41:25</div><div style=position:absolute;left:80px>19:41:45</div><div style=position:absolute;left:160px>19:42:05</div><div style=position:absolute;left:240px>19:42:25</div><div style=position:absolute;left:320px>19:42:45</div></div></div></div></div><p>This was a botched raid, but the bots that actually worked started spamming within 5 seconds of joining, giving the moderators a very narrow window to respond. The real problem, however, is that so many of them joined, the bot&rsquo;s API calls to add a role to silence them were rate-limited. They also sent messages once every 0.9 seconds, which is designed to get around Discord&rsquo;s rate limiting. This amounted to 33 messages sent every second, but it was difficult for the anti-spam to detect. Had the spambots reduced their spam cadence to 3 seconds or more, this attack could have bypassed the anti-spam detection <em>entirely</em>. My bot now instigates a lockdown by raising the verification level when a raid is detected, but it simply can&rsquo;t silence users fast enough to deal with hundreds of spambots, so at some point the moderators must use a mass ban function. Of course, banning is restricted by the global rate limit, because <strong><a href=https://discordapp.com/developers/docs/resources/guild#create-guild-ban>Discord has no mass ban API endpoint</a></strong>, but luckily the global rate limit is something like 50 requests per second, so if you&rsquo;re <em>only</em> banning people, you&rsquo;re probably okay.</p><p>However, a hostile attacker could sneak bots in one-by-one every 10 minutes or so, avoiding setting off the raid alarm, and then activate them all at once. 500 bots sending randomized messages chosen from an English dictionary once every 5 seconds after sneaking them in over a 48 hour period is the ultimate attack, and one that is almost impossible to defend against, because this also bypasses the 10-minute verification level. As a weapon of last resort, I added a command that immediately bans all users that sent their first message within the past two minutes, but, again, banning is subject to the <a href=https://discordapp.com/developers/docs/topics/rate-limits>global rate limit!</a> In fact, the rate limits can change at any time, and while message deletion has a higher rate limit for bots, bans don&rsquo;t.</p><p>The only other option is to disable the @everyone role from being able to speak on any channel, but you have to do this on a <em>per channel basis</em>, because Discord ignores you if you attempt to globally disable sending message permissions for @everyone. Even then, creating an &ldquo;approved&rdquo; role doesn&rsquo;t work because any automated assignment could be defeated by adding bots one by one. The only defense a small Discord server has is to require moderator approval for every single new user, which isn&rsquo;t a solution - you&rsquo;ve just given up having a public Discord server. It&rsquo;s only a matter of time until any angry 13-year-old can buy a sophisticated attack with a week&rsquo;s allowance. What will happen to public Discord servers then? Do we simply throw up our hands and admit that humanity is so awful we can&rsquo;t even have public communities anymore?</p><h5 id=the-discord-api-hates-you>The Discord API Hates You</h5><p>The rate-limits imposed on Discord API endpoints are exacerbated by temporary failures, and that&rsquo;s excluding network issues. Thus, if I attempt to set a silence role on a spammer that just joined, the API will repeatedly claim they do not exist. In fact, 3 separate API endpoints consistently fail to operate properly during a raid: A &ldquo;member joined&rdquo; event won&rsquo;t show up for several seconds, but if I fall back to calling <code>GetMember()</code>, this <em>also</em> claims the member doesn&rsquo;t exist, which means adding the role <em>also</em> fails! So I have to attempt to silence the user with every message they send until Discord actually adds the role, even though the API failures are also counted against the rate limit! This gets completely absurd once someone assaults your server with 1000 spambots, because this triggers all sorts of bottlenecks that normally aren&rsquo;t a problem. The alert telling you a user has joined? Rate limited. It&rsquo;ll take your bot 5-10 minutes to get through just <em>telling</em> you such a gigantic spambot army joined, unless you include code specifically designed to detect these situations and reduce the number of alerts. Because of this, a single user can trigger something like 5-6 API requests, all of which are counted against your global rate limit and can severely cripple a bot.</p><p>The general advice that is usually given here is &ldquo;just ban them&rdquo;, which is terrible advice because Discord&rsquo;s own awful message handling makes it incredibly easy to trigger a false positive. If a message fails to send, the client simply sends a completely new message, with it&rsquo;s own ID, and will continue re-sending the message until an Ack is received, at which point the user has probably send 3 or 4 copies of the same message, each of which have the same content, but completely unique IDs and timestamps, which looks <em>completely identical</em> to a spam attack.</p><p>Technically speaking, this is done because Discord assigns snowflake IDs server-side, so each message attempt sent by the client must have a unique snowflake assigned after it is sent. However, it can also be trivially fixed by adding an optional &ldquo;client ID&rdquo; field to the message, with a client-generated ID that stays the same if the message is resent due to a network failure. That way, the server (or the other clients) can simply drop any duplicate messages with identical client IDs while still ensuring all messages have unique IDs across their distributed cluster. This would single-handedly fix all duplicate messages across the entire platform, and eliminate almost every single false-positive I&rsquo;ve seen in my anti-spam bot.</p><h5 id=discord-doesnt-care>Discord Doesn&rsquo;t Care</h5><p>Sadly, Discord doesn&rsquo;t seem to care. The general advice in response to &ldquo;how do I defend against a large scale spam attack&rdquo; is &ldquo;just report them to us&rdquo;, so we did exactly that, and then got what has to be one of the dumbest customer service e-mails I&rsquo;ve ever seen in my life:</p><div class=imgwrap style=max-width:571px><a href=/img/res4.PNG target=_blank><img src=/img/res4.PNG alt="Discord Being Stupid" width=100%></a></div><p>Excuse me, <em><strong>WHAT?!</strong></em> Sorry about somebody spamming your service with horrifying gore images, but please don&rsquo;t delete them! What happens if the spammers just delete the messages themselves? What happens if they send <em>child porn?</em> &ldquo;Sorry guys, please ignore the images that are literally illegal to even look at, but we can&rsquo;t delete them because Discord is fucking stupid.&rdquo; Does Discord understand the concept of <em>marking messages for deletion</em> so they are viewable for a short time as evidence for law enforcement?! My anti-spam bot&rsquo;s database currently has more information than <em>Discord&rsquo;s own servers!</em> If this had involved child porn, the FBI would have had to ask <strong>me</strong> for <strong>my records</strong> because Discord would have deleted them all!</p><p>Obviously, we&rsquo;re not going to leave 500+ gore messages sitting in the chatroom while Discord&rsquo;s ass-backwards abuse team analyzes them. I just have to hope my own nuclear option can ban them quickly enough, or simply give up the entire concept of having a public Discord server.</p><p>The problem is that the armies of spambots that were once reserved for the big servers are now so easy and so trivial to make that they&rsquo;re beginning to target smaller servers, servers that don&rsquo;t have the resources or the means to deal with that kind of large scale DDoS attack. So instead, I have to fight the growing swarm alone, armed with only a crippled, rate-limited bot of my own, and hope the dragons flying overhead don&rsquo;t notice.</p><p>What the <em>fuck</em>, Discord.</p><script src=https://d3js.org/d3.v4.min.js></script>
<script>var data=[1,0,0,0,2,3,0,0,0,0,0,5,0,0,0,1,0,2,3,3,0,2,0,9,3,0,0,21,34,34,23,7,4,2,1,8,2,23,34,19,14,11,5,3,3,2,2,5,5,0,0,0,2,4,2,0,0,0,1,0,0,0,0,0,5,2,0,0,0,0,2,2,0,1,0,5,1,2,0,0,0,0,2,2,0,0,0,0,1,1],axis=[35,28,21,14,7,0];d3.select(".datachart").selectAll("div").data(data).enter().append("div").style("width","2px").style("margin-left","2px").style("flex-shrink","0").style("background","#7289DA").style("height",function(e){return e/axis[0]*100+"%"}),d3.select(".ychart").selectAll("div").data(axis).enter().append("div").style("flex-shrink","0").text(function(e){return e})</script></article></div></section><section><div class=dim><aside><h2>Companies Can't Be Apolitical</h2><ul></ul></aside><article><p>One of the most common things I hear from people is that companies should be &ldquo;apolitical&rdquo;. The most formal way this concept is expressed is that a company should make decisions based on what maximizes profits and not political opinions. Unfortunately, the statement &ldquo;companies should only care about maximizing profits&rdquo; is, itself, a political statement (and one I happen to disagree with). Thus, it is fundamentally impossible for a company to be <em>truly</em> apolitical, for the very act of attempting to be apolitical is a political statement.</p><p>How much a company can avoid politics generally depends on both the type and size of the company. Once your company becomes large enough, it will influence politics simply by virtue of its enormous size, and eventually becomes an integral part of political debates whether or wants to or not. Large corporations must take into account the political climate when making business decisions, because simply attempting to blindly maximize profit may turn the public against them and destroy their revenue sources—thus, politics themselves become part of the profit equation, and cannot be ignored. Certain types of businesses embody political statements simply by <em>existing</em>. Grindr, for example, is a dating app for gay men. It&rsquo;s entire business model is dependent on enabling an activity that certain fundamentalists consider inherently immoral.</p><p>You could, theoretically, try to solve part of this quandary by saying that companies should also be <em>amoral</em>, insofar that the free market should decide moral values. The fundamentalists would then protest the companies existence by not using it (but then, they never would have used it in the first place). However, the problem is that, once again, this very statement is itself political in nature. Thus, by either trying to be amoral or moral, a company is making a political statement.</p><p>The issue at play here is that <em>literally everything is political</em>. When most everyone agrees on basic moral principles, it&rsquo;s easier to pretend that politics is really just about economic policy and lawyers, but our current political divisions have demonstrated that this is a fantasy. Politics <em>are</em> the fundamental morals that society has decided on. It&rsquo;s just a lot easier to argue about minor differences in economic policy instead of fundamental differences in basic morality.</p><p>Of course, <em>how</em> companies participate in politics is also important to consider. Right now, a lot of companies participate in politics by spending exorbitant amounts of money on lobbyists. This is a symptom of money <em>in general</em>, and should be solved not by removing <em>corporate</em> money from politics, but removing <em>all</em> money, because treating spending money as a form of speech gives more speech to the rich, which inherently discriminates against the poor and violates the constitutional assertion that all men are created equal (but no one really seems to be paying attention to that line anyway).</p><p>Instead of using money, corporations should <em>do</em> things that uphold whatever political values they believe in. As the saying goes, actions speak louder than words (or money, in this case). You could support civil rights activism by being more inclusive with your hiring and promoting a diverse work environment. Or, if you live in the Philippines, you could create an app that helps death squads hunt down drug users so they can be brutally executed. What&rsquo;s interesting is that most people consider the latter to be a moral issue as opposed to a political one, which seems to derive from the fact that once you agree on most fundamental morals, we humans simply make up a bunch of pointless rules to satisfy our insatiable desire to tell other humans they&rsquo;re wrong.</p><p>We&rsquo;ve lived in a civilized world for so long, we&rsquo;ve forgotten the true roots of politics: a clash between our fundamental moral beliefs, not about how much parking fines should be. Your company will make a political statement whether you like it or not, so you&rsquo;d better make sure it&rsquo;s the one you want.</p></article></div></section><section><div class=dim><aside><h2>Windows Won't Let My Program Crash</h2><ul></ul></aside><article><p>It&rsquo;s been known for a while that windows has a bad habit of <a href=http://blog.paulbetts.org/index.php/2010/07/20/the-case-of-the-disappearing-onload-exception-user-mode-callback-exceptions-in-x64/>eating your exceptions</a> if you&rsquo;re inside a WinProc callback function. This behavior can cause all sorts of mayhem, like your program just vanishing into thin air without any error messages due to a stack overflow that terminated the program without actually throwing an exception. What I didn&rsquo;t realize is that it also eats <code>assert()</code>, which makes debugging hell, because the assertion would throw, the entire user callback would immediately terminate without any stack unwinding, and then windows would just&mldr; keep going, even though the program is now in a laughably corrupt state, because only half the function executed.</p><p>While trying to find a way to fix this, I discovered that there are no less than 4 different ways windows can choose to eat exceptions from your program. I had already told the kernel to stop eating my exceptions using the following code:<pre class=language-cpp><code>HMODULE kernel32 = LoadLibraryA(&#34;kernel32.dll&#34;);   
assert(kernel32 != 0);   
tGetPolicy pGetPolicy = (tGetPolicy)GetProcAddress(kernel32, &#34;GetProcessUserModeExceptionPolicy&#34;);  
tSetPolicy pSetPolicy = (tSetPolicy)GetProcAddress(kernel32, &#34;SetProcessUserModeExceptionPolicy&#34;);   
if(pGetPolicy &amp;&amp; pSetPolicy &amp;&amp; pGetPolicy(&amp;dwFlags))  
  pSetPolicy(dwFlags &amp; \~EXCEPTION_SWALLOWING); // Turn off the filter</code></pre>However, despite this, COM itself was wrapping an entire <code>try {} catch {}</code> statement around my program, so I had to figure out how to turn that off, too. Apparently <a href="https://blogs.msdn.microsoft.com/oldnewthing/20110120-00/?p=11713/">some genius at Microsoft</a> decided the default behavior should be to just swallow exceptions whenever they were making COM, and now they can&rsquo;t change this default behavior because it&rsquo;d break all the applications that now depend on COM eating their exceptions to run properly! So, I turned that off with this code:<pre class=language-cpp><code>CoInitialize(NULL); // do this first   
if(SUCCEEDED(CoInitializeSecurity(NULL, -1, NULL, NULL, RPC_C_AUTHN_L_VEL_PKT_PRIVACY, RPC_C_IMP_LEV_L_IMPERSONATE, NULL, EOAC_DYNAMIC_CLOAKING, NULL)))
{
  IGlobalOptions *pGlobalOptions;
  hr = CoCreateInstance(CLSID_GlobalOptions, NULL, CLSCTX_INPROC_SERVER, IID_PPV_ARGS(&amp;pGlobalOptions));
  if(SUCCEEDED(hr))
  {
    hr = pGlobalOptions-&gt;Set(COMGLB_EXCEPTION_HANDLING, COMGLB_EXCEPTION_DONOT_HANDLE);
    pGlobalOptions-&gt;Release();
  }
}
</code></pre>There are two additional functions that could be swallowing exceptions in your program: <code>_CrtSetReportHook2</code> and <code>SetUnhandledExceptionFilter</code>, but both of these are for SEH or C++ exceptions, and I was throwing an assertion, not an exception. I was actually able to verify, by replacing the assertion <code>#define</code> with my own version, that throwing an actual C++ exception <em>did</em> crash the program&mldr; but an assertion didn&rsquo;t. Specifically, an assertion calls <code>abort()</code>, which raises <code>SIGABRT</code>, which crashes any normal program. However, it turns out that Windows was eating the abort signal, along with every other signal I attempted to raise, which is a problem, because half the library is written in C, and C obviously can&rsquo;t raise C++ exceptions. The assertion failure even showed up in the output&mldr; but didn&rsquo;t crash the program!<pre class=language-cil><code>Assertion failed!

Program: ...udio 2015\\Projects\\feathergui\\bin\\fgDirect2D_d.dll
File: fgEffectBase.cpp
Line: 20

Expression: sizeof(_constants) == sizeof(float)_(4_4 + 2)
</code></pre>No matter what I do, Windows refuses to let the assertion failure crash the program, or even trigger a breakpoint in the debugger. In fact, calling the <code>__debugbreak()</code> intrinsic, which outputs an <code>int 3</code> CPU instruction, was <em>completely ignored</em>, as if it simply didn&rsquo;t exist. The only reliable way to actually crash the program without using C++ exceptions was to do something like divide by 0, or attempt to write to a null pointer, which triggers a segfault.</p><p>Any good developer should be using assertions to verify their assumptions, so having assertions silently fail and then <em>corrupt the program</em> is even worse than ignoring they exist! Now you could have an assertion in your code that&rsquo;s firing, terminating that callback, leaving your program in a broken state, and then the next message that&rsquo;s processed blows up for strange and bizarre reasons that make no sense because they&rsquo;re impossible.</p><p>I have a hard enough time getting my programs to <em>work</em>, I didn&rsquo;t think it&rsquo;d be this hard to make them <em>crash</em>.</p></article></div></section><section><div class=dim><aside><h2>DirectX Is Terrifying</h2><ul></ul></aside><article><p>About two months ago, I got a new laptop and proceeded to load all my projects on it. Despite compiling everything fine, my graphics engine that used DirectX mysteriously crashed upon running. I immediately suspected either a configuration issue or a driver issue, but this seemed weird because my laptop had a <em>newer</em> graphics card than my desktop. Why was it crashing on <em>newer</em> hardware? Things got even more bizarre once I narrowed down the issue - it was in my shader assignment code, which hadn&rsquo;t been touched in almost 2 years. While I initially suspected a shader compilation issue, there was no such error in the logs. All the shaders compiled fine, and then&mldr; didn&rsquo;t work.</p><p>Now, if this error had also been happening on my desktop, I would have immediately started digging through my constant assignments, followed by the vertex buffers assigned to the shader, but again, all of this ran perfectly fine on my desktop. I was completely baffled as to why things weren&rsquo;t working properly. I had eliminated all possible errors I could think of that would have resulted from moving the project from my desktop to my laptop: none of the media files were missing, all the shaders compiled, all the relative paths were correct, I was using the exact same compiler as before with all the appropriate updates. I even updated drivers on both computers, but it stubbornly refused to work on the laptop while running fine on the desktop.</p><p>Then I found something that nearly made me shit my pants.<pre class=language-cpp><code>if(profile &lt;= VERTEX_SHADER_5_0 &amp;&amp; _lastVS != shader) {
  //...
} else if(profile &lt;= PIXEL_SHADER_5_0 &amp;&amp; _lastPS != shader) { 
  //...
} else if(profile &lt;= GEOMETRY_SHADER_5_0 &amp;&amp; _lastGS != shader) {
  //...
}</code></pre>Like any sane graphics engine, I do some very simple caching by keeping track of the last shader I assigned and only setting the shader if it had actually changed. These if statements, however, have a very stupid but subtle bug that took me quite a while to catch. They&rsquo;re a standard range exclusion chain that figures out what type of shader a given shader version is. If it&rsquo;s less than say, 5, it&rsquo;s a vertex shader. Otherwise, if it&rsquo;s less than 10, that this means it&rsquo;s in the range 5-10 and is a pixel shader. Otherwise, if it&rsquo;s less than 15, it must be in the range 10-15, ad infinitum. The idea is that you don&rsquo;t need to check if the value is greater than 5 because the failure of the previous statement already implies that. However, adding that cache check on the end breaks all of this, because now you could be in the range 0-5, but the cache check could fail, throwing you down to the next statement checking to see if you&rsquo;re below 10. Because you&rsquo;re in the range 0-5, you&rsquo;re of course below 10, and the cache check will ALWAYS succeed, because no vertex shader would ever be in the pixel shader cache! <strong>All my vertex shaders were being sent in to directX as pixel shaders after their initial assignment!</strong></p><p>For almost 2 years, I had been feeding DirectX <em>total bullshit</em>, and had even tested it on multiple other computers, and it had never given me a single warning, error, crash, or any indication whatsoever that my code was <strong>completely fucking broken</strong>, in either debug mode or release mode. Somehow, deep in the depths of nVidia&rsquo;s insane DirectX driver, it had managed to detect that I had just tried to assign a vertex shader to a pixel shader, and either ignored it completely, or silently fixed my catastrophic fuckup. However, my laptop had the <em>mobile</em> drivers, which for some reason did not include this failsafe, and so it actually crashed like it was supposed to.</p><p>While this was an <em>incredibly</em> stupid bug that I must have written while sleep deprived or drunk (which is impressive, because I don&rsquo;t actually drink), it was simply impossible for me to catch because it produced <em>zero errors or warnings</em>. As a result, this bug has the honor of being both the dumbest and the longest-living bug of mine, <em>ever</em>. I&rsquo;ve checked every location I know of for any indication that anything was wrong, including hooking into the debug log of directX and dumping all it&rsquo;s messages. Nothing. Nada. Zilch. Zero.</p><p>I&rsquo;ve heard stories about the insane bullshit nVidia&rsquo;s drivers do, but this is <strong>fucking terrifying</strong>.</p><p>Alas, there is more. I had been experimenting with direct2D as an alternative because, well, it&rsquo;s a 2D engine, right? After getting text rendering working, a change in string caching suddenly broke the entire program. It broke in a particularly bizarre way, because it seemed to just stop rendering halfway through the scene. It took almost an hour of debugging for me to finally confirm that the moment I was rendering a particular text string, the direct2D driver just <em>stopped</em>. No errors were thrown. No warnings could be found. Direct2D&rsquo;s failure state was apparently to simply make every single function call silently fail with no indication that it was failing in the first place. It didn&rsquo;t even tell me that the device was missing or that I needed to recreate it. The text render call was made and then every single subsequent call was ignored and the backbuffer was forever frozen to that half-drawn frame.</p><p>The error itself didn&rsquo;t seem to make any more sense, either. I was passing a perfectly valid string to Direct2D, but because that string originated in a <em>different DLL</em>, it apparently made Direct2D completely shit itself. Copying the string onto the stack, however, worked (which itself could only work if the original string was valid).</p><p>The cherry on top of all this is when I discovered that Direct2D&rsquo;s matrix rotation constructor takes <strong>degrees</strong>, not radians, like <em>every single other mathematical function in the standard library.</em> <a href="https://msdn.microsoft.com/en-us/library/windows/desktop/bb205362(v=vs.85).aspx">Even DirectX takes radians!</a></p><p><em><strong>WHO WRITES THESE APIs?!</strong></em></p></article></div></section><section><div class=dim><aside><h2>Everyone Does sRGB Wrong Because Everyone Else Does sRGB Wrong</h2><ul></ul></aside><article><p>Did you know that CSS3 does all its linear gradients and color interpolation completely wrong? All color values in CSS3 are in the sRGB color space, because that&rsquo;s the color space that gets displayed on our monitor. However, the problem is that the sRGB color space looks like this:</p><p><center><a href=https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/SRGB_gamma.svg/250px-SRGB_gamma.svg.png target=_blank><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/SRGB_gamma.svg/250px-SRGB_gamma.svg.png alt="sRGB gamma curve"></a></center></p><p>Trying to do a linear interpolation along a nonlinear curve doesn&rsquo;t work very well. Instead, you&rsquo;re supposed to linearize your color values, transforming the sRGB curve to the linear RGB curve before doing your operation, and then transforming it back into the sRGB curve. This is gamma correction. Here are comparisons between gradients, transitions, alpha blending, and image resizing done directly in sRGB space (assuming your browser complies with the W3C spec) versus in linear RGB:</p><p><table style="margin:0 auto"><tr><th>sRGB</th><th>Linear</th></tr><tr><td><div style="background:linear-gradient( 0deg,#FF0000,#00FF00);width:100px;height:100px"></div></td><td style=vertical-align:center><img src=/img/srgb_gradient.png style=padding:0;border:0></td></tr><tr><td><div id=srgbani style=background:red;width:100px;height:100px></div></td><td><div id=linearani style=background:red;width:100px;height:100px></div></td></tr><tr><td><img src=/img/srgb_radial.png style=background:red;width:100px;height:100px;margin:0;padding:0;border:0></td><td><img src=/img/srgb_alpha.png style=padding:0;border:0></td></tr><tr><td><img src=/img/srgb_resize.png style=padding:0;border:0 height=100 width=100></td><td><img src=/img/srgb_resize_correct.png style="padding:0;height=" 100" width=100></td></tr></table>At this point you&rsquo;ve probably seen a bazillion posts about how you&rsquo;re doing color interpolation wrong, or gradients wrong, or alpha blending wrong, and the reason is because&mldr; you&rsquo;re doing it wrong. But one can hardly blame you, because <strong>everyone</strong> is doing it wrong. CSS does it wrong because SVG does it wrong because Photoshop does it wrong because everyone else does it wrong. It&rsquo;s all wrong.</p><p>The amazing thing here is that the W3C is <a href=https://lists.w3.org/Archives/Public/www-style/2012Jan/0635.html>entirely aware of how wrong CSS3 linear gradients are</a>, but did it anyway to be consistent with everything else that does them wrong. It&rsquo;s interesting that while SVG is wrong by default, it <em>does</em> provide a way to fix this, via <code>color-interpolation</code>. Of course, CSS doesn&rsquo;t have this property yet, so literally all gradients and transitions on the web are wrong because there is no other choice. Even if CSS provided a way to fix this, it would still have to default to being wrong.</p><p>It seems we have reached a point where, after years of doing sRGB interpolation incorrectly, we continue to do it wrong not because we don&rsquo;t know better, but because everyone else is doing it wrong. So everyone&rsquo;s doing it wrong because everyone else is doing it wrong. A single bad choice done long ago has locked us into <strong>compatibility hell</strong>. We got it wrong the first time so now we have to keep getting it wrong because everyone expects the wrong result.</p><p>It doesn&rsquo;t help that we don&rsquo;t always necessarily <em>want</em> the correct interpolation. The reason direct interpolation in sRGB is wrong is because it changes the perceived luminosity. Notice that when going from red to green, the sRGB gradient gets darker in the middle of the transition, while the gamma-corrected one has constant perceived luminosity. However, an artist may prefer the sRGB curve to the linearized one because it puts more emphasis on red and green. This problem gets worse when artists use toolchains inside sRGB and unknowingly compensate for any gamma errors such that the result is roughly what one would expect. This is a particular issue in games, because games really do need gamma-correct lighting pipelines, but the GUIs were often built using incorrect sRGB interpolation, so doing gamma-correct alpha blending gives you the wrong result because the alpha values were already chosen to compensate for incorrect blending.</p><p>In short, this is quite a mess we&rsquo;ve gotten ourselves into, but I think the most important thing we can do is give people the <em>option</em> of having a gamma correct pipeline. It is not difficult to selectively blend things with proper gamma correction. We need to have things like SVG&rsquo;s <code>color-interpolation</code> property in CSS, and other software needs to provide optional gamma correct pipelines (I&rsquo;m looking at you, photoshop).</p><p>Maybe, eventually, we can dig ourselves out of this sRGB hell we&rsquo;ve gotten ourselves into.</p><script>function decimalToHex(e){var t=Math.floor(e).toString(16),t="000000".substr(0,6-t.length)+t;return t}var start=null;function fromlinear(e){return e<=.0031308?e*12.92:1.055*Math.pow(e,1/2.4)-.055}function transitionlinear(e){start||(start=e);var n=e-start,s=document.getElementById("srgbani"),o=document.getElementById("linearani"),t=n*5e-4%1;s.style.backgroundColor="#"+decimalToHex(Math.floor(t*256)*256+Math.floor((1-t)*256)*256*256),o.style.backgroundColor="#"+decimalToHex(Math.floor(fromlinear(t)*256)*256+Math.floor(fromlinear(1-t)*256)*256*256),window.requestAnimationFrame(transitionlinear)}window.requestAnimationFrame(transitionlinear)</script></article></div></section><section><div class=dim><aside><h2>Mathematical Notation Is Awful</h2><ul></ul></aside><article><p>Today, a friend asked me for help figuring out how to calculate the standard deviation over a discrete probability distribution. I pulled up <a href=https://erikmcclure.com/blog/course-notes/>my notes from college</a> and was able to correctly calculate the standard deviation they had been unable to derive after hours upon hours of searching the internet and trying to piece together poor explanations from questionable sources. The crux of the problem was, as I had suspected, the astonishingly bad notation involved with this particular calculation. You see, the expected value of a given distribution <span class=math>$$ X $$</span> is expressed as <span class=math>$$ E[X] $$</span>, which is calculated using the following formula:<div class=math>\[ E[X] = \sum_{i=1}^{\infty} x_i p(x_i) \]</div>The standard deviation is the square root of the variance, and the variance is given in terms of the expected value.<div class=math>\[ Var(X) = E[X^2] - (E[X])^2 \]</div>Except that <span class=math>$$ E[X^2] $$</span> is of course completely different from <span class=math>$$ (E[X])^2 $$</span>, but it gets worse, because <span class=math>$$ E[X^2] $$</span> makes <em>no notational sense whatsoever</em>. In any other function, in math, doing <span class=math>$$ f(x^2) $$</span> means going through and substitution <span class=math>$$ x $$</span> with <span class=math>$$ x^2 $$</span>. In this case, however, <span class=math>$$ E[X] $$</span> actually doesn&rsquo;t have anything to do with the resulting equation, because <span class=math>$$ X \neq x_i $$</span>, and as a result, the equation for <span class=math>$$ E[X^2] $$</span> is this:<div class=math>\[ E[X^2] = \sum_i x_i^2 p(x_i) \]</div>Only the first <span class=math>$$ x_i $$</span> is squared. <span class=math>$$ p(x_i) $$</span> isn&rsquo;t, because it doesn&rsquo;t make any sense in the first place. It should really be just <span class=math>$$ P_{Xi} $$</span> or something, because it&rsquo;s a <em>discrete value</em>, not a function! It would also explain why the <span class=math>$$ x_i $$</span> inside <span class=math>$$ p() $$</span> isn&rsquo;t squared - because it <em>doesn&rsquo;t even exist</em>, it&rsquo;s just a gross abuse of notation. This situation is so bloody confusing I even explicitely laid out the equation for <span class=math>$$ E[X^2] $$</span> in my own notes, presumably to prevent me from trying to figure out what the hell was going on in the middle of my final.</p><p>That, however, was only the beginning. Another question required them to find the covariance between two seperate discrete distributions, <span class=math>$$ X $$</span> and <span class=math>$$ Y $$</span>. I have never actually done covariance, so my notes were of no help here, and I was forced to <a href=https://en.wikipedia.org/wiki/Covariance>return to wikipedia</a>, which gives this helpful equation.<div class=math>\[ cov(X,Y) = E[XY] - E[X]E[Y] \]</div>Oh shit. I&rsquo;ve already established that <span class=math>$$ E[X^2] $$</span> is impossible to determine because the notation doesn&rsquo;t rely on any obvious rules, which means that <span class=math>$$ E[XY] $$</span> could evaluate to <em>god knows what</em>. Luckily, wikipedia has an alternative calculation method:<div class=math>\[ cov(X,Y) = \frac{1}{n}\sum_{i=1}^{n} (x_i - E(X))(y_i - E(Y)) \]</div>This almost works, except for two problems. One, <span class=math>$$ \frac{1}{n} $$</span> doesn&rsquo;t actually work because we have a nonuniform discrete probability distribution, so we have to substitute multiplying in the probability mass function <span class=math>$$ p(x_i,y_i) $$</span> instead. Two, wikipedia refers to <span class=math>$$ E(X) $$</span> and <span class=math>$$ E(Y) $$</span> as the <strong>means</strong>, not the expected value. This gets even more confusing because, at the beginning of the Wikipedia article, it used brackets (<span class=math>$$ E[X] $$</span>), and now it&rsquo;s using parenthesis (<span class=math>$$ E(X) $$</span>). Is that the same value? Is it something completely different? Calling it the mean would be confusing because the average of a given data set isn&rsquo;t necessarily the same as finding what the average <em>expected value</em> of a probability distribution is, which is why we call it the <em>expected value</em>. But naturally, I quickly discovered that <strong>yes</strong>, the mean and the average and the expected value are all <strong>exactly the same thing!</strong> Also, I still don&rsquo;t know why Wikipedia suddenly switched to <span class=math>$$ E(X) $$</span> instead of <span class=math>$$ E[X] $$</span> because it stills means <em>the exact same goddamn thing</em>.</p><p>We&rsquo;re up to, what, five different ways of saying <em>the same thing?</em> At least, I&rsquo;m assuming it&rsquo;s the same thing, but there could be some incredibly subtle distinction between the two that nobody ever explains anywhere except in some academic paper locked up behind a paywall that was published 30 years ago, because apparently mathematicians are okay with this.</p><p>Even then, this is just <em>one instance</em> where the ambiguity and redundancy in our mathematical notation has caused enormous confusion. I find it particularly telling that the most difficult part about figuring out any mathematical equation for me is usually to simply figure out what all the goddamn notation even <em>means</em>, because usually most of it isn&rsquo;t explained at all. Do you know how many ways we have of taking the derivative of something?</p><p><span class=math>$$ f'(x) $$</span> is the same as <span class=math>$$ \frac{dy}{dx} $$</span> or <span class=math>$$ \frac{df}{dx} $$</span> even <span class=math>$$ \frac{d}{dx}f(x) $$</span> which is the same as <span class=math>$$ \dot x $$</span> which is the same as <span class=math>$$ Df $$</span> which is technically the same as <span class=math>$$ D_xf(x) $$</span> and also <span class=math>$$ D_xy $$</span> which is also the same as <span class=math>$$ f_x(x) $$</span> provided x is the only variable, because taking the partial derivative of a function with only one variable is the exact same as taking the derivative in the first place, and I&rsquo;ve actually seen math papers abuse this fact instead of use some other sane notation for the derivative. And that&rsquo;s just for the derivative!</p><p>Don&rsquo;t even get me started on multiplication, where we use <span class=math>$$ 2 \times 2 $$</span> in elementary school, <span class=math>$$ * $$</span> on computers, but use <span class=math>$$ \cdot $$</span> or simply stick two things next to each other in traditional mathematics. Not only is using <span class=math>$$ \times $$</span> confusing as a multiplicative operator when you have <span class=math>$$ x $$</span> floating around, but it&rsquo;s a <em>real operator!</em> It means <a href=https://en.wikipedia.org/wiki/Cross_product>cross product</a> in vector analysis. Of course, the <span class=math>$$ \cdot $$</span> also doubles as meaning the Dot Product, which is at least nominally acceptable since a dot product does reduce to a simple multiplication of scalar values. The Outer Product is generally given as <span class=math>$$ \otimes $$</span>, unless you&rsquo;re in Geometric Algebra, in which case it&rsquo;s given by <span class=math>$$ \wedge $$</span>, which of course means <strong>AND</strong> in <a href=https://en.wikipedia.org/wiki/Logical_conjunction>binary logic</a>. Geometric Algebra then re-uses the cross product symbol <span class=math>$$ \times $$</span> to instead mean <em>commutator product</em>, and also defines the <em>regressive product</em> as the dual of the outer product, which uses <span class=math>$$ \nabla $$</span>. This conflicts with the <a href=https://en.wikipedia.org/wiki/Gradient>gradient operator</a> in multivariable calculus, which uses the exact same symbol in a totally different context, and just for fun it also defined <span class=math>$$ * $$</span> as the &ldquo;scalar&rdquo; product, just to make sure every possible operator has been violently hijacked to mean something completely unexpected.</p><p>This is just <em>one area</em> of mathematics - it is common for many different subfields of math to redefine operators into their own meaning and god forbid any of these fields actually come into contact with each other because then no one knows what the hell is going on. Math is a language that is about as consistent as English, and that&rsquo;s on a good day.</p><p>I am sick and tired of people complaining that nobody likes math when they refuse to admit that mathematical notation sucks, and is a major roadblock for many students. It is useful only for advanced mathematics that take place in university graduate programs and research laboratories. It&rsquo;s hard enough to teach people calculus, let alone expose them to something useful like statistical analysis or matrix algebra that is relevant in our modern world when the notation looks like Greek and makes about as much sense as the English pronunciation rules. We simply cannot introduce people to advanced math by writing a bunch of incoherent equations on a whiteboard. We need to find a way to separate the underlying mathematical <em>concepts</em> from the arcane scribbles we force students to deal with.</p><p>Personally, I understand most of higher math by reformulating it in terms of lambda calculus and type theory, because they map to real world programs I can write and investigate and <em>explore</em>. Interpreting mathematical concepts in terms of computer programs is just one way to make math more tangible. There must be other ways we can explain math without having to explain the extraordinarily dense, outdated notation that we use.</p></article></div></section><section><div class=dim><aside><h2>I Tried To Install Linux And Now I Regret Everything</h2><ul></ul></aside><article><style>.imghover img{display:none;position:absolute;margin-top:-350px;right:0;background:#fff;border:1px solid #000}.imghover:hover img,.imghover:focus img{float:left;display:block}</style>I am going to tell you a story.<p>This story began with me getting pissed off at Windows for reasons that don&rsquo;t really need to be articulated. Just, pick your favorite reason to hate Windows, and let&rsquo;s pretend that was the tipping point. I had some spare space on my secondary drive, so I decided to give Linux another whirl. After all, it had been three or four years since I last attempted anything like this (it&rsquo;s been so long I don&rsquo;t rightly remember the last time I tried), and people have been saying that Linux has gotten a lot better recently!</p><p>The <em>primary</em> reason I was attempting this is because of <a href=http://store.steampowered.com/steamos>Valve&rsquo;s attempts to move to linux</a>, which have directly resulted in much better nVidia driver support. Trying to install proper nVidia drivers is the thing that wrecked my last attempt. This time, I figured, I could just install the nVidia drivers straight from the repo and everything would be hunky dory, and I could just spend the rest of my time beating on the rest of linux with a comically oversized wrench until it did what I wanted.</p><p>I&rsquo;d had good experiences with <a href=http://www.xfce.org/>XFCE</a> and <a href=https://getfedora.org/>Fedora</a> on my Linux VM, but I didn&rsquo;t really like Fedora itself (but it interfaced very well with my VM environment). I wanted to install <a href=http://www.ubuntu.com/>Ubuntu</a>, because it has the best support and I don&rsquo;t like trying to dig through arcane forum posts trying to figure out why my computer screen has suddenly turned into an invisible pink unicorn. Unfortunately, Ubuntu is a bloated mess, and I <em>hate</em> it&rsquo;s default desktop environment. In the past, I had tried Linux Mint, which had been okay, but support had been shaky. I spotted <a href=http://lubuntu.net/>Lubuntu</a>, which is supposed to be a lightweight ubuntu on top of LXDE, a minimal window manager similar to XFCE. This was perfect! So I downloaded Lubuntu 15.04 and installed it on my secondary drive and everything was nice and peachy.</p><p>Well, until Linux started, anyway. The first problem was that the package manager in pretty much every linux distro lists <em>all</em> the packages, including all the ones I don&rsquo;t understand. I was trying to remove some pre-included Paint application and it had two separate packages, one named after itself, and one named <code>&lt;name>-common</code>, but the common package wasn&rsquo;t automatically removed when the program was! The nVidia packages also had both an nvidia-drivers-340<sup><a href=#footnote1>[1]</a></sup> package, and an nvidia-drivers-340-update package, both of which had identical descriptions. I just went with the most basic one because it seemed sensible, but I felt really sorry for someone less tech savvy trying to find <em>anything</em> in that godforsaken list.</p><p>So after everything updated and I restarted and told the package manager to start installing my nVidia drivers, I started noticing annoying things about LXDE. A lot of annoying things. Here, let me list them for you:</p><ul><li>The file manager, when unmounting anything, would helpfully close itself.</li><li>Trying to change the time to not display military time involves editing some arcane string whose only value is <code>%r</code>, and I still don&rsquo;t know what that means and don&rsquo;t want to know what that means. All I wanted to do was change it to say AM or PM!</li><li>In order to get a shortcut onto the desktop, you had to <em>right-click the menu item</em> and then a new menu would show up that would let you add it to the desktop. You can&rsquo;t drag anything out of the start menu.</li><li>The shortcuts seemed disturbingly fickle, occasionally taking 4 rapid clicks to get things to start up.</li><li>Steam simply didn&rsquo;t start <em>at all</em>. Unfortunately, we will never know why.</li><li>Skype managed to spawn a window halfway above the top of the screen, which resulted in me having to look up <code>alt+space</code> in order to rescue it.</li><li>Skype also cut off everything below the baseline of the bottom-most line of text, so you couldn&rsquo;t tell if something was an i or a j.</li><li>Like in Windows, in Linux, modal dialogs have a really bad habit of sucking up keyboard focus, focusing on a button, and then making my next keystroke click the button. The fact that this is a widespread UI problem across most operating systems is just silly.</li></ul><p>There were more nitpicks, but I felt like a large number of these issues could have probably be resolved by switching to a better window manager. I was honestly expecting better than this, though. LXDE was <em>supposed</em> to be like XFCE, but apparently it&rsquo;s actually XCFE with all it&rsquo;s redeeming qualities removed. However, it turns out that <em>this doesn&rsquo;t matter!</em> To discover why, we have to examine what happened next.</p><p>My friend suggested to get Linux Mint, which has a better window manager and would probably address most of those issues. So, I downloaded the ISO. I already had a USB set up for booting that had Lubuntu on it, so I wanted to see if I could just extract the contents of the ISO and put them on the USB stick. I have no idea if this would have actually worked or not, and I never got to find out because upon clicking and dragging the contents of the ISO out of the archive manager, with the intent of extracting them, <em>the entire system locked up</em>.</p><p>Now, I&rsquo;m not sure how stupid trying to drag a file out of an ISO was, but I&rsquo;m pretty sure it shouldn&rsquo;t <em>render my entire computer unusable</em>. This seems like an unfair punishment. Attempts to recover the desktop utterly failed, as did <code>ctrl-alt-del</code>, <code>alt-F2</code>, <code>alt-</code>anything else, or even trying to open a terminal (my friend would later inform me that it is actually <code>*ctrl-alt-F2*</code> that opens the terminal, but I couldn&rsquo;t ask him because <em>the entire desktop had locked up!</em>). So I just restarted the machine.</p><p>That was the last time I saw my Lubuntu desktop.</p><p>Upon restarting my machine, I saw the Lubuntu loading screen come up, and then&mldr; nothing. Blackness. About 20 seconds later, an error message pops up: &ldquo;Soft Lock on CPU#0!&rdquo;. Then the machine rebooted.</p><p>After spending just <em>one hour</em> using linux, it had bricked itself.<sup><a href=#footnote2>[2]</a></sup> I hadn&rsquo;t even gotten steam working yet, and now it was completely unusable. This is not &ldquo;getting better&rdquo;, this is strapping a rocket to your ass and going so fast in the wrong direction you break the sound barrier. Now, if you have been paying attention, you will note that I had just finished installing my nVidia drivers before the desktop locked up, and that the drivers probably wouldn&rsquo;t actually be used by the desktop environment until the process was restarted. After mounting my linux partition in windows and extracting my log files, I sent them to my friend, who discovered that it had indeed been the nVidia driver that had crashed the system.<sup><a href=#footnote3>[3]</a></sup></p><p>This is problematic for a number of reasons, because those drivers were written for Ubuntu based distros, which means my system could potentially lock up if I installed any other ubuntu based distro, which is&mldr; just about all the ones that I cared about. Including Linux Mint. At this point, I had a few options:</p><ol><li>Install another ubuntu based distro anyway and hope it was a fluke.</li><li>Install something based on Debian or maybe Fedora.</li><li>Use the open-source drivers.</li><li><em><a href="https://www.youtube.com/watch?v=5FjWe31S_0g">Fuck this shit.</a></em></li></ol><p>Unfortunately, I have little patience left at this point, and after seeing Linux first <em>lock up after trying to drag files</em> before <em>bricking itself</em>, I don&rsquo;t really have much confidence in the reverse-engineered open-source nVidia drivers, and I certainly am not going to entrust my video card to them in the hopes it doesn&rsquo;t melt. I really, <em>really</em> don&rsquo;t want to play whack-a-mole with Linux distros, trying to find the magical one that <em>wouldn&rsquo;t</em> wreck my graphics card, so I have simply opted for option 4.</p><p>But it doesn&rsquo;t end there.</p><p>About two or three hours after this all happened (I had been fairly distracted, and by distracted I mean <span class=imghover><img src=https://googledrive.com/host/0B_2aDNVL_NGmUlp0bnh3Z3hUeWc/>}}<a href=javascript:void()>blinded by rage</a></span>), I noticed that my windows clock was way off. At this point, I remembered something my friend had told me about - Linux, like any sane operating system would, sets the hardware clock to UTC and then modifies it based on the timezone. Windows, on the other hand, decided it would be a fantastic idea to set the hardware clock to <em>local time</em>. Of course, this should have been an easy fix. I just set the time back, forced an update, and bam, time was accurate again. Crisis averted, right?</p><p>No, of course not. Linux had not yet finished punishing me for foolishly believing I was worthy of installing something of it&rsquo;s calibre. Because then I opened Skype, and started receiving messages <em>in the past</em>. Or more accurately, my chat logs were now full of messages that had been sent <em>tomorrow</em>.</p><p>It was at this point I realized what had happened. It wasn&rsquo;t that the timezone had been changed, or something reversible like that. The <em>hardware clock itself</em> had been modified to an incorrect value. Skype had spent the past two hours happily sending messages with a timestamp <em>8 hours in the future</em> because some idiot at Microsoft thought it was a good idea to set the hardware clock to local time, and now all these incorrect client side timestamps had been propagated to the cloud and synced across all my devices.</p><p>I slowly got up from my computer. I walked over to my bed, lied down, curled into a ball, and wept for the future of mankind.</p><p><center><iframe width=420 height=315 src=https://www.youtube.com/embed/HIWHMb3JxmE frameborder=0 allowfullscreen></iframe></center><hr><sup><a name=footnote1>1</a></sup> I may have gotten these package names slightly wrong, but I can&rsquo;t verify them because <em>Linux won&rsquo;t boot up!</em>
<sup><a name=footnote2>2</a></sup> It&rsquo;s probably entirely possible to rescue the system by mounting the file system and modifying the x config file to not use nvidia, but this was supposed to just be &ldquo;install linux and set it up&rdquo;, not &ldquo;install linux and spend the next 5 hours frantically trying to get it to boot properly.&rdquo;
<sup><a name=footnote3>3</a></sup> After submitting my log files to #ubuntu to report the issue, my friend discovered that the drivers for ATI/AMD graphics drivers are also currently broken for many ubuntu users. What timing! This certainly makes me feel confident about using this operating system!</p></article></div></section><section><div class=dim><aside><h2>We Aren't Designing Software For Robots</h2><ul></ul></aside><article><blockquote>"I have the solution, but it only works in the case of a spherical cow in a vacuum." - *[old physics proverb](https://en.wikipedia.org/wiki/Spherical_cow)*</blockquote>Whenever I am designing something, be it an API or a user interface, I try to remember that I am not designing this for a perfectly rational agent. Instead, I am designing software for a bunch of highly emotional, irrational creatures called *human beings*, all of whom have enormously different tastes. I try to include options, or customization, or if that isn't possible, a compromise. I try to keep the door open, to let my software be used as a tool to enhance someone's productivity no matter what workflow they use, instead of trying to impose my own workflow on them.<p>For some reason, many programmers seem to struggle with the concept of people being different. They get hung up on this naïve concept of <em>right</em> or <em>wrong</em>, as if life is some kind of mathematical equation that has a closed form solution. Let me say right now that any solution to life is going to be one heck of a chaotic nonlinear PDE, which won&rsquo;t have any closed form solution at all, and certainly not one using elementary functions. When you are developing software, you must keep in mind the <em>range</em> of customers who will be using your product, whether they are office workers or fellow programmers.</p><p>Maybe someone is using your product to try and finish a presentation in time to go home and catch a nap before they get up to work their second job so they can support a wife and a screaming baby. Someone else might use your product to track their progress as they try to revolutionize search from their bedroom instead of study for their finals next week. Someone else might be an elderly man trying to figure out how his vacation is going to go.</p><p><em>We are all different</em>. We arise from all walks of life and are bound together in a great journey on this blue ball hurtling through space. It is not cowardice when two people try to put aside their differences and work together, it is strength. It requires enormous courage to admit that there are no simple answers in life. There are no answers in the back of the textbook. There are many different answers, all different in subtle ways, all suitable for slightly different circumstances, all both <em>right</em> and <em>wrong</em> in their own, strange, quirky ways.</p><p>Some programmers seem distressingly incapable of compassion or empathy. Many claim to live through the cold hard logic of <em>data</em>, without apparently realizing that data itself is inherently meaningless. It is only given meaning through a human&rsquo;s interpretation, and a human can interpret data to mean whatever they want. They seem to think they can solve problems by reducing everyone into neat little collections of numbers that can be easily analyzed. It&rsquo;s certainly a lot less frustrating to work with real numbers instead of real people, but inevitably, a programmer must come to terms with the fact that life is about <em>human beings</em>, not numbers on a screen.</p><p>The cold hard logic of our code is good for speaking to computers—it is not good for speaking to other human beings.</p></article></div></section><section><div class=dim><aside><h2>Using Data To Balance Your Game: Pony Clicker Analysis</h2><ul></ul></aside><article><blockquote>*The only thing more addicting than heroine are numbers that keep getting larger.*</blockquote><p>Incrementer and idle games are seemingly simplistic games where you wait or click to increase a counter, then use that counter to buy things to make the counter go up faster. Because of the compounding effects involved, these types of games inevitably turn into a study of growth rates and how different functions interact. <a href=http://orteil.dashnet.org/cookieclicker/>Cookie Clicker</a> is perhaps the most well-known, which employs an exponential growth curve for the costs of buildings that looks like this:</p><div class=math>\[ Cost_n = Cost_0\cdot 1.15^n \]</div><p>Where <span class=math>$$ Cost_0 $$</span> is the initial cost of the building. Each building, however, has a fixed income, and so the entire game is literally the player trying to purchase upgrades and buildings to fight against an interminable exponential growth curve of the cost function. Almost every single feature added to Cookie Clicker is yet another way to battle the growth rate of the exponential function, delaying the plateauing of the CPS as long as possible. This includes the reset functionality, which grants heavenly chips that yield large CPS bonuses. However, no feature can compensate for the fact that the buildings do not have a sufficient growth rate to keep up with the exponential cost function, so you inevitably wind up in a dead end where it becomes almost impossible to buy anything in a reasonable amount of time regardless of player action.</p><p>Pony Clicker is based off Cookie Clicker, but takes a different approach. Instead of having set rates for each building, each building generates a number of smiles based on the number of ponies and friendships that you have, along with other buildings that &ldquo;synergize&rdquo; with that building. The more expensive buildings generate more smiles because they have a higher growth rate than the buildings below them. This makes the game extremely difficult to balance, because you only have equations and the cost curves to work with, instead of simply being able to set the per-building SPS. Furthermore, the SPS of a building continues to grow and change over the course of the game, further complicating the balance equation. Unfortunately, in the first version of the game, the growth rate of the end building exceeded the growth rate of the cost function, which resulted in immense end-game instability and all around unhappiness. To address balance problems in pony clicker, rather than simply throwing ideas at the wall and trying to play test them infinitely, I wrote a program that played the game for me. It uses a nearly optimal strategy of buying whatever the most efficient building is in terms of cost per +1 SPS increase. This is not a perfectly optimal strategy, which has to take into account how long the next building will need to take, but it was pretty close to how players tended to play.</p><p>Using this, I could analyze a game of pony clicker in terms of what the SPS looked like over time. My first graph was not very promising:</p><div class=imgwrap style=max-width:700px><a href=https://cloud.githubusercontent.com/assets/3387013/7786493/21b10990-0188-11e5-86de-7eaa7d5e07d5.png target=_blank><img src=https://cloud.githubusercontent.com/assets/3387013/7786493/21b10990-0188-11e5-86de-7eaa7d5e07d5.png width=100%></a></div><p>The SPS completely exploded and it was obviously terrible. To help me figure out what was going on, I included a graph of the optimal store purchases and the time until the next optimal purchase. My goal in terms of game experience was that no building would be left behind, and that there shouldn&rsquo;t be enormous gaps between purchases. I also wanted to make sure that the late game or the early game didn&rsquo;t take too long to get through.</p><div class=imgwrap style=max-width:700px><a href=https://cloud.githubusercontent.com/assets/3387013/7786492/1ccd4ac4-0188-11e5-840b-d3a47f4a898a.png target=_blank><img src=https://cloud.githubusercontent.com/assets/3387013/7786492/1ccd4ac4-0188-11e5-840b-d3a47f4a898a.png width=100%></a></div><p>In addition to this, I created a graph of the estimate SPS generation of each individual building, on a per-friendship basis. This helped compensate for the fact that the SPS changed as the game state itself changed, allowing me to ensure the SPS generation of any one building wasn&rsquo;t drastically out of whack with the others, and that it increased on a roughly linear scale.</p><div class=imgwrap style=max-width:700px><a href=https://cloud.githubusercontent.com/assets/3387013/7787681/f5c01296-01cc-11e5-829e-a0e9e1f89334.png target=_blank><img src=https://cloud.githubusercontent.com/assets/3387013/7787681/f5c01296-01cc-11e5-829e-a0e9e1f89334.png width=100%></a></div><p>This information was used to balance the game into a much more sane curve:</p><div class=imgwrap style=max-width:700px><a href=https://cloud.githubusercontent.com/assets/3387013/7899795/3bb8210c-06e7-11e5-9514-1b66669631dc.png target=_blank><img src=https://cloud.githubusercontent.com/assets/3387013/7899795/3bb8210c-06e7-11e5-9514-1b66669631dc.png width=100%></a></div><p>I then added upgrades to the main graph, and quickly learned that I was giving the player certain upgrades way too fast:</p><div class=imgwrap style=max-width:700px><a href=https://cloud.githubusercontent.com/assets/3387013/7806412/491fa6cc-0335-11e5-8e2e-6e6903d94240.png target=_blank><img src=https://cloud.githubusercontent.com/assets/3387013/7806412/491fa6cc-0335-11e5-8e2e-6e6903d94240.png width=100%></a></div><p>This was used to balance the upgrades and ensure they only gave a significant SPS increase when it was really needed (between expensive buildings, usually). The analysis page itself is <a href=https://erikmcclure.github.io/PonyClicker/utilities/analysis.html>available here</a>, so you can look at the current state of pony clicker&rsquo;s growth curve.</p><p>These graphs might not be perfect, but they are incredibly helpful when you are trying to eliminate exponential explosions. If you got a value that spirals out of control, a graph will tell you <em>immediately</em>. It makes it very easy to quickly balance purchase prices, because you can adjust the prices and see how this affects the optimal gameplay curve. Pony Clicker had so many interacting equations it was basically the <em>only</em> way i could come up with a game that was even remotely balanced (although it still needs some work). It&rsquo;s a great example of how important <strong>rapid feedback</strong> is when designing something. If you can get immediate feedback on what changing something does, it makes the process of refining something much faster, which lets you do a better job of refining it. It also lets you experiment with things that would otherwise be way too complex to balance by hand.</p></article></div></section><section><div class=dim><aside><h2>Does Anyone Actually Want Good Software?</h2><ul></ul></aside><article><p>Are there any programmers left that actually care about writing good software? As far as I can tell, the software development industry has turned into a series of echo chambers where managers scream about new features and shipping software and analyzing feedback from customers. Then they ignore all the feedback and implement whatever new things are supposed to be cool, like flat design, or cloud computing, or software as a service.</p><p>The entire modern web is built on top of the worst programming language that&rsquo;s still remotely popular. It&rsquo;s so awful that <a href=http://blogs.msdn.com/b/ie/archive/2015/02/18/bringing-asm-js-to-the-chakra-javascript-engine-in-windows-10.aspx>IE now supports asm.js</a> just so we can use other languages instead. With everyone relentlessly misquoting &ldquo;Premature optimization is the root of all evil&rdquo;, it&rsquo;s hard to get programmers to optimize any of their code <em>at all</em>, let alone get them to care about things like CPU caches and why allocation on the heap is slow and how memory locality matters.</p><p>Some coders exist at large corporations that simply pile on more and more lines of code and force everyone to use gigantic frameworks built on top of more gigantic frameworks built on top of even more gigantic frameworks and then wonder why everything is so slow. Other coders exist in startups that use Scala/Hadoop/Node.js and care only about pumping out features or fixing bugs. The thing is, all of these companies make a lot of money, which leads me to ask, <em>does anyone actually want good software anymore?</em></p><p>Do customers simply not care? Is everyone ok with Skype randomly not sending messages and trying (poorly) to sync all your messages and randomly deciding that certain actions are always unread on other computers and dropping calls and creating all sorts of other strange and bizarre bugs? Is everyone ok with an antivirus that demands you sign in to a buggy window that keeps losing focus every time you try to type in your password? Is everyone ok with Visual Studio deciding it needs to open a text file and taking 15 seconds to actually start up an entirely new instance even though I already have one running just to display the stupid file?</p><p>It seems to me that we&rsquo;re all so obsessed with making cool stuff, we&rsquo;ve forgotten how to make stuff that <em>actually works</em>.</p><p>Did you know that every single person I know (except for two people) hates flat design? They don&rsquo;t like it. I don&rsquo;t like it. There&rsquo;s a bunch of stuckup, narcissistic designers shoving flat design down everyone&rsquo;s throats and <em>I hate it</em>. The designers don&rsquo;t care. They insist that it&rsquo;s elegant and modern and a bunch of other crap that&rsquo;s all entirely subjective no matter how hard they try to pretend otherwise. Design is about opinions. If I don&rsquo;t like your design, you can&rsquo;t just go and say my opinion is wrong. My opinion isn&rsquo;t wrong, I just don&rsquo;t agree with you. There&rsquo;s a difference.</p><p>However, it has become increasingly apparent to me that opinions aren&rsquo;t allowed in programming. I&rsquo;m not allowed to say that garbage collectors are bad for high performance software. I&rsquo;m not allowed to say that pure functional programming isn&rsquo;t some kind of magical holy grail that will solve all your problems. I&rsquo;m not allowed to say that flat design is stupid. I&rsquo;m definitely not allowed to say that I hate Python, because apparently Python is a religion.</p><p>Because of this, I am beginning to wonder if I am simply delusional. Apparently I&rsquo;m the only human being left on planet earth who really, really doesn&rsquo;t like typing magical bullshit into his linux terminal just to get basic things working instead of having a GUI that wasn&rsquo;t designed by brain-dead monkeys. Apparently, I&rsquo;m the only one who is entirely willing to pay money for services instead of having awful, ad-infested online versions powered by JavaScript™ and Node.js™ that fall over every week because someone forgot to cycle the drives in a cloud service 5000 miles away. Apparently, no one can fix the <a href=https://erikmcclure.com/blog/how-not-to-sell-software/>audio sample library industry</a> or the fact that most of my VSTi&rsquo;s manage to use 20% of my CPU <em>when they aren&rsquo;t actually doing anything</em>.</p><p>Am I simply getting old? Has the software industry left me behind? Does anyone else out there care about these things? Should I throw in the towel and call it quits? Is the future of software development writing terrible monstrosities held together by duct tape? Is this the only way to have a sustainable business?</p><p>Is this the world our customers want? Because it sure isn&rsquo;t what I want.</p><p>Unfortunately, writing music doesn&rsquo;t pay very well.</p></article></div></section><section><div class=dim><aside><h2>How Not To Install Software</h2><ul></ul></aside><article><p>It&rsquo;s that time of the year again, when everyone and their pony puts on a sale, except now it seems to have started much earlier than the traditional Black Friday. Needless to say, this is the only time of year I go around buying expensive sample libraries. One of these libraries was recommended by a friend: LA Scoring Strings - First Chair 2, a cheaper version of LA Scoring Strings. It&rsquo;s $100 off, which is pretty nice, except that the page that describes the product <a href=http://audiobro.com/la-scoring-strings/la-scoring-strings-first-chair/>doesn&rsquo;t actually have a link to buy it</a>. You have to click the STORE link, and then buy it from <em>there</em>, because this is a completely obvious and intuitive interface design (it isn&rsquo;t).</p><p>So, after finding the proper link in the Store and verifying I am actually purchasing what I want to purchase, they give me exactly one payment option: Paypal. This is pretty common, so we&rsquo;ll let it slide, and the whole process seems to go smoothly until they give me my receipt. On the receipt page, they gave me a link to download the files, and a serial number. How helpful! Until I click the download link, which does not open in a new window, and instead opens a completely different webpage with absolutely no way to get back to the page I was just on, because there is no store page with this information and I have no user account on this site. So, I have to go to my e-mail, where they have helpfully e-mailed me a copy of the receipt (probably for this exact reason) to get the serial number.</p><p>I then go back to the download page only to discover that I am <em>required</em> to use their stupid download manager in order to download the product I just bought. There is no alternative option whatsoever. So I download their stupid download manager and it magically installs itself somewhere on my computer I will likely never be able to find because it never asked my permission to do anything, and then demands that I log in. Well, obviously, I don&rsquo;t have a log in, and no one asked me to register until now, so I go to register, which helpfully opens my web browser to register&mldr; on a forum. Well, ok, so I register on the forum with a randomly generated password, and activate my account.</p><p>So naturally, they then <strong>e-mail my password back to me</strong>, which by definition means they are storing it in plaintext. So now the password to my account was sent over an unencrypted, entirely open channel, which is insanely stupid, but this is just a sample library, so whatever. I go back to their download manager and put in my credentials and&mldr; the login fails. Well, maybe it takes a bit to propagate - no, it just isn&rsquo;t working. I try again, and triple check that I have the password right. I log out and back into the forum with that very same password, and it still works. It just doesn&rsquo;t work in the application.</p><p>Standard procedure at this point is for me to take every single weird punctuation character out of my password (making it much weaker) to address the possibility that these people are pants on head retarded and can&rsquo;t handle a password with punctuation in it. I change my password to an alphanumeric one, and lo and behold, I can suddenly log in to the download manager! Let&rsquo;s think about this for a moment. The password I used had some punctuation characters in it (like &ldquo;!&amp;#*@(?&rdquo; etc.), but in order to make sure it was still a valid password, I logged in to the forum with that password, and it succeeded. I then went to this application and put in the <em>same password</em> and it failed to log me in, which means the program actually only accepts some random subset of all valid passwords that the forum lets you register with.</p><p>This is laughably bad programming, but my woes aren&rsquo;t over yet. I click the download button only to get this incredibly helpful message: &ldquo;Cannot connect to download servers.&rdquo; Pissed off, I go play a game in the hopes that once I get back, the servers will work again. I close the game only to discover that my download manager is one giant grey screen no matter what i do to it. It&rsquo;s forgotten how to draw it&rsquo;s own UI at this point. I restart the program, and it has (of course) helpfully forgotten my login credentials. This time, it displays a EULA it apparently forgot to show me the first time around, and once I accept, clicking install successfully starts downloading the files!</p><p>Of course, once the files are installed, they aren&rsquo;t actually <em>installed</em> installed. I have to go into Kontakt and add the libraries to it&rsquo;s magical library in order for them to actually get recognized. I can&rsquo;t tell if this is AudioBro&rsquo;s fault or Native Instruments fault, but at this point I don&rsquo;t care, because this has already become the worst installation experience of any piece of software I have had to go through <em>in my entire life</em>.</p><p>What&rsquo;s frightening is that this is par for the course across the desolate wasteland that is Audio Sample Libraries. The entire audio engineering industry employs draconian and ultimately ineffective DRM security measures, often bundled with installers that look like they were written in 1998 and never updated. The entire industry uses software that is grotesquely bloated, digging it&rsquo;s filthy claws into my operating system and doing all sorts of unspeakable things, and there is <strong>no way out</strong>.</p><p>You can&rsquo;t disrupt this field, because samples rule everything. If you have good samples, people will buy your shitty sample libraries. EastWest moved from Kontakt (which is a pretty shitty piece of software considering it&rsquo;s the best sampler in the entire industry) to their own proprietary PLAY engine, which is unstable, bloated, entirely dependent on ASIO4ALL to even <em>work</em>, and prone to crashing. They still make tons of money, because <em>they have the best orchestral samples</em>, which means people will put up with their incredibly bad sampler just so they can use their samples, which are all in a proprietary format that will get you violently sued if you attempt to reverse engineer it.</p><p>So, even if you develop the best sampler in the world, it won&rsquo;t matter, because without samples, your software is dead on arrival. Almost all the samples that are worth having come in proprietary formats that your program can&rsquo;t understand, and no one can convert these samples to another format (unless they want to reverse engineer the program and get sued, that is). So now the entire sampling industry is locked in a oligopoly of competing samplers that refuse to talk to each other, thus crushing competition by making the cost of entrance so prohibitively high no one can possibly compete with them. And then you get this shit.</p></article></div></section><section><div class=dim><aside><h2>Can We Choose What We Enjoy?</h2><ul></ul></aside><article><p>One of the most bizarre arguments I have ever heard in ethics is whether or not people can choose to be gay or not. The idea is, if being gay is genetically predetermined, it&rsquo;s not their fault, therefore you can&rsquo;t prosecute them for something they have no control over.</p><p>Since when did <em>anyone</em> get to choose what makes them happy? Can you <em>choose</em> to like strawberries? Can you <em>choose</em> to enjoy the smell of dandelions? At best, you can subject yourself to something over and over and over again and enjoy it as a sort of acquired taste, but this doesn&rsquo;t always work, and the fact remains that you are still predisposed to enjoying certain experiences. Unless we make a concentrated effort to change our preferences, <em>all</em> enjoyable sensory experiences occur <em>without our consent</em>. We are not in charge of what combination of neural impulses our brain happens to find enjoyable. All we can do is slowly influence those preferences, and even then, only sometimes.</p><p>This concept of people <em>choosing</em> what they enjoy seems to have infected society, and is often at the root of much bizarre and often unfair prosecution. If we assume that people cannot significantly change the preferences they were dealt by life, either as a result of genetic or environmental influences, a host of moral issues become apparent.</p><p>Gender roles stop making sense. In fact, prosecuting anyone on the LGTB spectrum immediately becomes invalid. Attacking anyone&rsquo;s sexual preferences, provided they are harmless, becomes unacceptable. Trying to attack anyone&rsquo;s artistic or musical preferences becomes difficult, at best. We know for a fact that someone&rsquo;s culinary preferences are influenced by the genetic distribution of taste buds in their mouth. It&rsquo;s even hard to properly critique someone&rsquo;s fashion choices if they happened to despise denim or some other fabric.</p><p>As far as I&rsquo;m concerned, the answer to the question &ldquo;why would someone like [x]&rdquo; is always &ldquo;because their brain is wired in a way that enjoys it.&rdquo; Humans are, at a fundamental level, sensory processing machines that accidentally achieved self-awareness. We enjoy something because we are programmed to enjoy it. To insult what kinds of sensory input someone enjoys simply because they do not match up with your own is laughably juvenile. The only time this kind of critique is valid is when someone&rsquo;s preferences cause harm to another person. We all have our own unique ways of processing sensory input, and so we will naturally enjoy different things, through no fault of our own. Sometimes, with a substantial amount of effort, we can slowly change some of those preferences, but most of the time, we&rsquo;re stuck with whatever we were born with (or whatever environmental factors shaped our perception in our childhood).</p><p>Instead of accusing someone of liking something you don&rsquo;t approve of, maybe next time you should try to understand <em>why</em> they like it, instead. Maybe you&rsquo;ll find a new friend.</p></article></div></section><section><div class=dim><aside><h2>How To Make Your Profiler 10x Faster</h2><ul></ul></aside><article><p>Frustrated with C profilers that are either so minimal as to be useless, or giant behemoths that require you to install device drivers, I started writing a lightweight profiler for my <a href=http://bss-util.blackspherestudios.com>utility library</a>. I already had a high precision timer class, so it was just a matter of using a radix trie that didn&rsquo;t blow up the cache. I was very careful about minimizing the impact the profiler had on the code, even going so far as to check if extended precision floating point calculations were slowing it down.</p><p>Of course, since I was writing a profiler, I could use the profiler to profile itself. By pretending to profile a random number added to a cache-murdering int stuck in the middle of an array, I could do a fairly good simulation of profiling a function, while also profiling the act of profiling the function. The difference between the two measurements is how much overhead the profiler has. Unfortunately, my initial results were&mldr; <em>unfavorable</em>, to say the least.<pre class=language-cil><code>BSS Profiler Heat Output: 
[main.cpp:3851] test_PROFILE: 1370173 µs   [##########
  [code]: 545902.7 µs   [##########
  [main.cpp:3866] outer: 5530.022 ns   [....      
    [code]: 3872.883 ns   [...       
    [main.cpp:3868] inner: 1653.139 ns   [.         
  [main.cpp:3856] control: 1661.779 ns   [.         
  [main.cpp:3876] beginend: 1645.466 ns   [.         
</code></pre>The profiler had an overhead of almost <em>4 microseconds</em>. When you&rsquo;re dealing with functions that are called thousands of times a second, you need to be aware of code speed on the scale of <em>nanoseconds</em>, and this profiler would <em>completely ruin</em> the code. At first, I thought it was my fault, but none of my tweaks seemed to have any measureable effect on the speed whatsoever. On a whim, I decided to comment out the actual _querytime function that was calling QueryPerformanceCounter, then run an external profiler on it.<pre class=language-cil><code>Average control: 35 ns</code></pre><em>What?!</em> Well no wonder my tweaks weren&rsquo;t doing anything, all <em>my</em> code was taking a scant <em>35 nanoseconds</em> to run. The other <strong>99.9%</strong> of the time was spent on that single, stupid call, which also happened to be the one call I couldn&rsquo;t get rid of. However, that isn&rsquo;t the end of the story; _querytime() looks like this:<pre class=language-cpp><code>void cHighPrecisionTimer::_querytime(unsigned __int64* _pval)
{
  DWORD procmask=_getaffinity(); 
  HANDLE curthread = GetCurrentThread();
  SetThreadAffinityMask(curthread, 1);
  
  QueryPerformanceCounter((LARGE_INTEGER*)_pval);
  
  SetThreadAffinityMask(curthread, procmask);
}
</code></pre></p><p>Years ago, it was standard practice to wrap all calls to QueryPerformanceCounter in a CPU core mask to force it to operate on a single core due to potential glitches in the BIOS messing up your calculations. Microsoft itself had recommended it, and you could find this same code in almost any open-source library that was taking measurements. It turns out that this is <a href="http://msdn.microsoft.com/en-us/library/windows/desktop/dn553408(v=vs.85).aspx">no longer necessary</a>:</p><blockquote>**Do I need to set the thread affinity to a single core to use QPC?**<p>No. For more info, see Guidance for acquiring time stamps. This scenario is neither necessary nor desirable.</blockquote></p><p>I couldn&rsquo;t get rid of the QueryPerformanceCounter call itself, but I could get rid of all that other crap it was doing. I commented it out, and <em>voilà!</em> The overhead had been reduced to a scant <em>340 nanoseconds</em>, only a tenth of what it had been before. I&rsquo;m still spending 90% of my calculation time calling that stupid function, but there isn&rsquo;t much I can do about that. Either way, it was a good reminder about the entire reason for using a profiler - bottlenecks tend to crop up in the most unexpected places.</p><pre class=language-cil><code>BSS Profiler Heat Output: 
[main.cpp:3851] test_PROFILE: 142416 µs   [##########
  [code]: 56575.4 µs   [##########
  [main.cpp:3866] outer: 515.43 ns   [....      
    [code]: 343.465 ns   [...       
    [main.cpp:3868] inner: 171.965 ns   [.         
  [main.cpp:3876] beginend: 173.025 ns   [.         
  [main.cpp:3856] control: 169.954 ns   [.         
</code></pre><p>I also tried adding standard deviation measurements, but that ended up giving me ludicrous values of 342±27348 ns, which isn&rsquo;t very helpful. Apparently there&rsquo;s quite a lot of variance in function call times, so much so that while the averages always tend to be the same over time, the statistical variance goes through the roof. This is probably why most profilers don&rsquo;t include the standard deviation. I <em>was</em> able to add in accurate unprofiled code measurements, though, and the profiler uses a dynamic triple magnitude method of displaying how much time a function takes.</p></article></div></section><section><div class=dim><aside><h2>The Problem With Photorealism</h2><ul></ul></aside><article><p>Many people assume that modern graphics technology is now capable of rendering photorealistic video games. If you define <em>photorealistic</em> as <em>any still frame is indistinguishable from a real photo</em>, then we can get pretty close. Unfortunately, the problem with video games is that they are not still frames - they <em>move</em>.</p><p>What people don&rsquo;t realize is that modern games rely on faking a lot of stuff, and that means they only <em>look</em> photorealistic in a very tight set of circumstances. They rely on you not paying close attention to environmental details so you don&rsquo;t notice that the grass is actually just painted on to the terrain. They precompute environmental convolution maps and bake ambient occlusion and radiance information into level architecture. You can&rsquo;t knock down a building in a game unless it is specifically programmed to be breakable and all the necessary preparations are made. Changes in levels are often scripted, with complex physical changes and graphical consequences being largely precomputed and simply triggered at the appropriate time.</p><p>Modern photorealism, like the 3D graphics of ages past, is smoke and mirrors, the result of very talented programmers and artists using tricks of the eye to convince you that a level is much more detailed and interactive than it really is. There&rsquo;s nothing wrong with this, but we&rsquo;re so good at doing it that people think we&rsquo;re a heck of a lot closer to photorealistic games then we really are.</p><p>If you want to go beyond simple photorealism and build a game that <em>feels</em> real, you have to deal with a lot of extremely difficult problems. Our best antialiasing methods are perceptual, because doing real antialiasing is prohibitively expensive. Global illumination is achieved by deconstructing a level&rsquo;s polygons into an octree and using the GPU to cubify moving objects in realtime. Many advanced graphical techniques in use today depend on precomputed values and static geometry. The assumption that most of the world is probably going to stay the same is a powerful one, and enables huge amounts of optimization. Unfortunately, as long as we make that assumption, none of it will ever feel truly real.</p><p>Trying to build a world that does not take anything for granted rapidly spirals out of control. Where do you draw the line? Does gravity always point down? Does the atmosphere always behave the same way? Is the sun always yellow? What counts as solid ground? What happens when you blow it up? Is the object you&rsquo;re standing on even a <em>planet?</em> Imagine trying to code an engine that can take into account all of these possibilities <em>in realtime</em>. This is clearly horrendously inefficient, and yet there is no other way to achieve a <em>true</em> dynamic environment. At some point, we will have to make assumptions about what will and will not change, and these sometimes have surprising consequences. A volcanic eruption, for example, drastically changes the atmospheric composition and completely messes up the ambient lighting and radiosity.</p><p>Ok, well, at least we have dynamic animations, right? <strong>Wrong.</strong> Almost all modern games still use precomputed animations. Some fancy technology can occasionally try to interpolate between them, but that&rsquo;s about it. We have no reliable method of generating animations on the fly that don&rsquo;t look horrendously awkward and stiff. It turns out that trying to calculate a limb&rsquo;s shortest path from point A to point B while avoiding awkward positions and obstacles amounts to solving the <a href=http://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation>Euler-Lagrange equation</a> over an <em>n-dimensional manifold!</em> As a result, it&rsquo;s incredibly difficult to create smooth animations, because our ability to fluidly shift from one animation to another is extremely limited. This is why we still have weird looking walk animations and occasional animation jumping.</p><p>The worst problem, however, is that of <em>content creation</em>. The simple fact is that at photorealistic detail levels, it takes way too long for a team of artists to build a believable world. Even if we had super amazing 3D modelers that would allow an artist to craft any small object in a matter of minutes (which we don&rsquo;t), artists aren&rsquo;t machines. Things look real because they have a history behind them, a reason for their current state of being. We can make photorealistic CGI for movies because each scene is scripted and has a well-defined scope. If you&rsquo;re building GTA V, you can&rsquo;t somehow manage to come up with three hundred unique histories for every single suburban house you&rsquo;re building.</p><p>Even if we <em>did</em> invent a way to render photorealistic graphics, it would all be for naught until we figured out a way to generate obscene amounts of content at incredibly high levels of detail. Older games weren&rsquo;t just easier to render, they were easier to <em>make</em>. There comes a point where no matter how many artists you hire, you simply can&rsquo;t build an expansive game world at a photorealistic level of detail in just 3 years.</p><p>People always talk about realtime raytracing as the holy grail of graphics programming without realizing just what is required to take advantage of it. Photorealism isn&rsquo;t just about processing power, it&rsquo;s about <em>content</em>.</p></article></div></section><section><div class=dim><aside><h2>Google's Decline Really Bugs Me</h2><ul></ul></aside><article><p>Google is going down the drain.</p><p>That isn&rsquo;t to say they aren&rsquo;t fantastically successful. They are. I still use their products, mostly because I don&rsquo;t put things on the internet I don&rsquo;t want other people to find, and I&rsquo;m not female, so I don&rsquo;t have to worry about misogynists stalking me. They still make stupendous amounts of money and pump out some genuinely good software. They still have the best search engine. Like Microsoft, they&rsquo;ll be a force to be reckoned with for many decades to come.</p><p>Google, however, represented an <em>ideal</em>. They founded the company with the motto &ldquo;Don&rsquo;t Be Evil&rdquo;, and the unspoken question was, how long would this last? The answer, oddly enough, was &ldquo;until Larry Page took over&rdquo;.</p><p>In its early years, Google unleashed the creativity of the brilliant people it hired to the world and came up with a slew of fantastic products that were a joy to use. Google made huge contributions to the open-source world and solved scalability problems with an elegance that has yet to be surpassed. They famously let engineers use 20% of their time to pursue their own interests, and the result was an unstoppable tidal wave of innovation. Google was, for a brief moment, a shining beacon of hope, a force of good in a bleak world of corporations only concerned with maximizing profit.</p><p>Then Larry Page became CEO. Larry Page worshiped Steve Jobs, who gave him <a href=http://googlesystem.blogspot.com/2011/10/how-steve-jobs-influenced-googles.html>a bunch of bad advice</a> centered around <em>maximizing profit</em>. The result was predictable and catastrophic, as the entire basis of what had made Google so innovative was destroyed for the sake of <em>maximizing profit</em>. Now it&rsquo;s just another large company - only concerned about <em>maximizing profit</em>.</p><p>Google was a company that, for a time, I loved. To me, they represented the antithesis of Microsoft, a rebellion against a poisonous corporate culture dominated by profiteering that had no regard for its users. Google was just a bunch of really smart people trying to make the world a better place, and for a precious few years, they succeeded - until it all came tumbling down. Like an artist whose idol has become embroiled in a drug abuse scandal, I have lost my guiding light.</p><p>Google was largely the reason I wanted to start my own company, even if college kept me from doing so. As startup culture continued to suck the life out of silicon valley, I held on to Google as an ideal, an example of the kind of company I wanted to build instead of a site designed to sort cat photos. A company that made money because it solved real problems better than everyone else. A company that respected good programming practices, using the right tool for the job, and the value of actually <em>solving</em> a problem instead of just throwing more code at it.</p><p>Google was a company that solved problems first, and made money second.</p><p>Now, it has succumbed to maximizing stock price for a bunch of rich wall street investors who don&rsquo;t care about anything other than filling their own pockets with as much cash as they possibly can. Once again, the rest of the world is forced to sit around, waiting until an investor accidentally makes the world a better place in the process of trying to make as much money as possible.</p><p>Most people think this is the only way to get things done. For a precious few years, I could point to Google and say otherwise. Now, it has collapsed, and its collapse has made me doubt my own resolve. If Google, of all companies, couldn&rsquo;t maintain that idealistic vision, was it even <em>possible?</em></p><p>Google gave me a reason to believe that humanity could do better. That we could move past a Wall Street that has become nothing more than a rotting cesspool of greed and corruption.</p><p>Now, Google has fallen, along with the ideal it encompassed. Is there a light at the end of the tunnel? Or is it a train, a force of reality come to remind us that no matter how much we reach for utopia, we will be sentenced to drown in our own greed?</p></article></div></section><section><div class=dim><aside><h2>The Educational Imbroglio</h2><ul></ul></aside><article><blockquote>**im·bro·glio**
<span style=color:#666>*noun*</span>
<span style=padding-left:1em>1. an extremely confused, complicated, or embarrassing situation.</span></blockquote>Across the country, there is a heated debate over our educational system. Unfortunately, it's a lot like watching members of the flat earth society argue about whose theory is right - there is no right answer, because *everyone is wrong*.<p>The most recent and perhaps egregious example of this is a breathtakingly misguided <a href=http://blogs.seattletimes.com/educationlab/2013/10/27/roundtable-can-tracking-ever-work/#2>article by Kevin G. Welner</a>, who is the director of the National Education Policy center, which is absolutely terrifying. He is attempting to discredit the idea of &ldquo;tracking&rdquo;, wherein low-performing students are separated from higher achieving students, because obviously you can&rsquo;t teach kids who &ldquo;get it&rdquo; the same way as kids who are struggling. Unfortunately, the entire conclusion rests on a logical fallacy. He says, and I quote:</p><blockquote>"When children fall behind academically, we have a choice. We can choose to sort them into less demanding classes where they will fall further behind, or we can choose to include them in classes that maintain high expectations."</blockquote>This is a [false dichotomy](http://en.wikipedia.org/wiki/False_dilemma), since there are many other choices. We can sort them into a class with the same expectations, but an alternative teaching method. Sort them into a class that actually pays attention to the concepts that are giving them trouble. The idea is to help children who have fallen behind *catch up with their peers*, not throw them in a room and forget about them. Schools that simply lower their expectations of poorly performing students are doing it wrong. Furthermore, trying to argue that something can't work because no one's doing it properly is [another logical fallacy](http://en.wikipedia.org/wiki/Argument_from_ignorance).<p>There&rsquo;s also a persistent argument against charter schools, which claims that the money spent on charter schools should instead be used to improve public schools instead. This is laughable, because public schools receive funding based on test scores. So, all the money would be spent improving test scores instead of actually teaching children anything. Charter schools are important because they aren&rsquo;t bounded by these nonsensical restrictions and thus are free to experiment with alternative teaching styles. Throwing money at our public schools will only shore up a method of teaching that&rsquo;s fundamentally broken. It&rsquo;s like trying to fix the plumbing after the roof caved in - it&rsquo;s completely missing the point.</p><p>To make matters worse, there&rsquo;s also a war on free time. Recess is being cut in favor of increased instruction time, while educators cite minuscule increases in test scores as proof that this &ldquo;works&rdquo;. If by &ldquo;works&rdquo;, you mean it succeeds in cramming more useless junk into kids heads, then sure, it&rsquo;s &ldquo;working&rdquo;. However, if you want kids to actually <em>learn</em> instead of memorize pointless facts that they will immediately forget, you have to give them time to <em>process concepts</em>. Their brains need <em>rest</em>, not more work. Bodybuilders don&rsquo;t lift weights as long as they can every single day; they lift weights <em>every other day</em> and only for a few hours or so, because the muscle needs time to recover.</p><p>This, however, is an issue with a society that thinks hard work means working yourself to exhaustion. This is incredibly short-sighted and in direct opposition to plenty of evidence that points to rest being a necessary part of a healthy lifestyle. It can be your job, or school, or a hobby, it doesn&rsquo;t matter. Humans do not function effectively when forced to focus on one thing for hours at a time. The only reason we attempt to do this is because we used to work in factories, but nowadays we have robots. Modern jobs are all about <em>thinking creatively</em>, which cannot be forced. You can&rsquo;t force yourself to understand a concept. It&rsquo;s like trying to force a broken leg to heal faster by going for a jog. You <em>must</em> give kids time absorb concepts instead of trying to cram it down their throats. They need to <em>understand</em> what they are learning, not memorize formulas.</p><p>Mainstream education doesn&rsquo;t take this seriously. There are plenty of experiments that have effectively taught children advanced concepts with radically different teaching methods. One guy <a href=http://www.garlikov.com/Soc_Meth.html>taught 3rd graders binary</a>. <a href=http://edition.cnn.com/2013/02/27/opinion/ted-prize-students-teach-themselves/index.html>These kids</a> learned english and how to operate a computer <em>without a teacher at all</em>. There are plenty of cases that show just how woefully inadequate our teaching system is, but it seems that we care more about a one-size-fits-all method that can be mass-produced than a method that&rsquo;s actually effective.</p><p>Our educational system is failing our students, and we refuse to even entertain notions that could make a difference. Instead, we just legislate more tests and take away their recess.</p><p>Because really, memorizing the date of the Battle of Gettysburg is more important than playing outside and <em>having a childhood</em>.</p></article></div></section><section><div class=dim><aside><h2>Write Less Code</h2><ul></ul></aside><article><blockquote>*"Everything should be as simple as possible, but not simpler."* - Albert Einstein ([paraphrased](http://quoteinvestigator.com/2011/05/13/einstein-simple/#more-2363))</blockquote><p>The burgeoning complexity of software is perhaps one of the most persistent problems plaguing computer science. We have tried many, many ways of managing this complexity: inventing new languages, creating management systems, and enforcing coding styles. They have all proven to be nothing more than stopgaps. With the revelation that the NSA is utilizing the inherent complexity of software systems to sabotage our efforts at securing our systems, this problem has become even more urgent.</p><p>It&rsquo;s easy to solve a problem with a lot of code - it&rsquo;s hard to solve a problem with a little code. Since most businesses do not understand the advantage of having high quality software, the most popular method of managing complexity is not to reduce how much code is used, but to wrap it up in a self-contained library so we can ignore it, instead. The problem with this approach is that you haven&rsquo;t gotten rid of the code. It&rsquo;s still there, it&rsquo;s just easier to ignore.</p><p>Until it breaks, that is. After we have wrapped complex systems into easy-to-use APIs, we then build even more complex systems on top of them, creating networks of interconnected components, each one encapsulating it&rsquo;s own complex system. Instead of addressing this burgeoning complexity, we simply wrap it into <em>another</em> API and pave over it, much like one would pave over a landfill. When it finally breaks, we have to <a href="http://ptrthomas.files.wordpress.com/2006/06/jtrac-callstack1.png?w=630">extract a core sample</a> just to get an idea of what might be going wrong.</p><p>One of the greatest contributions functional programming has made is its concept of a pure function with no side-effects. When a function has side effects, it&rsquo;s another edge on the graph of components where something could go wrong. Unfortunately, this isn&rsquo;t enough. If you simply replace a large complex system with a bunch of large complex functions that technically have no side-effects, it&rsquo;s still a large complex system.</p><p>Object-oriented programming tries to manage complexity by compartmentalizing things and (in theory) limiting how they interact with each other. This is designed to address spaghetti code that has a bunch of inter-dependent functions that are impossible to effectively debug. The OO paradigm has the right idea: &ldquo;an object should do exactly one thing, and do it well&rdquo;, but suffers from over-application. OO programming was designed to manage <em>very large complex objects</em>, not concepts that don&rsquo;t need Get/Set functions for every class member. Writing these functions <em>just in case</em> you need them in the future is a case of future-proofing, which should almost always be avoided. If there&rsquo;s an elegant, simple way to express a solution that only works given the constraints you currently have, <em><strong>USE IT</strong></em>. We don&rsquo;t need any more <a href=http://limejuice.fastmail.fm/expand-all-example.png>leaning towers of inheritance</a>.</p><p>The number of bugs in a software program is proportional to how much code you write, and that doesn&rsquo;t include cheap tricks like the ternary operator or importing a bunch of huge libraries. The logic is still there, the complexity is still there, and it will still break in mysterious ways. In order to reduce the complexity in our software, it has to do <em>less stuff</em>. The less code you write, the harder it is for the NSA to sabotage your program without anyone noticing. The less code you write, the fewer points of failure your program will have. The less code you write, the less stuff will break. We cannot hide from complexity any longer; we must actively reduce it. We have to begin excavating the landfill instead of paving over it.</p></article></div></section><section><div class=dim><aside><h2>Most People Have Shitty Computers</h2><ul></ul></aside><article><blockquote>*Premature optimization is the root of all evil* - Donald Knuth</blockquote><p>Ever since I started putting words on the internet, I have complained about the misinterpretation and overgeneralization of this Donald Knuth quote. Developers essentially use it as an excuse to never optimize anything, because they can always say they &ldquo;aren&rsquo;t done yet&rdquo; and then permanently render all optimizations as premature. The fact that &ldquo;premature&rdquo; is inherently a subjective term doesn&rsquo;t help. The quote was meant to target very low-level optimizations that are largely useless until you&rsquo;re positive everything else is working properly and you have no other low-hanging fruit to optimize.</p><p>Instead of following that advice, and only doing complex optimizations after all the low-hanging fruit has been picked, developers tend to just <em>stop optimizing</em> after the low-hanging fruit is gone. Modern developers will sometimes stop optimizing things <em>entirely</em> and leave it up to the compiler, which almost always does a horrible job at it. Because web development is inherently dependent on one of the <a href=http://www.ecmascript.org/>worst programming languages currently being used by anyone</a>, these issues are even more prominent.</p><p>When people complain about stuff not working on their phones while on shitty internet connections (which is every single free wifi hotspot ever), the developers tell them to stop using shitty internet connections. When they complain about how slow and unresponsive a stupid, unnecessary app whose entire purpose is to just <em>display text</em> (I&rsquo;m looking at you, Blogger), the developers tell them to get a better phone. When someone writes an app that doesn&rsquo;t compress anything and sucks up bandwidth like a pig, for absolutely no reason at all, the developers tell them to get a better data plan.</p><p><strong>What the fuck?</strong></p><p>Just because you have a desktop with an 8-core processor and 32 gigs of RAM doesn&rsquo;t mean your customers do. The number of people using the top-of-the-line technology is a tiny fraction of the total number of people who actually use the internet, which at this point is <a href=http://en.wikipedia.org/wiki/Global_Internet_usage>more than 1/3 the population of the entire human race</a>. Only targeting hipster white kids who live in San Francisco and have rich parents may work for a small startup, but when Google&rsquo;s mail app turns into a piece of sludge that moves about as fast as <a href=http://en.wikipedia.org/wiki/Pitch_drop_experiment>pitch at room temperature</a>, we have crossed a line. Google is <em>everywhere</em>. Their stuff should work well on the shittiest internet connection you can imagine. You can&rsquo;t go around expecting all your customers to have perfect internet all the time when it&rsquo;s just not possible.</p><p>Don&rsquo;t just target the latest version of Windows and tell the other versions to stuff it, because it&rsquo;s almost never really necessary when all you&rsquo;re doing is using 3 or 4 new functions introduced in Windows 7 that could easily be encapsulated in a small check. Don&rsquo;t write your game in such a way that it will only be playable on a high end gaming machine because <a href=http://en.wikipedia.org/wiki/Crysis#Game_engine>you will get a lot of flak</a>. If making this happen is hard, you probably have a shitty game engine that has a bad architecture. Likewise, when my orchestral VSTi sucks up 10% of my CPU while it&rsquo;s just <em>sitting there</em> and <em>NOT ACTUALLY DOING ANYTHING</em>, something is seriously wrong. I know it&rsquo;s using the exact same sample format as Kontact (someone reverse-engineered it), except that it does everything 4 times slower. Yet, when people complain, and they complain all the time, the response is always &ldquo;you need a better computer.&rdquo;</p><p>Most people have shitty computers. Your customers don&rsquo;t care about all the cool new features you can implement for your mail app now that you have time to work on them because you never bothered to optimize it properly - they just want a mail app that <em>WORKS</em>.</p></article></div></section><section><div class=dim><aside><h2>Leap Motion Impressions, Input Sanitation, and 3D Gesture Ideas</h2><ul></ul></aside><article><blockquote>**Pros:**<ul><li>For the most part, does what it claims it does, and gives you extremely precise, fast tracking of fingers.</li></ul><p><strong>Cons:</strong></p><ul><li>Really hates thumbs for some reason.</li><li>Has a lot of trouble with pens or other implements.</li><li>Fingers must be separated.</li><li>Fairly easy to get positions that break the camera because it can&rsquo;t see the fingers.</li><li><em>No one has any idea how to write software for it.</em></blockquote></li></ul><p>I just got my Leap Motion device today, and for the most part, I like it. It does most of the things I expected it to do. It seems like something that could become very useful down the line. Unfortunately, right now, I wouldn&rsquo;t recommend getting one if you wanted something that&rsquo;s more than a toy.</p><p>This isn&rsquo;t entirely the fault of the device itself, but more a combination of driver limitations and boneheaded software that doesn&rsquo;t know what it&rsquo;s doing. The device is so new, no one knows how to properly utilize it, so almost all the apps that exist right now are either garbage or ridiculously sensitive to your setup. There are, however, a few more serious problems that should be addressed by the Leap Motion team.</p><p>First of all, the driver hates thumbs. It seems like it&rsquo;s trying to edit them out when they aren&rsquo;t positioned like a finger, which makes any sort of pinching gesture impossible to do. In addition, it doesn&rsquo;t like it when you try to use a pen, despite the fact that it&rsquo;s specifically designed to let you do so. Both of these problems seem more like driver analysis problems, so I expect they&rsquo;ll be corrected soon. I know these aren&rsquo;t hardware limitations, because the camera can see the stupid pen, the driver just has to recognize it as a tool and disregard the hand holding it, which is really doesn&rsquo;t want to do - It&rsquo;s obsessed with hands. I should call it the Lyra driver from now on.</p><p>A more fundamental and concerning issue is the ease in which I can assume a position that blocks the camera&rsquo;s view of some or most of my fingers. Pretty much anything that involves my fingers being vertically aligned breaks the driver, along with everything from a roughly 90 degree to 135 degree angle, pointed away from the device. This appears to be an inherent issue with the device itself, and its the same problem the kinect would have, because the camera&rsquo;s view of the fingers gets blocked. How much more effective would a leap motion device with <em>two</em> cameras, one on either side of the monitor, be? Or just a wider camera angle (since it&rsquo;s using dual emitters anyway to get depth information). As it is right now, it&rsquo;s cute and futuristic looking, but any minority-report-esque gestures completely break it.</p><p>A much more unexpected issue is the fact that the device makes no attempt to deal with fingers that are held together, despite the fact that the shape of two touching fingers is fairly easy to figure out. If you&rsquo;ve been tracking one finger and suddenly it meets up with another one and you end up with one really wide &ldquo;finger&rdquo;, it should be trivial to send this information as a &ldquo;double finger&rdquo; pointing, because this is an incredibly useful gesture (see below for details). It&rsquo;s not like the finger is going to change into a hippopotamus all of a sudden.</p><h5 id=3d-gestures>3D Gestures</h5><p>Most of the other problems are software oriented, not hardware or driver related. The simple fact is that no one has any idea how to properly utilize the device. Most apps seem to assume there&rsquo;s only going to be one finger pointed at the screen, and will get confused if you have any other fingers hanging off even if they&rsquo;re not pointed at the screen. The Leap Motion driver gives you a rough estimate of what it thinks the hand orientation is, and using this information its fairly trivial to figure out which finger someone is pointing with, even if their other fingers are only slightly below that finger. The camera is on a flat surface at a 90 degree angle from the screen, which means the finger you want is almost always going to be the finger that is the &ldquo;highest&rdquo; in relation to the hand orientation.</p><p>The rest of the apps either don&rsquo;t do much processing on the raw data, or do an incredibly bad job of it. The touch screen recreation had this horrible tendency to start sliding off, and the cursor would sometimes jump around. I knew it shouldn&rsquo;t be doing that because I&rsquo;ve been watching the diagnostic information and it&rsquo;s just getting confused by ghost fingers popping in and out. Guess what? A finger can&rsquo;t teleport. If you&rsquo;ve been tracking one finger, chances are you should keep tracking it no matter what else happens.</p><p>In addition, no one&rsquo;s really come up with effective 3D gestures. All the apps try to use the 3D plane by turning it into a virtual touchscreen and it doesn&rsquo;t work very well. Instead, we should be using gestures that are independent of a 2D plane. The apps need to be paying attention to the angle the finger is pointing in instead of trying to rely on moving your entire hand around. It seems like everyone is using the position information instead of the angle information, which the driver provides. If I&rsquo;m pointing up and to the right, my cursor should be in the corner of the screen no matter where my hand is.</p><p>Furthermore, no one seems to be trying very hard to deal with noise. When the driver starts losing precision, you can tell because the angle information or position information will start to jitter. The more jittering, the less accurate the reading is. A simple way to quantify this is by calculating the <a href=http://en.wikipedia.org/wiki/Variance>variance</a> of the recent data. The higher the variance, the less accurate your reading is. You should then scale how heavily you average out the points based on how much variance there is in the data. This allows you to avoid destroying precision while preventing the cursor from exploding when you lose accuracy. There&rsquo;s a lot of algorithms dedicated to solving this exact problem, so it&rsquo;s kind of ridiculous that no one is using them. In a more specific scenario, humans do precise manipulations with their fingers, not their hands. Small pertubations in the input data that occur in the hands should be removed from the finger positions, because it&rsquo;s just the hand trembling, not the actual finger.</p><p>Instead of trying to recreate &ldquo;pushing a button&rdquo; in 3D, we should use a gesture interface that lets you move the cursor around with your index finger, and then when you bring up your middle finger against your index finger (so you are now pointing at the screen with two fingers), this counts as a &ldquo;click&rdquo; or a mouse-down. You can then drag things around with two fingers, or separate them to generate a mouse-up event. To right-click, bring your two fingers together again, and then rotate your hand clockwise. This could be combined with a more traditional &ldquo;pushing a button&rdquo; gesture for easier clicks, but provides a much more robust way of moving things around that isn&rsquo;t awkward or difficult to use.</p><p>Since the driver currently doesn&rsquo;t let you track two fingers held together, this same behavior can be emulated by simply keeping track of two fingers and detecting when both of them accelerate towards each other until one of them vanishes. On this same note, because fingers can drop out of tracking, programs can&rsquo;t rely on the driver to be perfect. If you lose track of a finger, it&rsquo;s not that hard to guess where it might be based on the orientation of the hand, which is necessary if we&rsquo;re going to have good gesture recognition. The recognizer needs to be able to interpolate likely positions of lost fingers if it&rsquo;s going to make an effective guess about what gesture was intended.</p><p>Leap Motion is really cool. Leap Motion is definitely the future. Unfortunately, we haven&rsquo;t gotten to the future yet. We need a better software framework to build gesture interfaces on top of, and we need a new standard set of gestures for 3D that aren&rsquo;t simply 2D ripoffs. Incidentally, I&rsquo;d love to try building an open-source library that addresses a lot of the input sanitation problems I outlined in this article, but I&rsquo;m a bit busy with the whole &ldquo;finding a job so I don&rsquo;t starve to death&rdquo; thing. Hopefully, someone else will take up the cause, but until they do, don&rsquo;t expect to be very productive with the Leap Motion.</p></article></div></section><section><div class=dim><aside><h2>Aurora Theory Released!</h2><ul></ul></aside><article><p>Aurora Theory has been released! Buy it on <a href=http://erikmcclure.bandcamp.com/album/aurora-theory>bandcamp</a> for $9, or $10 on <a href=https://itunes.apple.com/us/album/id672763525>iTunes</a>, <a href=http://www.amazon.com/gp/product/B00DVS2EDO/>Amazon</a>, and <a href="https://play.google.com/store/music/album/Erik_McClure_Aurora_Theory?id=Bcsjkg4chdqymqrpsxagjxvkm2i">Google Play</a>. The album is also available on Spotify, last.fm, other online radios, and can be previewed on <a href="http://www.youtube.com/watch?v=qD5zdsIOkqQ&amp;list=PLo4BIkE52kLTO0Hh9ROqJUXy9OdtT5-rT">YouTube</a>.</p><p>Aurora Theory has been 4 years in the making, a compilation of all the songs I managed to make in the middle of attending university. The earlier songs have been extensively improved, and all songs have been remastered for the album&rsquo;s release. This album represents nothing less than the highest quality work I can possibly manage at this point in my career. I&rsquo;ve acquired a fair number of fans over the years, and I want to thank you for supporting my musical endeavors so far.</p><p><strong>Track List:</strong></p><ol><li><a href=http://erikmcclure.bandcamp.com/track/soar-original-mix>Soar (Original Mix)</a> [5:49]</li><li><a href=http://erikmcclure.bandcamp.com/track/aurora-theory-redux>Aurora Theory (Redux)</a> [4:10]</li><li><a href=http://erikmcclure.bandcamp.com/track/cosminox>Cosminox</a> [5:41]</li><li><a href=http://erikmcclure.bandcamp.com/track/tendril>Tendril</a> [5:32]</li><li><a href=http://erikmcclure.bandcamp.com/track/birefringence>Birefringence</a> [3:57]</li><li><a href=http://erikmcclure.bandcamp.com/track/critical-damage-extended-mix>Critical Damage (Extended Mix)</a> [3:45]</li><li><a href=http://erikmcclure.bandcamp.com/track/starstruck>Starstruck</a> [4:22]</li><li><a href=http://erikmcclure.bandcamp.com/track/dream-among-the-stars>Dream Among The Stars</a> [4:06]</li><li><a href=http://erikmcclure.bandcamp.com/track/at-the-nationals>At The Nationals</a> [4:02]</li><li><a href=http://erikmcclure.bandcamp.com/track/the-cloud-river>The Cloud River</a> [5:38]</li><li><a href=http://erikmcclure.bandcamp.com/track/the-piano-and-the-cello>The Piano And The Cello</a> [3:53]</li><li><a href=http://erikmcclure.bandcamp.com/track/snowflower>Snowflower</a> [5:53]</li><li><a href=http://erikmcclure.bandcamp.com/track/spiral-nebula>Spiral Nebula</a> [5:44]</li><li><a href=http://erikmcclure.bandcamp.com/track/next-year>Next Year</a> [5:31]</li></ol></article></div></section><section><div class=dim><aside><h2>What I Learned In College</h2><ul></ul></aside><article><blockquote>*"In times of change, learners inherit the earth, while the learned find themselves beautifully equipped to deal with a world that no longer exists."* ― Eric Hoffer</blockquote><p>Yesterday, the University of Washington finally mailed me my diploma. A Bachelor of Science in <em>Applied Computational Math and Science: Discrete Math and Algorithms</em>. I learned a lot of things in college. I learned how to take tests and how to pinpoint exactly what useless crap a particular final needed me to memorize. I learned that math is an incredibly beautiful thing that has been butchered so badly I hated it all the way until my second year of college. I learned that creativity is useless and all problems have one specific right answer you can find in the back of a textbook somewhere, because that&rsquo;s all I was ever graded on. I learned that getting into the CSE major is more about fighting an enormous, broken bureaucratic mess than actually being good at computer science. But most of all, I learned that our educational system is so obsessed with itself it can&rsquo;t even recognize it&rsquo;s own shortcomings.</p><p>The first accelerated program I was accepted into was the Gifted program in middle school. I went from getting As in everything to failing every single one of my core classes. Determined to prove myself, I managed to recover my grades to Bs and Cs by the end of 7th grade, and by the end of 8th grade I was back up to As and Bs. I didn&rsquo;t do this by getting smarter, I did it by getting better at <em>following directions</em>. I got better at taking tests. I became adept at figuring out precisely what the teacher wanted me to do, and then doing only that, so I could maximize both my free time and my grades. By the time I reached high school, I would always meticulously go over the project requirements, systematically satisfying each bullet point in order to maximize my score. During tests, I not only skipped over difficult questions, I would actively seek out hints in the later questions to help me narrow down possible answers. My ability to squeeze out high grades had more to do with my aptitude at filling in the right bubbles on a piece of paper then actually understanding the material.</p><p>I fantasized about attending college, where I would be judged on my intellectual prowess, and not on my test taking skills. I longed for the pursuit of knowledge in it&rsquo;s purest form, only for this dream to be completely and utterly crushed. Instead of a place free from the endless battery of tests I had been subjected to during high school, I quickly realized that college was nothing <em>but</em> tests. I once had a math course where <em>95%</em> of my grade was split between a first midterm, a second midterm, and a final. By the end of my second year of college, I simply stopped attending lectures. I could teach myself the material out of the textbook, and went to class only to take a test or turn in homework. I earned my degree by becoming incredibly adept at memorizing precisely which useless, specific facts were needed to properly answer questions. I was never asked nor told how to apply these to real world scenarios.</p><p>Thankfully, in one of the last classes I actually attended lecture in, the TA teaching the class said something that sparked a epiphany in me: <em>&ldquo;Math is simply repeated abstraction and generalization.&rdquo;</em> Suddenly, I was able to connect math and programming, and began to realize that I had loved math all my life. What I hated about math was the trivial nonsense they taught in middle school. I signed up for the most advanced math classes I could get away with, even when I could barely pass them. I began to realize that the most important thing these classes taught me was <em>what I didn&rsquo;t know</em>. Once I knew what I didn&rsquo;t know, I could teach it to myself, but only after I found the holes in my knowledge. You can&rsquo;t fill a hole if you don&rsquo;t know where it is. I didn&rsquo;t know what combinatorics <em>was</em> until it was mentioned to me by that TA; Chrome still doesn&rsquo;t think combinatorics is even a word.</p><p>Everyone finds the beauty of math in their own way, but we teach it like an automated assembly line of cars. Math is taught as some kind of rigid tool, when it is really a language for expressing logic, one with multiple dialects, each with their own personality. We invented music to express emotions that cannot be described; we invented math to express logical abstractions that defy explanation. Every tool in math is like another instrument in a grand orchestra, each note echoing off the others, reflecting a whole that is greater than the sum of its parts. Some composers prefer the string section, others prefer the woodwinds. There is no single right answer, only different ones. Instead of giving our children a brush and telling them to use their imagination, we give them a coloring book and grade them on how well they stay inside the lines.</p><p>I mean, we all know creativity is overrated. It must be, since we systematically destroy it even when we try to encourage it. It doesn&rsquo;t matter how many programs we fund for encouraging things like art and music when the kids are still ultimately judged on how well they follow instructions and fill in little scantron bubbles. Kids are not stupid. I cannot believe how many adults still think they can get away with telling kids one thing and then doing another. They know what you&rsquo;re up to. They know the only thing the school system cares about is their grades, and that their grades are based entirely on how well they follow directions. They know that answering a question &ldquo;almost right&rdquo; doesn&rsquo;t matter. They know all problems have one answer in the back of the teacher&rsquo;s textbook, and their job is to figure out what it is. The vast majority of them have absolutely no idea how to approach a problem that has no correct answer. They don&rsquo;t know how to judge the correctness of a solution, because in school, everything is either right or wrong. All they know how to do is guess how likely it is that their solution is the solution the teacher wants, not how well the solution would <em>actually work</em>.</p><p>This is, of course, completely contradictory to everything in life. Life does not have answers in the back of the book. Life does not have a single correct answer to any problem. There is no right way to do <em>anything</em>, there are simply pros and cons. The fact that many people continue to delude themselves into thinking otherwise is a sad symptom of this issue. Our obsession with tests has trained a generation of robots, not engineers. They&rsquo;re more skilled at working their way through a bureaucracy than designing rockets. Then again, considering that colleges have now turned into enormous, obstructive bureaucracies, perhaps this isn&rsquo;t entirely a bad thing.</p><p>After all, with only 160 (now 200) spots open in its CSE major program each year when it has over 27000 undergraduate students enrolled<sup><a href=http://admit.washington.edu/quickfacts#enrollment>[1]</a></sup>, the University of Washington gets mighty picky about who they let in. After getting a 3.7 and 3.4 in my first two calculus classes, I slipped and got a 2.8 in my third math class, so despite the fact that I got a perfect 5 on the AP Computer Science AB exam and was qualified to skip both introductory programming courses, they rejected my application and demanded I take Matrix Algebra before letting me in. So I got a 3.9 in Matrix Algebra (a grade that was exceptionally good, according to one professor), and then&mldr; they still didn&rsquo;t let me in. They complained that my entrance essay sounded &ldquo;too cocky&rdquo; and had me take the second introductory programming course even though I already had credit for it. When I failed to get an exceptionally good grade in that class for all the wrong reasons (like being graded down for having both too few comments and <em>too many</em> comments in my code), I simply could not bring myself to compete in a hyper-competitive environment where the only thing I was judged on was how many irrelevant details I could regurgitate. So, I majored in Applied Mathematics and simply took all the condensed, non-major CSE courses instead.</p><p>This obsession with tests extends into the evaluation of the educational system itself. One of the reasons nothing is getting better is because we use the very thing that is wrong with the educational system to judge it. We fill out ridiculous polls made out of those same interminable bubbles that are destroying the curriculum. We introduce more standardized testing. The entire system is completely obsessed with tests, and yet the only place that tests actually exist is&mldr; inside the system itself. Education has become so enraptured with this imaginary world it has constructed, it&rsquo;s completely forgotten about the reality it&rsquo;s <em>supposed</em> to be teaching kids about.</p><p>Kids know this imaginary world has nothing to do with reality. We lament about how to teach kids math when they <a href=http://math.stackexchange.com/questions/416226/my-sister-absolutely-refuses-to-learn-math>refuse to understand it</a>, without realizing that they are simply applying the same method of learning they use in <em>everything else</em> - memorize useless facts, then regurgitate them on a test. The reason our math curriculum is failing so badly is because in math, you can&rsquo;t simply memorize things, you need to <em>understand</em> them. Consequently, Math acts as a canary in the coal mine for our entire educational system. Kids make no effort to understand anything, because they aren&rsquo;t graded on how well they understand concepts, they are graded on how well they memorize random, useless details and follow directions.</p><p>We live in a world being overrun by automation. Any task that can be reduced to simply following a set of instructions over and over is being done by robots and software. This constant attrition of jobs involving menial work and physical labor will continue at a steady pace for the foreseeable future. We are teaching our kids skills that are being made irrelevant in a modern economy. We are preparing our children for a world that no longer exists. At the same time, while I could write a series of blog posts outlining an effective educational system, it will never be implemented in public schools. The establishment is too deeply entrenched. Foolish startups repeatedly and continually attempt to &ldquo;disrupt&rdquo; the educational system without realizing just how laughably outmatched they are. This is not something you can fix with a cute website. People complain about global warming, space travel, all sorts of adorable little problems, but miss the elephant in the room.</p><p><strong>The greatest challenge our species has ever faced is the educational system itself.</strong></p></article></div></section><section><div class=dim><aside><h2>Course Notes</h2><ul></ul></aside><article><div class=math>It's standard procedure at the University of Washington to allow a single sheet of handwritten notes during a Mathematics exam. I started collecting these sheets after I realized how useful it was to have a reference that basically summarized all the useful parts of the course on a single sheet of paper. Now that I've graduated, it's easy for me to quickly forget all the things I'm not using. The problem is that, when I need to say, develop an algorithm for simulating turbulent airflow, I need to go back and re-learn vector calculus, differential equations and nonlinear dynamics. So I've decided to digitize all my notes and put them in one place where I can reference them. I've uploaded it here in case they come in handy to anyone else.<br><br>The earlier courses listed here had to be reconstructed from my class notes because I'd thrown my final notesheet away or otherwise lost it. The classes are not listed in the order I took them, but rather organized into related groups. This post may be updated later with expanded explanations for some concepts, but these are highly condensed notes for reference, and a lot of it won't make sense to someone who hasn't taken a related course.<br><br><span style=font-size:120%><b>Math 124 - Calculus I</b></span><br><i>lost</i><br><br><span style=font-size:120%><b>Math 125 - Calculus II</b></span><br><i>lost</i><br><br><span style=font-size:120%><b>Math 126 - Calculus III</b></span><br><i>lost</i><br><br><span style=font-size:120%><b>Math 324 - Multivariable Calculus I</b></span><br><div style=display:inline-block;margin-right:2em>\[ r^2 = x^2 + y^2 \]</div><div style=display:inline-block;margin-right:2em>\[ x= r\cos\theta \]</div><div style=display:inline-block;margin-right:2em>\[ y=r\sin\theta \]</div><br><div style=display:inline-block;margin-right:2em>\[ \iint\limits_R f(x,y)\,dA = \int_\alpha^\beta\int_a^b f(r\cos\theta,r\sin\theta)r\,dr\,d\theta=\int_\alpha^\beta\int_{h_1(\theta}^{h_2(\theta)} f(r\cos\theta,r\sin\theta)r\,dr\,d\theta \]</div><br><div style=display:inline-block;margin-right:2em>\[ m=\iint\limits_D p(x,y)\,dA \begin{cases} \begin{aligned} M_x=\iint\limits_D y p(x,y)\,dA & \bar{x}=\frac{M_y}{m}=\frac{1}{m}\iint x p(x,y)\,dA \end{aligned} \\ \begin{aligned} M_y=\iint\limits_D x p(x,y)\,dA & \bar{y}=\frac{M_x}{m}=\frac{1}{m}\iint y p(x,y)\,dA \end{aligned} \end{cases} \]</div><br><div style=display:inline-block;margin-right:2em>\[ Q = \iint\limits_D \sigma(x,y)\,dA \]</div><div style=display:inline-block;margin-right:2em>\[ I_x = \iint\limits_D y^2 p(x,y)\,dA \]</div><div style=display:inline-block;margin-right:2em>\[ I_y = \iint\limits_D x^2 p(x,y)\,dA \]</div><div style=display:inline-block>\[ I_0 = \iint\limits_D (x^2 + y^2) p(x,y)\,dA \]</div><br><div style=display:inline-block;margin-right:2em>\[ \iiint f(x,y,z) dV = \lim_{l,m,n\to\infty}\sum_{i=1}^l\sum_{j=1}^m\sum_{k=1}^n f(x_i,y_j,z_k) \delta V \]</div><div style=display:inline-block;margin-right:2em>\[ \iiint\limits_B f(x,y,z)\,dV=\int_r^s\int_d^c\int_a^b f(x,y,z)\,dx\,dy\,dz = \int_a^b\int_r^s\int_d^c f(x,y,z)\,dy\,dz\,dx \]</div><br>$$E$$ = general bounded region<br>Type 1: $$E$$ is between graphs of two continuous functions of $$x$$ and $$y$$.<div style=display:inline-block;margin-right:2em>\[ E=\{(x,y,z)|(x,y)\in D, u_1(x,y) \le z \le u_2(x,y)\} \]</div><br>$$D$$ is the projection of E on to the xy-plane, where<div style=display:inline-block;margin-right:2em>\[ \iiint\limits_E f(x,y,z)\,dV = \iint\limits_D\left[\int_{u_1(x,y)}^{u_2(x,y)} f(x,y,z)\,dz \right]\,dA \]</div><br>$$D$$ is a type 1 planar region:<div style=display:inline-block;margin-right:2em>\[ \iiint\limits_E f(x,y,z)\,dV = \int_a^b \int_{g_1(x)}^{g_2(x)} \int_{u_1(x,y)}^{u_2(x,y)} f(x,y,z)\,dz\,dy\,dx \]</div><br>$$D$$ is a type 2 planar region:<div style=display:inline-block;margin-right:2em>\[ \iiint\limits_E f(x,y,z)\,dV = \int_d^c \int_{h_1(y)}^{h_2(y)} \int_{u_1(x,y)}^{u_2(x,y)} f(x,y,z)\,dz\,dx\,dy \]</div><br><br>Type 2: $$E$$ is between $$y$$ and $$z$$, $$D$$ is projected on to $$yz$$-plane<div style=display:inline-block;margin-right:2em>\[ E=\{(x,y,z)|(y,z)\in D, u_1(y,z) \le x \le u_2(y,z)\} \]</div><div style=display:inline-block;margin-right:2em>\[ \iiint\limits_E f(x,y,z)\,dV = \iint\limits_D\left[\int_{u_1(y,z)}^{u_2(y,z)} f(x,y,z)\,dx \right]\,dA \]</div><br>Type 3: $$E$$ is between $$x$$ and $$z$$, $$D$$ is projected on to $$xz$$-plane<div style=display:inline-block;margin-right:2em>\[ E=\{(x,y,z)|(x,z)\in D, u_1(x,z) \le y \le u_2(x,z)\} \]</div><div style=display:inline-block;margin-right:2em>\[ \iiint\limits_E f(x,y,z)\,dV = \iint\limits_D\left[\int_{u_1(x,z)}^{u_2(x,z)} f(x,y,z)\,dy \right]\,dA \]</div><br>Mass<div style=display:inline-block;margin-right:2em>\[ m = \iiint\limits_E p(x,y,z)\,dV \]</div><div style=display:inline-block;margin-right:2em>\[ \bar{x} = \frac{1}{m}\iiint\limits_E x p(x,y,z)\,dV \]</div><div style=display:inline-block;margin-right:2em>\[ \bar{y} = \frac{1}{m}\iiint\limits_E y p(x,y,z)\,dV \]</div><div style=display:inline-block;margin-right:2em>\[ \bar{z} = \frac{1}{m}\iiint\limits_E z p(x,y,z)\,dV \]</div>Center of mass: $$(\bar{x},\bar{y},\bar{z})$$<br><div style=display:inline-block;margin-right:2em>\[ Q = \iiint\limits_E \sigma(x,y,z)\,dV \]</div><div style=display:inline-block;margin-right:2em>\[ I_x = \iiint\limits_E (y^2 + z^2) p(x,y,z)\,dV \]</div><div style=display:inline-block;margin-right:2em>\[ I_y = \iiint\limits_E (x^2 + z^2) p(x,y,z)\,dV \]</div><div style=display:inline-block;margin-right:2em>\[ I_z = \iiint\limits_E (x^2 + y^2) p(x,y,z)\,dV \]</div><br>Spherical Coordinates:<div style=display:inline-block;margin-right:2em>\[ z=p\cos\phi \]</div><div style=display:inline-block;margin-right:2em>\[ r=p\sin\phi \]</div><div style=display:inline-block;margin-right:2em>\[ dV=p^2\sin\phi \]</div><div style=display:inline-block;margin-right:2em>\[ x=p\sin\phi\cos\theta \]</div><div style=display:inline-block;margin-right:2em>\[ y=p\sin\phi\sin\theta \]</div><div style=display:inline-block;margin-right:2em>\[ p^2 = x^2 + y^2 + z^2 \]</div><div style=display:inline-block;margin-right:2em>\[ \iiint\limits_E f(x,y,z)\,dV = \int_c^d\int_\alpha^\beta\int_a^b f(p\sin\phi\cos\theta,p\sin\phi\sin\theta,p\cos\phi) p^2\sin\phi\,dp\,d\theta\,d\phi \]</div><br>Jacobian of a transformation $$T$$ given by $$x=g(u,v)$$ and $$y=h(u,v)$$ is:<div style=display:inline-block;margin-right:2em>\[ \begin{aligned} \frac{\partial (x,y)}{\partial (u,v)} = \begin{vmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\[0.1em] \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{vmatrix} = \frac{\partial x}{\partial u} \frac{\partial y}{\partial v} - \frac{\partial x}{\partial v} \frac{\partial y}{\partial u} \end{aligned} \]</div><br>Given a transformation T whose Jacobian is nonzero, and is one to one:<div style=display:inline-block;margin-right:2em>\[ \iint\limits_R f(x,y)\,dA = \iint\limits_S f\left(x(u,v),y(u,v)\right)\left|\frac{\partial (x,y)}{\partial (u,v)}\right|\,du\,dv \]</div><br>Polar coordinates are just a special case:<div style=display:inline-block;margin-right:2em>\[ x = g(r,\theta)=r\cos\theta \]</div><div style=display:inline-block;margin-right:2em>\[ y = h(r,\theta)=r\sin\theta \]</div><br><div style=display:inline-block;margin-right:2em>\[ \frac{\partial (x,y)}{\partial (u,v)} = \begin{vmatrix} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} \end{vmatrix} = \begin{vmatrix} \cos\theta & -r\sin\theta \\ \sin\theta & r\cos\theta \end{vmatrix} = r\cos^2\theta + r\sin^2\theta=r(\cos^2\theta + \sin^2\theta) = r \]</div><br><div style=display:inline-block;margin-right:2em>\[ \iint\limits_R f(x,y)\,dx\,dy = \iint\limits_S f(r\cos\theta, r\sin\theta)\left|\frac{\partial (x,y)}{\partial (u,v)}\right|\,dr\,d\theta=\int_\alpha^\beta\int_a^b f(r\cos\theta,r\sin\theta)|r|\,dr\,d\theta\]</div><br>For 3 variables this expands as you would expect: $$x=g(u,v,w)$$ $$y=h(u,v,w)$$ $$z=k(u,v,w)$$<div style=display:inline-block;margin-right:2em>\[ \frac{\partial (x,y,z)}{\partial (u,v,w)}=\begin{vmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} & \frac{\partial x}{\partial w} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} & \frac{\partial y}{\partial w} \\ \frac{\partial z}{\partial u} & \frac{\partial z}{\partial v} & \frac{\partial z}{\partial w} \end{vmatrix} \]</div><br><div style=display:inline-block;margin-right:2em>\[ \iiint\limits_R f(x,y,z)\,dV = \iiint\limits_S f(g(u,v,w),h(u,v,w),k(u,v,w)) \left|\frac{\partial (x,y,z)}{\partial (u,v,w)} \right| \,du\,dv\,dw\]</div><br><ins>Line Integrals</ins><br>Parameterize: $$r(t)=\langle x(t),y(t),z(t) \rangle$$ where $$r'(t)=\langle x'(t),y'(t),z'(t) \rangle$$ and $$|r'(t)|=\sqrt{x'(t)^2 + y'(t)^2 + z'(t)^2} $$<br><div style=display:inline-block;margin-right:2em>\[ \int_C f(x,y,z)\,ds = \int_a^b f(r(t))\cdot |r'(t)|\,dt = \int_a^b f(x(t),y(t),z(t))\cdot\sqrt{x'(t)^2 + y'(t)^2 + z'(t)^2}\,dt\;\;\;a&lt;t&lt;b \]</div><br>For a vector function $$\mathbf{F}$$:<div style=display:inline-block;margin-right:2em>\[ \int_C \mathbf{F}\cdot dr = \int_a^b \mathbf{F}(r(t))\cdot r'(t)\,dt \]</div><br><ins>Surface Integrals</ins><div style=display:inline-block;margin-right:2em>\[ \]</div><br>Parameterize: $$r(u,v) = \langle x(u,v),y(u,v),z(u,v) \rangle$$<div style=display:inline-block>\[ \begin{matrix} r_u=\frac{\partial x}{\partial u}\vec{\imath} + \frac{\partial y}{\partial u}\vec{\jmath} + \frac{\partial z}{\partial u}\vec{k} \\ r_v=\frac{\partial x}{\partial v}\vec{\imath} + \frac{\partial y}{\partial v}\vec{\jmath} + \frac{\partial z}{\partial v}\vec{k} \end{matrix}\]</div><div style=display:inline-block>\[ r_u \times r_v = \begin{vmatrix} \vec{\imath} & \vec{\jmath} & \vec{k} \\ \frac{\partial x}{\partial u} & \frac{\partial y}{\partial u} & \frac{\partial z}{\partial u} \\ \frac{\partial x}{\partial v} & \frac{\partial y}{\partial v} & \frac{\partial z}{\partial v} \end{vmatrix} \]</div><br><div style=display:inline-block;margin-right:2em>\[ \iint\limits_S f(x,y,z) dS = \iint\limits_D f(r(t))|r_u \times r_v|\,dA \]</div><br>For a vector function $$\mathbf{F}$$:<div style=display:inline-block;margin-right:2em>\[ \iint\limits_S \mathbf{F}\cdot dr = \iint\limits_D \mathbf{F}(r(u,v))\cdot (r_u \times r_v)\,dA) \]</div><br>Any surface $$S$$ with $$z=g(x,y)$$ is equivalent to $$x=x$$, $$y=y$$, and $$z=g(x,y)$$, so<br>$$xy$$ plane:<div style=display:inline-block;margin-right:2em>\[ \iint\limits_S f(x,y,z)\,dS = \iint\limits_D f(x,y,g(x,y))\sqrt{\left(\frac{\partial z}{\partial x}\right)^2+\left(\frac{\partial z}{\partial y}\right)^2+1}\,dA \]</div><br>$$yz$$ plane:<div style=display:inline-block;margin-right:2em>\[ \iint\limits_S f(x,y,z)\,dS = \iint\limits_D f(x,h(x,z),z)\sqrt{\left(\frac{\partial y}{\partial x}\right)^2+\left(\frac{\partial y}{\partial z}\right)^2+1}\,dA \]</div><br>$$xz$$ plane:<div style=display:inline-block;margin-right:2em>\[ \iint\limits_S f(x,y,z)\,dS = \iint\limits_D f(g(y,z),y,z)\sqrt{\left(\frac{\partial x}{\partial y}\right)^2+\left(\frac{\partial x}{\partial z}\right)^2+1}\,dA \]</div><br>Flux:<div style=display:inline-block;margin-right:2em>\[ \iint\limits_S\mathbf{F}\cdot dS = \iint\limits_D\mathbf{F}\cdot (r_u \times r_v)\,dA \]</div><br>The gradient of $$f$$ is the vector function $$\nabla f$$ defined by:<div style=display:inline-block;margin-right:2em>\[ \nabla f(x,y)=\langle f_x(x,y),f_y(x,y)\rangle = \frac{\partial f}{\partial x}\vec{\imath} + \frac{\partial f}{\partial y}\vec{\jmath} \]</div><br>Directional Derivative:<div style=display:inline-block;margin-right:2em>\[D_u\,f(x,y) = f_x(x,y)a + f_y(x,y)b = \nabla f(x,y)\cdot u \text{ where } u = \langle a,b \rangle \]</div><div style=display:inline-block;margin-right:2em>\[ \int_C\,ds=\int_a^b |r'(t)|=L \]</div><div style=display:inline-block;margin-right:2em>\[ \]</div><br>If $$\nabla f$$ is conservative, then:<div style=display:inline-block;margin-right:2em>\[ \int_{c_1} \nabla f\,dr=\int_{c_2} \nabla f\,dr \]</div><br>This means that the line integral between two points will always be the same, no matter what curve is used to go between the two points - the integrals are path-independent and consequently only depend on the starting and ending positions in the conservative vector field.<br>A vector function is conservative if it can be expressed as the gradient of some potential function $$\psi$$: $$\nabla \psi = \mathbf{F}$$<br><div style=display:inline-block;margin-right:2em>\[ curl\,\mathbf{F} =\nabla\times\mathbf{F}\]</div><div style=display:inline-block;margin-right:2em>\[ div\,\mathbf{F} =\nabla\cdot\mathbf{F}\]</div><br><br><span style=font-size:120%><b>Math 326 - Multivariable Calculus II</b></span><br>$$f(x,y)$$ is continuous at a point $$(x_0,y_0)$$ if<div style=display:inline-block>\[ \lim\limits_{(x,y)\to(0,0)} f(x,y) = f(x_0,y_0) \]</div><br>$$f+g$$ is continuous if $$f$$ and $$g$$ are continuous, as is $$\frac{f}{g}$$ if $$g \neq 0$$<br>A composite function of a continuous function is continuous<div style=display:inline-block;margin-right:2em>\[ \frac{\partial f}{\partial x} = f_x(x,y) \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{\partial f}{\partial x}\bigg|_{(x_0,y_0)}=\left(\frac{\partial f}{\partial x}\right)_{(x_0,y_0)}=f_x(x_0,y_0) \]</div><br>To find $$\frac{\partial z}{\partial x} F(x,y,z)$$, differentiate $$x$$ as normal, hold y constant, and differentiate $$z$$ as a function (such that $$z^2 = 2z \frac{\partial z}{\partial x}$$ and $$2z = 2 \frac{\partial z}{\partial x}$$)<br>Ex:<div style=display:inline-block;margin-right:2em>\[ F(x,y,z) = \frac{x^2}{16} + \frac{y^2}{12} + \frac{z^2}{9} = 1 \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{\partial z}{\partial x} F = \frac{2x}{16} + \frac{2z}{}\frac{\partial z}{\partial x} = 0 \]</div><br>The tangent plane of $$S$$ at $$(a,b,c): z-c = f_x(a,b)(x-a) + f_y(a,b)(y-b)$$ where $$z=f(x,y)$$, or $$z =f(a,b) + f_x(a,b)(x-a) + f_y(a,b)(y-b)$$<br>Note that<div style=display:inline-block>\[ f_x(a,b)=\frac{\partial z}{\partial x}\bigg|_{(a,b)} \]</div>which enables you to find tangent planes implicitly.<br>Set $$z=f(x,y)$$. $$f_x=f_y=0$$ at a relative extreme $$(a,b)$$.<br>Distance from origin:<div style=display:inline-block;margin-right:2em>\[ D^2 = z^2 + y^2 + x^2 = f(a,b)^2 + y^2 + x^2 \]</div><br>Minimize $$D$$ to get point closest to the origin.<br>The differential of $$f$$ at $$(x,y)$$:<div style=display:inline-block;margin-right:2em>\[ df(x,y;dx,dy)=f_x(x,y)\,dx + f_y(x,y)\,dy \]</div><div style=display:inline-block;margin-right:2em>\[ dz=\frac{\partial f}{\partial x}\,dx + \frac{\partial f}{\partial y}\,dy \]</div><br>$$f$$ is called differentiable at $$(x,y)$$ if it $$s$$ defined for all points near $$(x,y)$$ and if there exists numbers $$A$$,$$B$$ such that<div style=display:inline-block;margin-right:2em>\[ \lim_{(n,k)\to(0,0)}\frac{|f(x+h,y+k) - f(x,y) - Ah - Bk|}{\sqrt{h^2 + k^2}}=0 \]</div><br>If $$f$$ is differentiable at $$(x,y)$$ it is continuous there.<div style=display:inline-block;margin-left:2em>\[ A = f_x(x,y) \]</div>and<div style=display:inline-block>\[ B=f_y(x,y) \]</div><br>If $$F_x(x,y)$$ and $$F_y(x,y)$$ are continuous at a point $$(x_0,y_0)$$ defined in $$F$$, then $$F$$ is differentiable there.<br>Ex: The differential of $$f(x,y)=3x^2y+2xy^2+1$$ at $$(1,2)$$ is $$df(1,2;h,k)=20h+11k$$<div style=display:inline-block;margin-right:2em>\[ d(u+v)=du+dv \]</div><div style=display:inline-block;margin-right:2em>\[ d(uv)=u\,dv + v\,du \]</div><div style=display:inline-block>\[ d\left(\frac{u}{v}\right)=\frac{v\,du -u\,dv}{v^2} \]</div><br>Taylor Series:<div style=display:inline-block;margin-right:2em>\[ f^{(n)}(t)=\left[\left(h\frac{}{} + k\frac{}{})^n F(x,y) \right) \right]_{x=a+th,y=b+tk} \]</div>Note:<div style=display:inline-block>\[ f''(t)=h^2 F_{xx} + 2hk F_{xy} + k^2 F_{yy} \]</div><br><div style=display:inline-block;margin-right:2em>\[ \begin{matrix} x=f(u,v) \\ y=g(u,v) \end{matrix} \]</div><div style=display:inline-block;margin-right:2em>\[ J=\frac{\partial(f,g)}{\partial(u,v)} \]</div><div style=display:inline-block;margin-right:2em>\[ \begin{matrix} u=F(x,y) \\ v=G(x,y) \end{matrix} \]</div><div style=display:inline-block>\[ j = J^{-1} = \frac{1}{\frac{\partial(f,g)}{\partial(u,v)}} \]</div><br><div style=display:inline-block;margin-right:2em>\[ \begin{matrix} x = u-uv \\ y=uv \end{matrix} \]</div><div style=display:inline-block;margin-right:2em>\[ \iint\limits_R\frac{dx\,dy}{x+y} \]</div>R bounded by<div style=display:inline-block;margin-right:2em>\[ \begin{matrix} x+y=1 & x+y=4 \\ y=0 & x=0 \end{matrix}\]</div><div style=display:inline-block>\[ \int_0^1\int_0^x \frac{du\,dv}{u-uv+ux}\left|\frac{\partial(x,y)}{\partial(u,v)}\right| \]</div><br><div style=display:inline-block;margin-right:2em>\[ \frac{\partial(F,G)}{\partial(u,v)}=\begin{vmatrix} \frac{\partial F}{\partial u} & \frac{\partial F}{\partial v} \\ \frac{\partial G}{\partial u} & \frac{\partial G}{\partial v}  \end{vmatrix} \]</div><div style=display:inline-block;margin-right:2em>\[ \nabla f = \langle f_x, f_y, f_z \rangle \]</div><div style=display:inline-block>\[ G_x(s_0,t_0) =F_x(a,b)f_x(s_0,t_0) + F_y(a,b)g_x(s_0,t_0) \]</div><div style=display:inline-block>\[ U\times V = U_xV_y - U_yV_x \text{ or } A\times B = \left\langle \begin{vmatrix} a_y & a_z \\ b_y & b_z \end{vmatrix}, -\begin{vmatrix} a_x & a_z \\ b_x & b_z \end{vmatrix}, \begin{vmatrix} a_x & a_y \\ b_x & b_y \end{vmatrix} \right\rangle \]</div><br>Given $$G(s,t)=F(f(s,t),g(s,t))$$, then:<div style=display:inline-block>\[ \begin{matrix} \frac{\partial G}{\partial s} = \frac{\partial F}{\partial x}\frac{\partial f}{\partial s} + \frac{\partial F}{\partial y}\frac{\partial g}{\partial s} \\ \frac{\partial G}{\partial t} = \frac{\partial F}{\partial x}\frac{\partial f}{\partial t} + \frac{\partial F}{\partial y}\frac{\partial g}{\partial t} \end{matrix} \]</div><br>Alternatively, $$ u=F(x,y,z)=F(f(t),g(t),h(t))$$ yields<div style=display:inline-block>\[ \frac{du}{dt}=\frac{\partial u}{\partial x}\frac{dx}{dt} + \frac{\partial u}{\partial y}\frac{dy}{dt} + \frac{\partial u}{\partial z}\frac{dz}{dt} \]</div><br>Examine limit along line $$y=mx$$:<div style=display:inline-block>\[ \lim_{x\to 0} f(x,mx) \]</div><br>If $$g_x$$ and $$g_y$$ are continuous, then $$g$$ is differentiable at that point (usually $$0,0$$).<br>Notice that if $$f_x(0,0)=0$$ and $$f_y(0,0)=0$$ then $$df(0,0;h,l)=0h+0k=0$$<br><br>The graph of $$y(x,y)$$ lies on a level surface $$F(x,y,z)=c$$ means $$f(x,y(x,z),z)=c$$. So then use the chain rule to figure out the result in terms of $$F$$ partials by considering $$F$$ a composite function $$F(x,y(x,z),z)$$.<br><br>Fundamental implicit function theorem: Let $$F(x,y,z)$$ be a function defined on an open set $$S$$ containing the point $$(x_0,y_0,z_0)$$. Suppose $$F$$ has continuous partial derivatives in $$S$$. Furthermore assume that: $$F(x_0,y_0,z_0)=0, F_z(x_0,y_0,z_0)\neq 0$$. Then $$z=f(x,y)$$ exists, is continuous, and has continuous first partial derivatives.<div style=display:inline-block;margin-right:2em>\[ f_x = -\frac{F_x}{F_z} \]</div><div style=display:inline-block;margin-right:2em>\[ f_y = -\frac{F_y}{F_z} \]</div><br>Alternatively, if<div style=display:inline-block>\[ \begin{vmatrix} F_x & F_y \\ G_x & G_y \end{vmatrix} \neq 0 \]</div>, then we can solve $$x$$ and $$y$$ as functions of $$z$$. Since the cross-product is made of these determinants, if the $$x$$ component is nonzero, you can solve $$y,z$$ as functions of $$x$$, and therefore graph it on the $$x-axis$$.<br><br>To solve level surface equations, let $$F(x,y,z)=c$$ and $$G(x,y,z)=d$$, then use the chain rule, differentiating by the remaining variable (e.g. $$\frac{dy}{dx}$$ and $$\frac{dz}{dx}$$ means do $$x$$)<br><div style=display:inline-block;margin-right:2em>\[ \begin{matrix} F_x + F_y y_x + F_z z_x = 0 \\ G_x + G_y y_x + G_z z_x = 0 \end{matrix} \]</div>if you solve for $$y_x$$,$$z_x$$, you get<div style=display:inline-block;margin-right:2em>\[ \left[ \begin{matrix} F_y & F_z \\ G_y & G_z \end{matrix} \right] \left[ \begin{matrix} y_x \\ z_x \end{matrix} \right] = \left[\begin{matrix}F_x \\ G_x \end{matrix} \right] \]</div><br>Mean value theorem:<div style=display:inline-block;margin-right:2em>\[ f(b) - f(a) = (b-a)f'(X) \]</div><div style=display:inline-block>\[ a &lt; X &lt; b \]</div><br>or:<div style=display:inline-block;margin-right:2em>\[ f(a+h)=f(a)+hf'(a + \theta h) \]</div><div style=display:inline-block>\[ 0 &lt; \theta &lt; 1 \]</div><br>xy-plane version:<div style=display:inline-block;margin-right:2em>\[ F(a+h,b+k)-F(a,b)=h F_x(a+\theta h,b+\theta k)+k F_y(a+\theta h, b+\theta k) \]</div><div style=display:inline-block>\[ 0 &lt; \theta &lt; 1 \]</div><br>Lagrange Multipliers: $$\nabla f = \lambda\nabla g$$ for some scale $$\lambda$$ if $$(x,y,z)$$ is a minimum:<div style=display:inline-block;margin-right:2em>\[ f_x=\lambda g_x \]</div><div style=display:inline-block;margin-right:2em>\[ f_y=\lambda g_y \]</div><div style=display:inline-block>\[ f_z=\lambda g_z \]</div>Set $$f=x^2 + y^2 + z^2$$ for distance and let $$g$$ be given.<br><br><span style=font-size:120%><b>Math 307 - Introduction to Differential Equations</b></span><br><i>lost</i><br><br><span style=font-size:120%><b>Math 308 - Matrix Algebra</b></span><br><i>lost</i><br><br><span style=font-size:120%><b>Math 309 - Linear Analysis</b></span><br><i>lost</i><br><br><span style=font-size:120%><b>AMath 353 - Partial Differential Equations</b></span><br><br><b>Fourier Series</b>:<div style=display:inline-block;margin-right:2em>\[ f(x)=b_0 + \sum_{n=1}^{\infty} \left(a_n \sin\frac{n\pi x}{L} + b_n\cos\frac{n\pi x}{L} \right) \]</div><div style=display:inline-block;margin-right:2em>\[ b_0 = \frac{1}{2L}\int_{-L}^L f(y)\,dy \]</div><div style=display:inline-block;margin-right:2em>\[ a_m \frac{1}{L}\int_{-L}^L f(y) \sin\frac{m\pi y}{L}\,dy \]</div><div style=display:inline-block;margin-right:2em>\[ b_m = \frac{1}{L}\int_{-L}^L f(y)\cos\frac{m\pi y}{L}\,dy \]</div><div style=display:inline-block;margin-right:2em>\[ m \ge 1 \]</div><br><div style=display:inline-block;margin-right:2em>\[ u_t=\alpha^2 u_{xx} \]</div><div style=display:inline-block;margin-right:2em>\[ \alpha^2 = \frac{k}{pc} \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,t)=F(x)G(t) \]</div><br><b>Dirichlet</b>: $$u(0,t)=u(L,t)=0$$<br><b>Neumann</b>:$$u_x(0,t)=u_x(L,t)=0$$<br><b>Robin</b>:$$a_1 u(0,t)+b_1 u_x(0,t) = a_2 u(L,t) + b_2 u_x(L,t) = 0$$<br><b>Dirichlet</b>:<div style=display:inline-block;margin-right:2em>\[ \lambda_n=\frac{n\pi}{L}\;\;\; n=1,2,... \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,t)=\sum_{n=1}^{\infty} A_n \sin\left(\frac{n\pi x}{L} \right)\exp\left(-\frac{n^2\alpha^2\pi^2 t}{L^2}\right) \]</div><div style=display:inline-block;margin-right:2em>\[ A_n = \frac{2}{L} \int_0^L f(y) \sin\frac{n\pi y}{L}\,dy = 2 a_m\text{ for } 0\text{ to } L \]</div><br><b>Neumann</b>:<div style=display:inline-block;margin-right:2em>\[ \lambda_n=\frac{n\pi}{L}\;\;\; n=1,2,... \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,t)=B_0 + \sum_{n=1}^{\infty} B_n \cos\left(\frac{n\pi x}{L} \right)\exp\left(-\frac{n^2\alpha^2\pi^2 t}{L^2}\right) \]</div><div style=display:inline-block;margin-right:2em>\[ B_0 = \frac{1}{L} \int_0^L f(y)\,dy \]</div><div style=display:inline-block;margin-right:2em>\[ B_n = \frac{2}{L} \int_0^L f(y) \cos\frac{n\pi y}{L}\,dy = 2 b_m\text{ for } 0\text{ to } L \]</div><br><b>Dirichlet/Neumann</b>:<div style=display:inline-block;margin-right:2em>\[ \lambda_n=\frac{\pi}{2L}(2n + 1)\;\;\; n=0,1,2,... \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,t)=\sum_{n=0}^{\infty} A_n \sin\left(\lambda_n x\right) \exp\left(-\alpha^2 \lambda_n^2 t\right) \]</div><div style=display:inline-block;margin-right:2em>\[ A_n = \frac{2}{L} \int_0^L f(y) \sin\left(\lambda_n y\right)\,dy \]</div><br><b>Neumann/Dirichlet</b>:<div style=display:inline-block;margin-right:2em>\[ \lambda_n=\frac{\pi}{2L}(2n + 1)\;\;\; n=0,1,2,... \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,t)=\sum_{n=0}^{\infty} B_n \cos\left(\lambda_n x\right) \exp\left(-\alpha^2 \lambda_n^2 t\right) \]</div><div style=display:inline-block;margin-right:2em>\[ B_n = \frac{2}{L}\int_0^L f(y)\cos(\lambda_n y)\,dy \]</div><br><div style=display:inline-block;margin-right:2em>\[ v_2(x,y)=\sum_{n=1}^{\infty} C_n(t)[\sin/\cos](\lambda_n x) \]</div>Replace $$[\sin/\cos]$$ with whatever was used in $$u(x,t)$$<div style=display:inline-block;margin-right:2em>\[ C_n(t)=\int_0^t p_n(s) e^{\lambda_n^2\alpha^2 (s-t)}\,ds \]</div><div style=display:inline-block;margin-right:2em>\[ p_n(t)=\frac{2}{L}\int_0^L p(y,t)[\sin/\cos](\lambda_n y)\,dy \]</div>Replace $$[\sin/\cos]$$ with whatever was used in $$u(x,t)$$. Note that $$\lambda_n$$ for $$C_n$$ and $$p_n(t)$$ is the same as used for $$u_1(x,t)$$<br><b>Sturm-Liouville</b>:<div style=display:inline-block;margin-right:2em>\[ p(x)\frac{d^2}{dx^2}+p'(x)\frac{d}{dx}+q(x) \]</div><div style=display:inline-block;margin-right:2em>\[ a_2(x)\frac{d^2}{dx^2} + a_1(x)\frac{d}{dx} + a_0(x) \rightarrow p(x)=e^{\int\frac{a_1(x)}{a_2(x)}\,dx}\]</div><div style=display:inline-block;margin-right:2em>\[ q(x)=p(x)\frac{a_0(x)}{a_2(x)} \]</div><br><b>Laplace's Equation</b>:<div style=display:inline-block;margin-right:2em>\[ \nabla^2 u=0 \]</div><div style=display:inline-block;margin-right:2em>\[ u=F(x)G(y) \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{F''(x)}{F(x)} = -\frac{G''(y)}{G(y)} = c \]</div>for rectangular regions<br><div style=display:inline-block;margin-right:2em>\[ F_?(x)=A =\sinh(\lambda x) + B \cosh(\lambda x) \]</div>use odd $$\lambda_n$$ if $$F$$ or $$G$$ equal a single $$\cos$$ term.<br><div style=display:inline-block;margin-right:2em>\[ G_?(y)=C =\sinh(\lambda y) + D \cosh(\lambda y) \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,?)=G(?) \]</div><div style=display:inline-block;margin-right:2em>\[ u(?,y)=F(y) \]</div><br><b>Part 1</b>: All $$u_?(x,?)=0$$<div style=display:inline-block;margin-left:2em>\[ \lambda_n=\frac{n\pi}{L y}\text{ or }\frac{(2n+1)\pi}{2L y} \]</div><div style=display:inline-block;margin-left:2em>\[ u_1(x,y)=\sum_{n=0}^{\infty}A_n F_1(x)G_1(y) \]</div><div style=display:inline-block;margin-right:2em>\[ A_n = \frac{2}{L_y F_1(?)}\int_0^{L_y} u(?,y)G_1(y)\,dy \]</div><div style=display:inline-block;margin-right:2em>\[ u(?,y)=h(y) \]</div><div style=display:inline-block;margin-right:2em>\[ ? = L_x\text{ or }0 \]</div><br><b>Part 2</b>: All $$u_?(?,y)=0$$<div style=display:inline-block;margin-left:2em>\[ \lambda_n=\frac{n\pi}{L x}\text{ or }\frac{(2n+1)\pi}{2L x} \]</div><div style=display:inline-block;margin-left:2em>\[ u_2(x,y)=\sum_{n=1}^{\infty}B_n F_2(x)G_2(y) \]</div><div style=display:inline-block;margin-right:2em>\[ B_n = \frac{2}{L_x G_2(?)}\int_0^{L_x} u(x,?)F_2(x)\,dx \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,?)=q(x) \]</div><div style=display:inline-block;margin-right:2em>\[ ? = L_y\text{ or }0 \]</div><br><div style=display:inline-block;margin-right:2em>\[ u(x,y)=u_1(x,y)+u_2(x,y) \]</div><br><b>Circular</b> $$\nabla^2 u$$:<div style=display:inline-block;margin-right:2em>\[ u_{rr} + \frac{1}{r} u_r + \frac{1}{r^2}u_{\theta \theta} = 0 \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{G''(\theta)}{G(\theta)}= - \frac{r^2 F''(r) + r F(r)}{F(r)} = c\]</div><br><div style=display:inline-block;margin-right:2em>\[ \left\langle f,g \right\rangle = \int_a^b f(x) g(x)\,dx \]</div><div style=display:inline-block;margin-right:2em>\[ \mathcal{L}_s=-p(x)\frac{d^2}{dx^2}-p'(x)\frac{d}{dx} + q(x) \]</div><div style=display:inline-block;margin-right:2em>\[ \mathcal{L}_s\phi(x)=\lambda r(x) \phi(x) \]</div><div style=display:inline-block;margin-right:2em>\[ \left\langle\mathcal{L}_s y_1,y_2 \right\rangle =\int_0^L \left(-[p y_1']'+q y_1\right)y_2\,dx \]</div><br>$$\mathcal{L}_s = f(x)$$, then $$\mathcal{L}_s^{\dagger} v(x) = 0$$, where if $$v=0$$, $$u(x)$$ is unique, otherwise $$v(x)$$ exists only when forcing is orthogonal to all trivial solutions.<br>if $$c=0$$,<div style=display:inline-block;margin-right:2em>\[ G(\theta)=B \]</div><div style=display:inline-block;margin-right:2em>\[ F(r)=C\ln r + D \]</div><br>if $$c=-\lambda^2 &lt;0$$,<div style=display:inline-block;margin-right:2em>\[ G(\theta)=A\sin(\lambda\theta) + B\cos(\lambda\theta) \]</div><div style=display:inline-block;margin-right:2em>\[ F(r) = C\left(\frac{r}{R} \right)^n + D\left( \frac{r}{R} \right)^{-n}\]</div><div style=display:inline-block;margin-right:2em>\[ u(r,\theta)=B_0 + \sum_{n=1}^{\infty} F(r) G(\theta) = B_0 + \sum_{n=1}^{\infty}F(r)[A_n\sin(\lambda\theta) + B_n\cos(\lambda\theta)] \]</div><div style=display:inline-block;margin-right:2em>\[ A_n = \frac{1}{\pi}\int_0^{2\pi} f(\theta)\sin(n\theta)\,d\theta \]</div><div style=display:inline-block;margin-right:2em>\[ B_n = \frac{1}{\pi}\int_0^{2\pi} f(\theta)\cos(n\theta)\,d\theta \]</div><div style=display:inline-block;margin-right:2em>\[ B_0 = \frac{1}{2\pi}\int_0^{2\pi} f(\theta)\,d\theta \]</div><br><b>Wave Equation:</b><div style=display:inline-block;margin-right:2em>\[ u_{tt}=c^2 u_{xx} \]</div><div style=display:inline-block;margin-right:2em>\[ u = F(x)G(t) \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{F''(x)}{F(x)}=\frac{G''(t)}{G(t)}=k \text{ where } u(t,0)=F(0)=u(t,L)=F(L)=0 \]</div><div style=display:inline-block;margin-right:2em>\[ F(x) = A\sin(\lambda x) + B\cos(\lambda x) \]</div><div style=display:inline-block;margin-right:2em>\[ u_t(x,0)=g(x)=\sum_{n=1}^{\infty} A_n\lambda_n c \sin(\lambda_n x) \]</div><div style=display:inline-block>\[ A_n = \frac{2}{\lambda_n c L}\int_0^L g(y)\sin(\lambda_n y)\,dy \]</div><br><div style=display:inline-block;margin-right:2em>\[ G(t) = C\sin(\lambda t) + D\cos(\lambda t) \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,0)=f(x)=\sum_{n=1}^{\infty}B_n\sin(\lambda_n x) \]</div><div style=display:inline-block;margin-right:2em>\[ B_n = \frac{2}{L}\int_0^L f(y)\sin(\lambda_n y)\,dy \]</div><br><div style=display:inline-block;margin-right:2em>\[ u(x,t)=\sum_{n=1}^{\infty}F(x)G(t)=\sum_{n=1}^{\infty}F(x)\left[C\sin(\lambda t) + D\cos(\lambda t)\right] \]</div><div style=display:inline-block;margin-right:2em>\[ \lambda_n=\frac{n\pi}{L}\text{ or }\frac{(2n+1)\pi}{2L} \]</div><br><b>Inhomogeneous:</b><br><div style=display:inline-block;margin-right:2em>\[ u(0,t)=F(0)=p(t) \]</div><div style=display:inline-block;margin-right:2em>\[ u(L,t)=F(L)=q(t) \]</div><div style=display:inline-block>\[ u(x,t)=v(x,t) + \phi(x)p(t) + \psi(x)q(t) \]</div><br><b>Transform to forced:</b><br><div style=display:inline-block;margin-right:2em>\[ v(x,y) = \begin{cases} v_{tt}=c^2 v_{xx} - \phi(x)p''(x) - \psi(x)q''(t) = c^2 v_{xx} + R(x,t) & \phi(x)=1 - \frac{x}{L}\;\;\;\psi(x)=\frac{x}{L}\\ v(0,t)=v(L,t)=0 & t&gt;0 \\ v(x,0)=f(x)-\phi(x)p(0) - \psi(x)q(0) & f(x)=u(x,0) \\ v_t(x,0)=g(x)-\phi(x)p'(0) - \psi(x)q'(0) & g(x)=u_t(x,0)  \end{cases} \]</div><br>Then solve as a forced equation.<br><b>Forced:</b><div style=display:inline-block;margin-right:2em>\[ u_{tt}=c^2 u_{xx} + R(x,t) \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,t)=u_1(x,t)+u_2(x,t) \]</div>$$u_1$$ found by $$R(x,t)=0$$ and solving.<div style=display:inline-block;margin-right:2em>\[ u_2(x,t)=\sum_{n=1}^{\infty} C_n(t)\sin\left(\frac{n\pi x}{L}\right) \]</div>where $$\sin\frac{n\pi x}{L}$$ are the eigenfunctions from solving $$R(x,t)=0$$<br><div style=display:inline-block;margin-right:2em>\[ R(x,t)=\sum_{n=1}^{\infty} R_n(t)\sin\left(\frac{n\pi x}{L}\right) \]</div><div style=display:inline-block;margin-right:2em>\[ R_n(t)=\frac{2}{L}\int_0^L R(y,t)\sin\left(\frac{n\pi x}{L}\right)\,dy \]</div><div style=display:inline-block>\[ C_n''(t) + k^2 C_n(t)=R_n(t) \]</div><div style=display:inline-block;margin-right:2em>\[ C_n(t)=\alpha\sin(k t) + \beta\cos(k t) + sin(k t)\int_0^t R_n(s)\cos(k s)\,ds - \cos(k t)\int_0^t R_n(s)\sin(k s)\,ds \]</div><br>where $$C_n(0)=0$$ and $$c_n'(0)=0$$<br><b>Fourier Transform</b>:<div style=display:inline-block;margin-right:2em>\[ \mathcal{F}(f)=\hat{f}(k)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} f(\xi) e^{-i k \xi}\,d\xi \]</div><div style=display:inline-block;margin-right:2em>\[ \mathcal{F}^{-1}(f)=f(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} \hat{f}(k) e^{ikx}\,dk \]</div><div style=display:inline-block;margin-right:2em>\[ \mathcal{F}(f+g)=\widehat{f+g}=\hat{f} + \hat{g} \]</div><div style=display:inline-block;margin-right:2em>\[ \widehat{fg}\neq\hat{f}\hat{g} \]</div><div style=display:inline-block;margin-right:2em>\[ \widehat{f'}=ik\hat{f}\text{ or }\widehat{f^{(n)}}=(ik)^n\hat{f} \]</div><br><div style=display:inline-block;margin-right:2em>\[ \widehat{u_t} = \frac{\partial \hat{u}}{\partial t} = \hat{u}_t \]</div><div style=display:inline-block;margin-right:2em>\[ \widehat{u_{tt}}=\frac{\partial^2\hat{u}}{\partial t^2}=\hat{u}_{tt} \]</div><div style=display:inline-block;margin-right:2em>\[ \widehat{u_{xx}}=(ik)^2\hat{u}=-k^2\hat{u} \]</div><br><div style=display:inline-block;margin-right:2em>\[ u(x,t)=\mathcal{F}^{-1}(\hat{u})=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \hat{f}(k) e^{\alpha^2 k^2 t} e^{ikx}\,dk \]</div><div style=display:inline-block;margin-right:2em>\[ u(x,t)=\mathcal{F}^{-1}\left(\hat{f}(k) e^{-\alpha^2 k^2 t}\right) \]</div><br>Semi-infinite:<div style=display:inline-block;margin-right:2em>\[ \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \hat{F}(k) e^{\alpha^2 k^2 t} e^{ikx}\,dk \]</div>where<div style=display:inline-block;margin-right:2em>\[ \hat{F}(k)=-i\sqrt{\frac{2}{\pi}}\int_0^{\infty} f(\xi)\sin(k\xi)\,d\xi \]</div><table cellspacing=0 cellpadding=0><tr><td style="text-align:center;border-right:1px solid #000;border-bottom:1px solid #000">$$f(x)$$</td><td style="text-align:center;border-bottom:1px solid #000">$$\hat{f}(k)$$</td></tr><tr><td style="border-right:1px solid #000">\[1\text{ if } -s &lt; x &lt; s, 0\text{ otherwise }\]</td><td>\[ \sqrt{\frac{2}{\pi}}\frac{\sin(ks)}{k} \]</td></tr><tr><td style="border-right:1px solid #000">\[ \frac{1}{x^2 + s^2} \]</td><td>\[ \frac{1}{s}\sqrt{\frac{\pi}{2}}e^{-s|k|} \]</td></tr><tr><td style="border-right:1px solid #000">\[ e^{-sx^2} \]</td><td>\[ \frac{1}{\sqrt{2s}}e^{\frac{k^2}{4s}} \]</td></tr><tr><td style="border-right:1px solid #000">\[ \frac{\sin(sx)}{x}\]</td><td>\[ \sqrt{\frac{\pi}{2}}\text{ if } |k|&lt;s, 0\text{ otherwise} \]</td></tr><tr><td style="border-right:1px solid #000">\[ e^{isx}\text{ if } a &lt; x &lt; b, 0\text{ otherwise} \]</td><td>\[ \frac{i}{\sqrt{2\pi}}\left(\frac{1}{s-k} \right)\left(e^{ea(s-k)} - e^{ib(s-k)} \right) \]</td></tr></table><br><br><span style=font-size:120%><b>AMath 402 - Dynamical Systems and Chaos</b></span><br>Dimensionless time<div style=display:inline-block>\[ \tau = \frac{t}{T} \]</div>where T gets picked so that<div style=display:inline-block>\[\frac{}{}\]</div>and<div style=display:inline-block>\[\frac{}{}\]</div>are of order 1.<br>Fixed points of $$x'=f(x)$$: set $$f(x)=0$$ and solve for roots<br>Bifurcation: Given $$x'=f(x,r)$$, set $$f(x,r)=0$$ and solve for $$r$$, then plot $$r$$ on the $$x-axis$$ and $$x$$ on the $$y-axis$$.<div style=display:inline-block;margin-right:1em;text-align:center><img src=/img/saddle_node.png alt="Saddle Node"><br>Saddle Node</div><div style=display:inline-block;margin-right:1em;text-align:center><img src=/img/transcritical.png alt="Transcritical Node"><br>Transcritical</div><div style=display:inline-block;margin-right:1em;text-align:center><img src=/img/subcritical_pitchfork.png alt="Subcritical Pitchfork"><br>Subcritical Pitchfork</div><div style=display:inline-block;text-align:center><img src=/img/supercritical_pitchfork.png alt="Supercritical Pitchfork"><br>Supercritical Pitchfork</div><br><div style=display:inline-block;margin-right:2em>\[ \begin{matrix} x'=ax+by \\ y'=cx+dy \end{matrix} \]</div><div style=display:inline-block;margin-right:2em>\[ A=\begin{bmatrix} a & b \\ c & d \end{bmatrix}\]</div><div style=display:inline-block;margin-right:2em>\[ \begin{vmatrix} a-\lambda & b \\ c & d-\lambda \end{vmatrix}=0 \]</div><div style=display:inline-block;margin-right:2em>\[ \begin{matrix} \lambda^2 - \tau\lambda + \delta=0 & \lambda = \frac{\tau \pm \sqrt{\tau^2 - 4\delta}}{2} \\ \tau = a+d & \delta=ad-bc \end{matrix} \]</div><div style=display:inline-block;margin-right:2em>\[ x(t)=c_1 e^{\lambda_1 t}v_1 + c_2 e^{\lambda_2 t}v_2 \]</div>$$v_1,v_2$$ are eigenvectors.<br>Given $$\tau = \lambda_1 + \lambda_2$$ and $$\delta=\lambda_1\lambda_2$$:<br>if $$\delta &lt; 0$$, the eigenvalues are real with opposite signs, so the fixed point is a saddle point.<br>otherwise, if $$\tau^2-4\delta > 0$$, its a node. This node is stable if $$\tau &lt; 0$$ and unstable if $$\tau > 0$$<br>if $$\tau^2-4\delta &lt; 0$$, its a spiral, which is stable if $$\tau &lt; 0$$, unstable if $$\tau > 0$$, or a center if $$\tau=0$$<br>if $$\tau^2-4\delta = 0$$, its degenerate.<br><div style=display:inline-block;margin-right:2em>\[ \begin{bmatrix} x'=f(x,y) \\ y'=g(x,y) \end{bmatrix} \]</div>Fixed points are found by solving for $$x'=0$$ and $$y'=0$$ at the same time.<br>nullclines are when either $$x'=0$$ <b>or</b> $$y'=0$$ and are drawn on the phase plane.<br><div style=float:left;display:inline-block;margin-right:2em>\[ \begin{bmatrix} \frac{\partial f}{\partial x} & \frac{\partial f}{\partial y} \\ \frac{\partial g}{\partial x} & \frac{\partial g}{\partial y} \end{bmatrix} \]</div><div style=width:400px;margin-right:2em><br><br>$$\leftarrow$$ For nonlinear equations, evaluate this matrix at each fixed point, then use the above linear classification scheme to classify the point.</div><br>A basin for a given fixed point is the area of all trajectories that eventually terminate at that fixed point.<br>Given $$x'=f(x)$$, $$E(x)$$ is a conserved quantity if $$\frac{dE}{dt}=0$$<br>A limit cycle is an isolated closed orbit in a nonlinear system. Limit cycles can only exist in nonlinear systems.<br>If a function can be written as $$\vec{x}'=-\nabla V$$, then its a gradient function and can't have closed orbits.<br>The Liapunov function $$V(x)$$ for a fixed point $$x^*$$ satisfies $$V(x) > 0 \forall x \neq x^*, V(x^*)=0, V' &lt; 0 \forall x \neq x^*$$<br>A Hopf bifurcation occurs as the fixed points eigenvalues (in terms of $$\mu$$) cross the imaginary axis.<br><br><span style=font-size:120%><b>Math 300 - Introduction to Mathematical Reasoning</b></span><br><table style=float:right;text-align:center;margin-left:1.5em cellspacing=0 cellpadding=4><tr><td>P</td><td>Q</td><td>$$P \Rightarrow Q$$</td><td>$$\neg P \vee Q$$</td></tr><tr><td>T</td><td>T</td><td>T</td><td>T</td></tr><tr><td>T</td><td>F</td><td>F</td><td>F</td></tr><tr><td>F</td><td>T</td><td>T</td><td>T</td></tr><tr><td>F</td><td>F</td><td>T</td><td>T</td></tr></table><br><span style=padding-left:5em>All valid values of $$x$$ constitute the Domain.</span><br>$$f(x)=y$$ The range in which $$y$$ must fall is the codomain<br><span style=padding-left:5em>The image is the set of $$y$$ values that are possible given all valid values of $$x$$</span><br>So, $$\frac{1}{x}$$ has a domain $$\mathbb{R} - \{0\}$$ and a codomain of $$\mathbb{R}$$. However, no value of $$x$$ can ever produce $$f(x)=0$$, so the Image is $$\mathbb{R}-\{0\}$$<br><br>Injective: No two values of $$x$$ yield the same result. $$f(x_1)\neq f(x_2)$$ if $$x_1 \neq x_2$$ for all $$x$$<br>Surjective: All values of $$y$$ in the codomain can be produced by $$f(x)$$. In other words, the codomain equals the image.<br>Bijective: A Bijective function is both Injective and Surjective - all values of $$y$$ are mapped to exactly one value of $$x$$. A simple way to prove this is to solve $$f(x)$$ for $$x$$. If this can be done without creating multiple solutions (a square root, for example, yields $$\pm$$, not a single answer), then it's a bijective function.<br>If $$f(x)$$ is a bijection, it is denumerable. Any set that is equivalent to the natural numbers is denumerable.<br>$$\forall x \in \mathbb{R}$$ means "for all $$x$$ in $$\mathbb{R}$$", where $$\mathbb{R}$$ can be replaced by any set.<br>$$\exists y \in \mathbb{R}$$ means "there exists a $$y$$ in $$\mathbb{R}$$", where $$\mathbb{R}$$ can be replaced by any set.<br><div style=display:inline-block;margin-right:2em>\[ A \vee B = A\text{ or }B \]</div><div style=display:inline-block;margin-right:2em>\[ A \wedge B = A\text{ and }B \]</div><div style=display:inline-block;margin-right:2em>\[ P \Leftrightarrow Q\text{ means }P \Rightarrow Q \wedge Q\Rightarrow P \text{ or } P\text{ iff }Q \text{ (if and only if)} \]</div><br>$$A \cup B$$ = Union - All elements that are in either A or B, or both.<div style=display:inline-block;margin-left:2em>\[ \{x|x \in A \text{ or } x \in B \} \]</div><br>$$A \cap B$$ = Intersection - All elements that are in both A and B<div style=display:inline-block;margin-left:2em>\[ \{x|x \in A \text{ and } x \in B \} \]</div><br>$$A\subseteq B$$ = Subset - Indicates A is a subset of B, meaning that all elements in A are also in B.<div style=display:inline-block;margin-left:2em>\[ \{x \in A \Rightarrow x \in B \} \]</div><br>$$A \subset B$$ = Strict Subset - Same as above, but does not allow A to be equal to B (which happens if A has all the elements of B, because then they are the exact same set).<br>$$A-B$$ = Difference - All elements in A that aren't also in B<div style=display:inline-block;margin-left:2em>\[ \{x|x \in A \text{ and } x \not\in B \} \]</div><br>$$A\times B$$ = Cartesian Product - All possible ordered pairs of the elements in both sets:<div style=display:inline-block;margin-left:2em>\[ \{(x,y)|x \in A\text{ and }y\in B\} \]</div><br><table><tr><td style=text-align:center>Proof</td><td>We use induction on $$n$$</td></tr><tr><td style=text-align:right>Base Case:</td><td>[Prove $$P(n_0)$$]</td></tr><tr><td style=text-align:right;width:7em>Inductive Step:</td><td>Suppose now as inductive hypothesis that [$$P(k)$$ is true] for some integer $$k$$ such that $$k \ge n_0$$. Then [deduce that $$P(k+1)$$ is true]. This proves the inductive step.</td></tr><tr><td style=text-align:right>Conclusion:</td><td>Hence, by induction, [$$P(n)$$ is true] for all integers $$n \ge n_0$$.</td></tr></table><table><tr><td style=text-align:center>Proof</td><td>We use strong induction on $$n$$</td></tr><tr><td style=text-align:right>Base Case:</td><td>[Prove $$P(n_0)$$, $$P(n_1)$$, ...]</td></tr><tr><td style=text-align:right;width:7em>Inductive Step:</td><td>Suppose now as inductive hypothesis that [$$P(n)$$ is true for all $$n \le k$$] for some positive integer $$k$$, then [deduce that $$P(k+1)$$ is true]. This proves the inductive step.</td></tr><tr><td style=text-align:right>Conclusion:</td><td>Hence, by induction [$$P(n)$$ is true] for all positive integers $$n$$.</td></tr></table><table><tr><td>Proof:</td><td>Suppose, for contradiction, that the statement $$P$$ is false. Then, [create a contradiction]. Hence our assumption that $$P$$ is false must be false. Thus, $$P$$ is true as required.</td></tr></table><br>The composite of $$f:X\rightarrow Y$$ and $$g:Y\rightarrow Z$$ is $$g\circ f: x\rightarrow Z$$ or just $$gf: X\rightarrow Z$$. $$g\circ f = g(f(x)) \forall x \in X$$<br><div style=display:inline-block;margin-right:2em>\[ a\equiv b \bmod m \Leftrightarrow b\equiv a \bmod m \]</div>If<div style=display:inline-block;margin-right:2em>\[ a \equiv b \bmod m\text{ and }b \equiv c \bmod m,\text{ then }a \]</div><br>Negation of $$P \Rightarrow Q$$ is $$P \wedge (\neg Q)$$ or $$P and (not Q)$$<br>If $$m$$ is divisible by $$a$$, then $$a b_1 \equiv a b_2 \bmod m \Leftrightarrow b_1 \equiv b_2 \bmod\left(\frac{m}{a} \right)$$<br>Fermat's little theorem: If $$p$$ is a prime, and $$a \in \mathbb{Z}^+$$ which is not a multiple of $$p$$, then $$a^{p-1}\equiv 1 \bmod p$$.<br>Corollary 1: If $$p$$ is prime, then $$\forall a \in \mathbb{Z}, a^p = a \bmod p$$<br>Corollary 2: If $$p$$ is prime, then $$(p-1)! \equiv -1 \bmod p$$<br><br>Division theorem: $$a,b \in \mathbb{Z}$$, $$b > 0$$, then $$a = bq+r$$ where $$q,r$$ are unique integers, and $$0 \le r &lt; b$$. Thus, for $$a=bq+r$$, $$\gcd(a,b)=\gcd(b,r)$$. Furthermore, $$b$$ divides $$a$$ if and only if $$r=0$$, and if $$b$$ divides $$a$$, $$\gcd(b,a)=b$$<br><br>Euclidean Algorithm: $$\gcd(136,96)$$<br>$$136=96\cdot 1 + 40$$<div style=display:inline-block;margin-left:2em>\[ 290x\equiv 5\bmod 357 \rightarrow 290x + 357y = 5 \]</div><br>$$96=40\cdot 2 + 16$$<div style=display:inline-block;margin-left:3em>\[ ax\equiv b \bmod c \rightarrow ax+cy=b \]</div><br>$$40=16\cdot 2 + 8$$<br>$$16=8\cdot 2 + 0 \leftarrow$$ stop here, since the remainder is now 0<br>$$\gcd(136,96)=8$$<br><br>Diophantine Equations (or linear combinations):<br>$$140m + 63n = 35$$ <span style=margin-left:2em>$$m,n \in \mathbb{Z}$$ exist for $$am+bn=c$$ iff $$\gcd(a,b)$$ divides $$c$$</span><br>$$140=140\cdot 1 + 63\cdot 0$$<br>$$63=140\cdot 0 + 63\cdot 1$$ <span style=margin-left:2em>dividing $$am+bn=c$$ by $$\gcd(a,b)$$ always yields coprime $$m$$ and $$n$$.</span><br>$$14=140\cdot 1 - 63\cdot 2$$<br>$$7=63\cdot 1 - 14\cdot 4 = 63 - (140\cdot 1 - 63\cdot 2)\cdot 4 = 63 - 140\cdot 4 + 63\cdot 8 = 63\cdot 9 - 140\cdot 4 $$<br>$$0=14 - 7\cdot 2 =140\cdot 1 - 63\cdot 2 - 2(63\cdot 9 - 140\cdot 4) = 140\cdot 9-63\cdot 20 $$<br>So $$m=9$$ and $$n=-20$$<br><br>Specific: $$7=63\cdot 9 - 140\cdot 4 \rightarrow 140\cdot(-4)+63\cdot 9=7\rightarrow 140\cdot(-20)+63\cdot45=35$$<br>So for $$140m+63n=35, m=-20, n=45$$<br>Homogeneous: $$140m+63n=0 \; \gcd(140,63)=7 \; 20m + 9n = 0 \rightarrow 29m=-9n$$<br>$$m=9q, n=-20q$$ for $$q\in \mathbb{Z}$$ or $$am+bn=0 \Leftrightarrow (m,n)=(bq,-aq)$$<br>General: To find <i>all</i> solutions to $$140m+63n=35$$, use the specific solution $$m=-20, n=45$$<br>$$140(m+20) + 63(n-45) = 0$$<br>$$(m+20,n-45)=(9q,-20q)$$ use the homogeneous result and set them equal.<br>$$(m,n)=(9q-20,-20q+45)$$ So $$m=9q-20$$ and $$n=-20q+45$$<br><div style=display:inline-block;margin-right:2em>\[ [a]_m = \{x\in \mathbb{Z}| x\equiv a \bmod m \} \Rightarrow \{ mq + a|q \in \mathbb{Z} \} \]</div><br>$$ax\equiv b \bmod m$$ has a unique solution $$\pmod{m}$$ if $$a$$ and $$m$$ are coprime. If $$\gcd(a,b)=1$$, then $$a$$ and $$b$$ are coprime. $$[a]_m$$ is invertible if $$\gcd(a,m)=0$$.<br><br><span style=font-size:120%><b>Math 327 - Introduction to Real Analysis I</b></span><br>Cauchy Sequence: For all $$\epsilon > 0$$ there exists an integer $$N$$ such that if $$n,m \ge N$$, then $$|a_n-a_m|&lt;\epsilon$$<br><br>Alternating Series: Suppose the terms of the series $$\sum u_n$$ are alternately positive and negative, that $$|u_{n+1}| \le |u_n|$$ for all $$n$$, and that $$u_n \to 0$$ as $$n\to\infty$$. Then the series $$\sum u_n$$ is convergent.<br><br>Bolzano-Weierstrass Theorem: If $$S$$ is a bounded, infinite set, then there is at least one point of accumulation of $$S$$.<br><br>$$\sum u_n$$ is absolutely convergent if $$\sum|u_n|$$ is convergent<br>If $$\sum u_n$$ is absolutely convergent, then $$\sum u_n = \sum a_n - \sum b_n$$ where both $$\sum a_n$$ and $$\sum b_n$$ are convergent. If $$\sum u_n$$ is conditionally convergent, both $$\sum a_n$$ and $$\sum b_n$$ are divergent.<br><br><div style=display:inline-block;margin-right:2em>\[ \sum u_n = U = u_0 + u_1 + u_2 + ... \]</div><div style=display:inline-block;margin-right:2em>\[ w_0 = u_0 v_0 \]</div><div style=display:inline-block;margin-right:2em>\[ w_1 = u_0 v_1 + u_1 v_0 \]</div><br><div style=display:inline-block;margin-right:2em>\[ \sum v_n = V = v_0 + v_1 + v_2 + ... \]</div><div style=display:inline-block;margin-right:2em>\[ w_n = u_0 v_n + u_1 v_{n-1} + ... + u_n v_0 \]</div><br><div style=display:inline-block;margin-right:2em>\[ \sum w_n = UV = w_0 + w_1 + w_2 + ... \]</div>Provided<div style=display:inline-block>\[ \sum u_n, \sum v_n \]</div>are absolutely convergent.<br><br>$$\sum a_n x^n$$ is either absolutely convergent for all $$x$$, divergent for all $$x\neq 0$$, or absolutely convergent if $$-R &lt; x &lt; R$$ (it may or may not converge for $$x=\pm R$$ ).<br>$$-R &lt; x &lt; R$$ is the interval of convergence. $$R$$ is the radius of convergence.<div style=display:inline-block>\[ R=\lim_{n \to \infty}\left|\frac{a_n}{a_{n+1}}\right| \]</div>if the limit exists or is $$+\infty$$<br>Let the functions of the sequence $$f_n(x)$$ be defined on the interval $$[a,b]$$. If for each $$\epsilon > 0$$ there is an integer $$N$$ independent of x such that $$|f_n(x) - f_m(x)| &lt; \epsilon$$ where $$N \le m,n$$.<br><br>You can't converge uniformly on any interval containing a discontinuity.<br><div style=display:inline-block;margin-right:2em>\[ |a|+|b|\le|a+b| \]</div><div style=display:inline-block;margin-right:2em>\[ \lim_{n \to \infty}(1+\frac{k}{n})^n = e^k \]</div><div style=display:inline-block;margin-right:2em>\[ lim_{n \to \infty} n^{\frac{1}{n}}=1 \]</div><div style=display:inline-block>\[ \sum_{k=0}^{\infty}=\frac{a}{1-r} \]</div>if $$|r| &lt; 1$$<br><div style=display:inline-block>\[\sum a_n x^n\]</div>has a radius of convergence<div style=display:inline-block>\[R = \lim_{n \to \infty}\left|\frac{a_n}{a_{n+1}} \right|\]</div>if it exists or is $$+\infty$$. The interval of convergence is $$-R &lt; x &lt; R$$<br>\[ \sum^{\infty}\frac{x^2}{(1+x^2)^n} = x^2\sum\left(\frac{1}{1+x^2}\right)^n = x^2\left(\frac{1}{1-\frac{1}{1+x^2}}\right) = x^2\left( \frac{1+x^2}{1+x^2-1}\right) = x^2\left( \frac{1+x^2}{x^2} \right) = 1+x^2 \]<br><br><span style=font-size:120%><b>Math 427 - Complex Analysis</b></span><br><table cellspacing=0 cellpadding=4><tr><td></td><td>0</td><td>$$\frac{\pi}{6}$$</td><td>$$\frac{\pi}{4}$$</td><td>$$\frac{\pi}{3}$$</td><td>$$\frac{\pi}{2}$$</td></tr><tr><td>$$\sin$$</td><td>0</td><td>$$\frac{1}{2}$$</td><td>$$\frac{\sqrt{2}}{2}$$</td><td>$$\frac{\sqrt{3}}{2}$$</td><td>1</td></tr><tr><td>$$\cos$$</td><td>1</td><td>$$\frac{\sqrt{3}}{2}$$</td><td>$$\frac{\sqrt{2}}{2}$$</td><td>$$\frac{1}{2}$$</td><td>0</td></tr><tr><td>$$\tan$$</td><td>0</td><td>$$\frac{1}{\sqrt{3}}$$</td><td>1</td><td>$$\sqrt{3}$$</td><td>$$\varnothing$$</td></tr></table><div style=display:inline-block;margin-right:2em>\[ \frac{z_1}{z_2} = \frac{x_1 x_2 + y_1 y_2}{x_2^2 + y_2^2} + i\frac{y_1 x_1 - x_1 y_2}{x_2^2 + y_2^2} \]</div><div style=display:inline-block;margin-right:2em>\[ |z| = \sqrt{x^2 + y^2} \]</div><div style=display:inline-block;margin-right:2em>\[ \bar{z} = x-i y \]</div><div style=display:inline-block;margin-right:2em>\[ |\bar{z}|=|z| \]</div><br><div style=display:inline-block;margin-right:2em>\[ \text{Re}\,z=\frac{z+\bar{z}}{2} \]</div><div style=display:inline-block;margin-right:2em>\[ \text{Im}\,z=\frac{z-\bar{z}}{2i} \]</div><div style=display:inline-block;margin-right:2em>\[ \text{Arg}\,z=\theta\text{ for } -\pi &lt; \theta &lt; \pi \]</div><div style=display:inline-block;margin-right:2em>\[ z=re^{i\theta}=r(\cos(\theta) + i\sin(\theta)) \]</div><br><div style=display:inline-block;margin-right:2em>\[ \lim_{z \to z_0} f(z) = w_0 \iff \lim_{x,y \to x_0,y_0} u(x,y) = u_0 \text{ and }\lim_{x,y \to x_0,y_0} v(x,y)=v_0 \text{ where } w_0=u_0+iv_0 \text{ and } z_0=x_0+iy+0 \]</div><br><div style=display:inline-block;margin-right:2em>\[ \lim_{z \to z_0} f(z) = \infty \iff \lim_{z \to z_0} \frac{1}{f(z)}=0 \]</div><br><div style=display:inline-block;margin-right:2em>\[ \lim_{z \to \infty} f(z) = w_0 \iff \lim_{z \to 0} f\left(\frac{1}{z}\right)=w_0\]</div><div style=display:inline-block;margin-right:2em>\[ \text{Re}\,z\le |\text{Re}\,z|\le |z| \]</div><div style=display:inline-block;margin-right:2em>\[ \text{Im}\,z\le |\text{Im}\,z| \le z \]</div><br><div style=display:inline-block;margin-right:2em>\[ \lim_{z \to \infty} f(z) = \infty \iff \lim_{z\to 0}\frac{1}{f\left(\frac{1}{z}\right)}=0\]</div><div style=display:inline-block;margin-right:2em>\[ \left| |z_1| - |z_2| \right| \le |z_1 + z_2| \le |z_1| + |z_2| \]</div><br>Roots:<div style=display:inline-block;margin-right:2em>\[ z = \sqrt[n]{r_0} \exp\left[i\left(\frac{\theta_0}{n} + \frac{2k\pi}{n} \right)\right] \]</div>Harmonic:<div style=display:inline-block;margin-right:2em>\[ f_{xx} + f_{yy} = 0 \text{ or } f_{xx}=-f_{yy} \]</div><br>Cauchy-Riemann:<br><div style=display:inline-block;margin-right:2em>\[ f(z) = u(x,y)+iv(x,y) \]</div><div style=display:inline-block;margin-right:2em>\[ \sinh(z)=\frac{e^z-e^{-z}}{2}=-i\sin(iz) \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{d}{dz}\sinh(z)=\cosh(z) \]</div><br><div style=display:inline-block;margin-right:2em>\[ u_x = v_y \]</div><div style=display:inline-block;margin-right:2em>\[ u_y = -v_x \]</div><div style=display:inline-block;margin-right:2em>\[ \cosh(z) = \frac{e^z + e^{-z}}{2} = cos(iz) \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{d}{dz}\cosh(z)=\sinh(z) \]</div><br><div style=display:inline-block;margin-right:2em>\[ f(z) = u(r,\theta)+iv(r,\theta) \]</div><div style=display:inline-block;margin-right:2em>\[ \sin(z)=\frac{e^{iz}-e{-iz}}{2i}=i-\sinh(iz)=\sin(x)\cosh(y) + i\cos(x)\sinh(y) \]</div><br><div style=display:inline-block;margin-right:2em>\[ r u_r = v_{\theta} \]</div><div style=display:inline-block;margin-right:2em>\[ u_{\theta}=-r v_r \]</div><div style=display:inline-block;margin-right:2em>\[ \cos(z)=\frac{e^{iz}+e^{-iz}}{2}=\cosh(iz)=\cos(x)\cosh(y) - i \sin(x)\sinh(y) \]</div><br><div style=display:inline-block;margin-right:2em>\[ \sin(x)^2 + \sinh(y)^2 = 0 \]</div><div style=display:inline-block;margin-right:2em>\[ |\sin(z)|^2 = \sin(x)^2 + \sinh(y)^2 \]</div><div style=display:inline-block;margin-right:2em>\[ |\cos(z)|^2 = \cos(x)^2 + \sinh(y)^2 \]</div><br>$$e^z$$ and Log:<div style=display:inline-block;margin-right:2em>\[ e^z=e^{x+iy}=e^x e^{iy}=e^x(\cos(y) + i\sin(y)) \]</div><div style=display:inline-block;margin-right:2em>\[ e^x e^{iy} = \sqrt{2} e^{i\frac{\pi}{4}} \]</div><div style=display:inline-block;margin-right:2em>\[ e^x = \sqrt{2} \]</div><div style=display:inline-block;margin-right:2em>\[ y = \frac{\pi}{4} + 2k\pi \]</div><div style=display:inline-block;margin-right:2em>\[ x = \ln\sqrt{2} \]</div>So,<div style=display:inline-block;margin-right:2em>\[ z = \ln\sqrt{2} + i\pi\left(\frac{1}{4} + 2k\right) \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{d}{dz} log(f(z)) = \frac{f'(z)}{f(z)} \]</div><br>Cauchy-Gourgat: If $$f(z)$$ is analytic at all points interior to and on a simple closed contour $$C$$, then<div style=display:inline-block;margin-right:2em>\[ \int_C f(z) \,dz=0 \]</div><br><div style=display:inline-block;margin-right:2em>\[ \int_C f(z) \,dz = \int_a^b f[z(t)] z'(t) \,dt \]</div><div style=display:inline-block;margin-right:2em>\[ a \le t \le b \]</div>$$z(t)$$ is a parameterization. Use $$e^{i\theta}$$ for circle!<br>Maximum Modulus Principle: If $$f(z)$$ is analytic and not constant in $$D$$, then $$|f(z)|$$ has no maximum value in $$D$$. That is, no point $$z_0$$ is in $$D$$ such that $$|f(z)| \le |f(z_0)|$$ for all $$z$$ in $$D$$.<br>Corollary: If $$f(z)$$ is continuous on a closed bounded region $$R$$ and analytic and not constant through the interior of $$R$$, then the maximum value of $$|f(z)|$$ in $$R$$ always exists on the boundary, never the interior.<br>Taylor's Theorem: Suppose $$f(z)$$ is analytic throughout a disk $$|z-z_0|&lt;R$$. Then $$f(z)$$ has a power series of the form<div style=display:inline-block;margin-right:2em>\[ f(z) = \sum_{n=0}^{\infty}\frac{f^{(n)}(z_0)}{n!}(z-z_0)^n \]</div>where $$|z-z_0| &lt; R_0$$ and $$n=0,1,2,...$$<br>Alternatively,<div style=display:inline-block;margin-right:2em>\[ \sum_{n=0}^{\infty} a_n(z-z_0)^n \text{ where } a_n=\frac{f^{(n)}(z_0)}{n!} \]</div><br>For $$|z|&lt;\infty$$:<div style=display:inline-block;margin-right:2em>\[ e^z = \sum_{n=0}^{\infty}\frac{z^n}{n!} \]</div><div style=display:inline-block;margin-right:2em>\[ \sin(z) = \sum_{n=0}^{\infty}(-1)^n\frac{z^{2n+1}}{(2n+1)!} \]</div><div style=display:inline-block;margin-right:2em>\[ \cos(z) = \sum_{n=0}^{\infty}(-1)^n\frac{z^{2n}}{(2n)!} \]</div><br>Note that<div style=display:inline-block>\[ \frac{1}{1 - \frac{1}{z}}=\sum_{n=0}^{\infty}\left(\frac{1}{z}\right)^n \text{ for } \left|\frac{1}{z}\right| &lt; 1\]</div>, which is really $$1 &lt; |z|$$ or $$|z| > 1$$<br>For $$|z| &lt; 1$$:<div style=display:inline-block;margin-right:2em>\[ \frac{1}{1-z} = \sum_{n=0}^{\infty} z^n \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{1}{1+z} = \sum_{n=0}^{\infty} (-1)^n z^n \]</div>For $$|z-1|&lt;1$$:<div style=display:inline-block;margin-right:2em>\[ \frac{1}{z} = \sum_{n=0}^{\infty} (-1)^n (z-1)^n \]</div><br>Analytic: $$f(z)$$ is analytic at point $$z_0$$ if it has a derivative at each point in some neighborhood of $$z_0$$.<br>$$f(z)=u(x,y)+iv(x,y)$$ is analytic if and only if $$v$$ is a harmonic conjugate of $$u$$.<br>If $$u(x,y)$$ and $$v(x,y)$$ are harmonic functions such that $$u_{xx}=-u{yy}$$ and $$v_{xx}=-v_{yy}$$, and they satisfy the Cauchy-Reimann conditions, then $$v$$ is a harmonic conjugate of $$u$$.<br><br>Differentiable: $$f(z)=u(x,y)+iv(x,y)$$ is differentiable at a point $$z_0$$ if $$f(z)$$ is defined within some neighborhood of $$z_0=x_0+iy_0$$, $$u_x$$ $$u_y$$ $$v_x$$ and $$v_y$$ exist everywhere in the neighborhood, those first-order partial derivatives exist at $$(x_0,y_0)$$ and satisfy the Reimann-Cauchy conditions at $$(x_0,y_0)$$.<br>Cauchy Integral Formula:<div style=display:inline-block;margin-right:2em>\[ \int_C \frac{f(z)}{z-z_0}\,dz = 2\pi i f(z_0) \]</div>where $$z_0$$ is any point interior to $$C$$ and $$f(z)$$ is analytic everywhere inside and on $$C$$ taken in the positive sense. Note that $$f(z)$$ here refers to the actual function $$f(z)$$, <i>not</i> $$\frac{f(z)}{z-z_0}$$<br>Generalized Cauchy Integral Formula:<div style=display:inline-block;margin-right:2em>\[ \int_C \frac{f(z)}{(z-z_0)^{n+1}}\,dz = \frac{2\pi i}{n!} f^{(n)}(z_0) \]</div>(same conditions as above)<br>Remember, discontinuities that are roots can be calculated and factored:<div style=display:inline-block;margin-right:2em>\[ \frac{1}{z^2-w_0}=\frac{1}{(z-z_1)(z-z_2)} \]</div><br>Residue at infinity:<div style=display:inline-block;margin-right:2em>\[ \int_C f(z)\,dz = 2\pi i \,\mathop{Res}\limits_{z=0}\left[\frac{1}{z^2} f\left(\frac{1}{z}\right) \right] \]</div><br>A residue at $$z_0$$ is the $$n=-1$$ term of the Laurent series centered at $$z_0$$.<br>A point $$z_0$$ is an isolated singular point if $$f(z_0)$$ fails to be analytic, but is analytic at some point in every neighborhood of $$z_0$$, and there's a deleted neighborhood $$0 &lt; |z-z_0| &lt; \epsilon$$ of $$z_0$$ throughout which $$f$$ is analytic.<br><div style=display:inline-block;margin-right:2em>\[ \mathop{Res}\limits_{z=z_0}\,f(z)=\frac{p(z_0)}{q'(z_0)}\text{ where }f(z) = \frac{p(z)}{q(z)}\text{ and } p(z_0)\neq 0 \]</div><br><div style=display:inline-block;margin-right:2em>\[ \int_C f(z)\,dz = 2\pi i \sum_{k=1}^n\,\mathop{Res}\limits_{z=z_k}\,f(z) \]</div>where $$z_0$$, $$z_1$$, $$z_2$$,... $$z_k$$ are all the singular points of $$f(z)$$ (which includes poles).<br><br>If a series has a finite number of $$(z-z_0)^{-n}$$ terms, its a pole ($$\frac{1}{z^4}$$ is a pole of order 3). If a function, when put into a series, has <i>no</i> $$z^{-n}$$ terms, it's a removable singularity. If it has an infinite number of $$z^{-n}$$ terms, its an essential singularity (meaning, you can't get rid of it).<br><br><span style=font-size:120%><b>AMath 401 - Vector Calculus and Complex Variables</b></span><br>$$||\vec{x}||=\sqrt{x_1^2+x_2^2+ ...}$$<br>$$\vec{u}\cdot\vec{v} = ||\vec{u}||\cdot||\vec{v}|| \cos(\theta)=u_1v_1+u_2v_2+ ...$$<br>$$||\vec{u}\times\vec{v}|| = ||\vec{u}||\cdot||\vec{v}|| \sin(\theta) = \text{ area of a parallelogram}$$<br><div style=display:inline-block;margin-right:2em>\[ \vec{u}\times\vec{v} = \begin{vmatrix} \vec{\imath} & \vec{\jmath} & \vec{k} \\ u_1 & u_2 & u_3 \\ v_1 & v_2 & v_3 \end{vmatrix} \]</div><div style=display:inline-block;margin-right:2em>\[ \begin{vmatrix} a & b \\ c & d \end{vmatrix} = ad-bc \]</div><div style=display:inline-block;margin-right:2em>\[ \vec{u}\cdot\vec{v}\times\vec{w} = \vec{u}\cdot(\vec{v}\times\vec{w})=\text{ volume of a parallelepiped} \]</div></br><div style=display:inline-block;margin-right:2em>\[ h_?\equiv\left|\left|\frac{\partial\vec{r}}{\partial ?}\right|\right| \]</div><div style=display:inline-block;margin-right:2em>\[ \hat{e}_r(\theta)=\frac{1}{h_r}\frac{\partial\vec{r}}{\partial r} \]</div><div style=display:inline-block;margin-right:2em>\[ \hat{e}_r'(\theta)=\frac{d\hat{e}_r}{d\theta}=\hat{e}_{\theta}(\theta) \]</div><br><div style=display:inline-block;margin-right:2em>\[ \vec{r}=r(t)\hat{e}_r(\theta(t)) \]</div><div style=display:inline-block;margin-right:2em>\[ \vec{r}'(t)=r'(t)\hat{e}_r + r(t)\frac{d\hat{e}_r}{dt}\]</div><div style=display:inline-block;margin-right:2em>\[ \vec{r}'(t)=r'(t)\hat{e}_r + r(t)\frac{d\hat{e}_r}{d\theta}\frac{d\theta}{dt}=r'(t)\hat{e}_r + r(t)\hat{e}_{\theta} \theta'(t) \]</div><br>Projection of $$\vec{u}$$ onto $$\vec{v}$$:<div style=display:inline-block;margin-right:2em>\[ ||\vec{u}|| \cos(\theta) = \frac{\vec{u}\cdot\vec{v}}{||\vec{v}||} \]</div>Arc Length:<div style=display:inline-block;margin-right:2em>\[ \int_a^b \left|\left| \frac{d\vec{r}}{dt} \right|\right| \,dt = \int_a^b ds \]</div><br>Parametrization:<div style=display:inline-block;margin-right:2em>\[ s(t) = \int_a^t \left|\left|\frac{d\vec{r}}{dt}\right|\right| \,dt \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{dx}{dt}=x'(t) \]</div><div style=display:inline-block;margin-right:2em>\[ dx = x'(t)\,dt \]</div><div style=display:inline-block;margin-right:2em>\[ \oint x \,dy - y \,dx = \oint\left(x \frac{dy}{dt} - y \frac{dx}{dt}\right)\,dt = \oint x y'(t) - y x'(t) \,dt \]</div><div style=display:inline-block;margin-right:2em>\[ \nabla = \frac{\partial}{\partial x}\vec{\imath} + \frac{\partial}{\partial y}\vec{\jmath} + \frac{\partial}{\partial z}\vec{k} \]</div><div style=display:inline-block;margin-right:2em>\[ \nabla f = \left\langle \frac{\partial f}{\partial x},\frac{\partial f}{\partial y},\frac{\partial f}{\partial z} \right\rangle \]</div><div style=display:inline-block;margin-right:2em>\[ D_{\vec{u}} f(\vec{P})=f'_{\vec{u}}=\nabla f \cdot \vec{u} \]</div><div style=display:inline-block;margin-right:2em>\[ \nabla(fg)=f\nabla g + g \nabla f \]</div><br><div style=display:inline-block;margin-right:2em>\[ \vec{F}(x,y,z)=F_1(x,y,z)\vec{\imath} + F_2(x,y,z)\vec{\jmath} + F_3(x,y,z)\vec{k} \]</div><br><div style=display:inline-block;margin-right:2em>\[ \text{div}\, \vec{F} = \nabla \cdot \vec{F} = \frac{\partial F_1}{\partial x} + \frac{\partial F_2}{\partial y} + \frac{\partial F_3}{\partial z} \]</div><br><div style=display:inline-block;margin-right:2em>\[ \text{curl}\, \vec{F} = \nabla \times \vec{F} = \left\langle \frac{\partial F_3}{\partial y} - \frac{\partial F_2}{\partial z}, \frac{\partial F_1}{\partial z} - \frac{\partial F_3}{\partial x}, \frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y} \right\rangle \]</div><br>Line integral:<div style=display:inline-block;margin-right:2em>\[ \int_{\vec{r}(a)}^{\vec{r}(b)}\vec{F}\cdot \vec{r}'(t)\,dt = \int_{\vec{r}(a)}^{\vec{r}(b)}\vec{F}\cdot d\vec{r} \]</div>Closed path:<div style=display:inline-block;margin-right:2em>\[ \oint \vec{F}\cdot d\vec{r} \]</div><div style=display:inline-block;margin-right:2em>\[ d\vec{r} = \nabla\vec{r}\cdot \langle du_1,du_2,du_3 \rangle = h_1 du_1 \hat{e}_1 + h_2 du_2 \hat{e}_2 + h_3 du_3 \hat{e}_3 \]</div><div style=display:inline-block;margin-right:2em>\[ h_?= \left|\left| \frac{\partial\vec{r}}{\partial ?} \right|\right| = \frac{1}{||\partial ?||} \]</div><div style=display:inline-block;margin-right:2em>\[ \hat{e}_?=\frac{1}{h_?}\frac{\partial\vec{r}}{\partial?}=h_?\nabla?(x,y,z)=\frac{\nabla?}{||\nabla?||} \]</div>If $$\vec{r}=\vec{r}(s)$$ then<div style=display:inline-block;margin-right:2em>\[ \int_a^b\vec{F}\cdot \frac{d\vec{r}}{ds} \,ds = \int_a^b\vec{F}\cdot\vec{T} \,ds\]</div><br><div style=display:inline-block;margin-right:2em>\[ \nabla f \cdot \hat{e}_?=\frac{1}{h_?}\frac{\partial f}{\partial ?} \]</div><div style=display:inline-block;margin-right:2em>\[ \nabla f = (\nabla f \cdot \hat{e}_r)\hat{e}_r + (\nabla f \cdot \hat{e}_{\theta})\hat{e}_{\theta} = \frac{\partial f}{\partial r}\hat{e}_r + \frac{1}{r}\frac{\partial f}{\partial \theta}\hat{e}_{\theta} \]</div><div style=display:inline-block;margin-right:2em>\[ ds = \sqrt{d\vec{r}\cdot d\vec{r}} = \left|\left| \frac{d\vec{r}}{dt}dt \right|\right| = \sqrt{h_1^2 du_1^2 + h_2^2 du_2^2+h_3^2 du_3^2} \]</div><br><div style=display:inline-block;margin-right:2em>\[ Spherical: h_r=1, h_{\theta}=r, h_{\phi}=r\sin(\theta) \]</div><br><div style=display:inline-block;margin-right:2em>\[ \vec{r}=\frac{\nabla f}{||\nabla f||} \]</div><div style=display:inline-block;margin-right:2em>\[ \vec{r}=\vec{r}(u,v) \]</div><div style=display:inline-block;margin-right:2em>\[ \vec{n}= \frac{\frac{\partial\vec{r}}{\partial u}\times\frac{\partial\vec{r}}{\partial v}}{\left|\left| \frac{\partial\vec{r}}{\partial u}\times\frac{\partial\vec{r}}{\partial v} \right|\right|} \]</div><br>$$\vec{F}$$ is conservative if:<div style=display:inline-block;margin-right:2em>\[ \vec{F}=\nabla\phi \]</div><div style=display:inline-block;margin-right:2em>\[ \nabla\times\vec{F}=0 \]</div><div style=display:inline-block;margin-right:2em>\[ \vec{F}=\nabla\phi \iff \nabla\times\vec{F}=0 \]</div><br><div style=display:inline-block;margin-right:2em>\[ \iint\vec{F}\cdot d\vec{S} \]</div><div style=display:inline-block;margin-right:2em>\[ d\vec{S}=\left(\frac{\partial\vec{r}}{\partial u} \times \frac{\partial\vec{r}}{\partial v} \right)\,du\,dv \]</div><div style=display:inline-block;margin-right:2em>\[ d\vec{S}=\vec{n}\,dS \]</div>Surface Area:<div style=display:inline-block;margin-right:2em>\[ \iint \,dS \]</div><br>Flux:<div style=display:inline-block;margin-right:2em>\[ \iint\vec{F}\cdot d\vec{S} \]</div><br>Shell mass:<div style=display:inline-block;margin-right:2em>\[ \iint p\cdot dS \]</div><br>Stokes:<div style=display:inline-block;margin-right:2em>\[ \iint(\nabla\times\vec{F})\cdot d\vec{S} = \oint\vec{F}\cdot d\vec{r} \]</div><br>Divergence:<div style=display:inline-block;margin-right:2em>\[ \iiint\limits_V\nabla\cdot\vec{F}\,dV=\iint\limits_{\partial v} \vec{F}\cdot d\vec{S} \]</div><br>If $$z=f(x,y)$$, then<div style=display:inline-block;margin-right:2em>\[ \vec{r}(x,y) = \langle x,y,f(x,y) \rangle \]</div><br><div style=display:inline-block;margin-right:2em>\[ \nabla\cdot\vec{F}=\frac{1}{h_1 h_2 h_3}\left[\frac{\partial}{\partial u_1}(F_1 h_2 h_3) + \frac{\partial}{\partial u_2}(F_2 h_1 h_3) + \frac{\partial}{\partial u_3}(F_3 h_1 h_2) \right] \]</div><div style=display:inline-block;margin-right:2em>\[ dV = \left| \frac{\partial\vec{r}}{\partial u_1}\cdot\left(\frac{\partial\vec{r}}{\partial u_2}\times\frac{\partial\vec{r}}{\partial u_3} \right) \right| du_1 du_2 du_3 \]</div>If orthogonal,<div style=display:inline-block;margin-right:2em>\[ I = \iiint\limits_V f(u_1,u_2,u_3)h_1 h_2 h_3\,du_1\,du_2\,du_3 \]</div><div style=display:inline-block;margin-right:2em>\[ (x-c_x)^2 + (y-c_y)^2 = r^2 \Rightarrow \vec{r}(t)=\langle c_x + r\cos(t),c_y+r\sin(t) \rangle \]</div><br>Ellipse:<div style=display:inline-block;margin-right:2em>\[ \Rightarrow \vec{r}(t)=\langle c_x+a\cos(t), c_y+b\sin(t) \rangle \]</div><br>For spherical, if $$\theta$$ is innermost, its max value is $$\pi$$, or if its on the z value.<br><div style=display:inline-block;margin-right:2em>\[ \vec{r}(\theta,\phi)=\langle \sin(\theta)\cos(\phi), \sin(\theta)\sin(\phi),\cos(\theta) \rangle \]</div><br><br>Laplace Transform:<div style=display:inline-block;margin-right:2em>\[ \mathcal{L}\left[f(t)\right]=F(s)=\int_0^{\infty}e^{-st}f(t)\,dt \]</div><div style=display:inline-block;margin-right:2em>\[ \mathcal{L}[f'(t)] = s\mathcal{L}[f(t)] - f(0) = s\cdot F(s)-f(0)\]</div><div style=display:inline-block>\[ \mathcal{L}[f''(t)] = s^2\mathcal{L}[f(t)] - s\cdot f(0) - f'(0) = s^2\cdot F(s) - s\cdot f(0) - f'(0)\]</div><br><div style=display:inline-block;margin-right:2em>\[ \mathcal{L}[0] = 0 \]</div><div style=display:inline-block;margin-right:2em>\[ \mathcal{L}[1] = \frac{1}{s} \]</div><div style=display:inline-block;margin-right:2em>\[ \mathcal{L}[k] = \frac{k}{s} \]</div><div style=display:inline-block;margin-right:2em>\[ \mathcal{L}[e^{at}] = \frac{1}{s-a} \]</div><div style=display:inline-block;margin-right:2em>\[ \mathcal{L}[\cos(\omega t)] = \frac{s}{s^2 + \omega^2} \]</div><div style=display:inline-block>\[ \mathcal{L}[\sin(\omega t)] = \frac{\omega}{s^2 + \omega^2} \]</div><br><br><span style=font-size:120%><b>Math 461 - Combinatorial Theory I</b></span><br>General Pigeon: Let $$n$$,$$m$$, and $$r$$ be positive integers so that $$n>rm$$. Let us distribute $$n$$ balls into $$m$$ boxes. Then there will be at least 1 box in which we place at least $$r+1$$ balls.<br><table><tr><td style=text-align:right>Base Case:</td><td>Prove $$P(0)$$ or $$P(1)$$</td></tr><tr><td style=text-align:right>Inductive Step:</td><td>Show that by assuming the inductive hypothesis $$P(k)$$, this implies that $$P(k+1)$$ must be true.</td></tr><tr><td style=text-align:right>Strong Induction:</td><td>Show that $$P(k+1)$$ is true if $$P(n)$$ for all $$n < k+1$$ is true.</td></tr><br><br><br></table><br>There are $$n!$$ permutations of an $$n$$-element set (or $$n!$$ linear orderings of $$n$$ objects)<br>$$n$$ objects sorted into $$a,b,c,...$$ groups have $$\frac{n!}{a!b!c!...}$$ permutations. This is also known as a $$k$$-element multiset of $$n$$.<br>Number of $$k$$-digit strings in an $$n$$-element alphabet: $$n^k$$. All subsets of an $$n$$-element set: $$2^n$$<br>Let $$n$$ and $$k$$ be positive integers such that $$n \ge k$$, then the number of $$k$$-digit strings over an $$n$$-element alphabet where no letter is used more than once is $$\frac{n!}{(n-k)!}=(n)_k$$<br><div style=display:inline-block>\[ \binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{(n)_k}{k!} \rightarrow \]</div>This is the number of $$k$$-element subsets in $$[n]$$, where $$[n]$$ is an $$n$$-element set.<br><div style=display:inline-block>\[ \binom{n}{k} = \binom{n}{n-k} \]</div><div style=display:inline-block;margin-right:2em>\[ \binom{n}{0} = \binom{n}{n} = \binom{0}{0} = 1 \]</div><br>Number of $$k$$-element multisets in $$[n]$$:<div style=display:inline-block>\[ \binom{n+k-1}{k} \]</div><br>Binomial Theorem:<div style=display:inline-block;margin-right:2em>\[ (x+y)^n = \sum_{k=0}^n\binom{n}{k}x^k y^{n-k} \text{ for } n \ge 0 \]</div><br>Multinomial Theorem:<div style=display:inline-block;margin-right:2em>\[ (x_1 + x_2 + ... + x_k)^n = \sum_{a_1,a_2,...,a_k} \binom{n}{a_1,a_2,...,a_k} x_1^{a_1} x_2^{a_2} ... x_k^{a_k} \]</div><div style=display:inline-block;margin-right:2em>\[ \binom{n}{k} + \binom{n}{k+1} = \binom{n+1}{k+1} \text{ for } n,k \ge 0 \]</div><div style=display:inline-block;margin-right:2em>\[ \binom{k}{k} + \binom{k+1}{k} + ... + \binom{n}{k} = \binom{n+1}{k+1} \text{ for } n,k \ge 0 \]</div><div style=display:inline-block;margin-right:2em>\[ \binom{n}{k} \le \binom{n}{k} \text{ if and only if } k \le \frac{n-1}{2} \text{ and } n=2k+1 \]</div><div style=display:inline-block;margin-right:2em>\[ \binom{n}{k} \ge \binom{n}{k} \text{ if and only if } k \ge \frac{n-1}{2} \text{ and } n=2k+1 \]</div><div style=display:inline-block;margin-right:2em>\[ \binom{n}{a_1,a_2,...,a_k} = \frac{n!}{a_1!a_2!...a_k!} \]</div><div style=display:inline-block;margin-right:2em>\[ \binom{n}{a_1,a_2,...,a_k} = \binom{n}{a_1}\cdot \binom{n-a_1}{a_2} \cdot ... \cdot \binom{n-a_1-a_2-...-a_{k-1}}{a_k} \text{ if and only if } n=\sum_{i=1}^k a_i\]</div><div style=display:inline-block;margin-right:2em>\[ \binom{m}{k} = \frac{m(m-1)...(m-k+1)}{k!} \text{ for any real number } m \]</div><div style=display:inline-block;margin-right:2em>\[ (1+x)^m = \sum_{n\ge 0}^{\infty} \binom{m}{n}x^n \]</div><div style=display:inline-block;margin-right:2em>\[ \sum_{k=0}^n (-1)^k\binom{n}{k} = 0 \text{ for } n \ge 0 \]</div><div style=display:inline-block;margin-right:2em>\[ 2^n = \sum_{k=0}^n \binom{n}{k} = 0 \text{ for } n \ge 0 \]</div><div style=display:inline-block;margin-right:2em>\[ \sum_{k=1}^n k\binom{n}{k} = n 2^{n-1} \text{ for } n \ge 0 \]</div><div style=display:inline-block;margin-right:2em>\[ \binom{n+m}{k} = \sum_{i=0}^k \binom{n}{i} \binom{n}{k-i} \text{ for } n,m,k \ge 0 \]</div><br><div style=display:inline-block;margin-right:2em>\[ |E| = \frac{1}{2}\sum_{v\in V} deg(v) \]</div>or, the number of edges in a graph is half the sum of its vertex degrees.<table cellspacing=4><tr><td style=text-align:right>Compositions:</td><td>$$n$$ identical objects<br>$$k$$ distinct boxes</td><td>\[\binom{n-1}{k-1}\]</td><td>$$n$$ identical objects<br>any number of distinct boxes</td><td>$$2^{n-1}$$</td></tr><tr><td style=text-align:right>Weak Compositions:<br><i>(empty boxes allowed)</i></td><td>$$n$$ identical objects<br>$$k$$ distinct boxes</td><td>\[ \binom{n+k-1}{k-1} \]</td><td>$$n$$ distinct objects<br>$$k$$ distinct boxes</td><td>$$k^n$$</td></tr></table><br>Split $$n$$ people into $$k$$ groups of $$\frac{n}{k}$$:<br>Unordered:<div style=display:inline-block;margin-right:2em>\[ \frac{n!}{\left(\left(\frac{n}{k}\right)!\right)^k k!} \]</div>Ordered:<div style=display:inline-block;margin-right:2em>\[ \frac{n!}{\left(\left(\frac{n}{k}\right)!\right)^k} \]</div><br>Steps from (0,0) to (n,k) on a lattice:<div style=display:inline-block;margin-right:2em>\[\binom{n+k}{k}\]</div><br>Ways to roll $$n$$ dice so all numbers 1-6 show up at least once: $$6^n - \binom{6}{5}5^n + \binom{6}{4}4^n + \binom{6}{3}3^n + \binom{6}{2}2^n + \binom{6}{1}$$<br>The Sieve: $$|A_1| + |A_2| - |A_1\cap A_2|$$ or $$|A_1| + |A_2| + |A_3| - |A_1\cap A_2| - |A_1\cap A_3| - |A_2\cap A_3| + |A_1\cap A_2\cap A_3|$$<br>Also, $$|S - (S_A\cap S_B\cap S_C)| = |S| - |S_A| - |S_B| - |S_C| + |S_A\cap S_B| + |S_A\cap S_C| + |S_B\cap S_C| - |S_A\cap S_B\cap S_C|$$<br><br>Graphs: A <ins>simple graph</ins> has no loops (edges connecting a vertex to itself) or multiple edges between the same vertices. A <ins>walk</ins> is a path through a series of connected edges. A <ins>trail</ins> is a walk where no edge is traveled on more than once. A <ins>closed trail</ins> starts and stops on the same vertex. A <ins>Eulerian trail</ins> uses all edges in a graph. A trail that doesn't touch a vertex more than once is a <ins>path</ins>. $$K_n$$ is the complete graph on $$n$$-vertices.<br>If one can reach any vertex from any other on a graph $$G$$, then $$G$$ is a <ins>connected graph</ins>.<br>A connected graph has a closed Eulerian trail if and only if all its vertices have even degree. Otherwise, it has a Eulerian trail starting on S and ending on T if only S and T have odd degree.<br>In a graph without loops there are an even number of vertices of odd-degree. A cycle touches all vertices only once, except for the vertex it starts and ends on. A Hamiltonian cycle touches all vertices on a graph.<br>Let $$G$$ be a graph on $$n \ge 3$$ vertices, then $$G$$ has a Hamiltonian cycle if all vertices in $$G$$ have degree equal or greater than $$n/2$$. A complete graph $$K_n$$ has $$\binom{n}{k}\frac{(k-1)!}{2}$$ cycles of $$k$$-vertices.<br>Total Cycles:<div style=display:inline-block;margin-right:2em>\[ \sum_{k=3}^n\binom{n}{k}\frac{(k-1)!}{2} \]</div>Hamiltonian cycles:<div style=display:inline-block;margin-right:2em>\[ \frac{(n-1)!}{2} \]</div><br>Two graphs are the same if for any pair of vertices, the number of edges connecting them is the same in both graphs. If this holds when they are unlabeled, they are isomorphic.<br>A Tree is a minimally connected graph: Removing any edge yields a disconnected graph. Trees have no cycles, so any connected graph with no cycles is a tree.<br>All trees on $$n$$ vertices have $$n-1$$ edges, so any connected graph with $$n-1$$ edges is a tree.<br>Let $$T$$ be a tree on $$n \ge 2$$ vertices, then T has at least two vertices of degree 1.<br>Let $$F$$ be a forest on $$n$$ vertices with $$k$$ trees. Then F has $$n-k$$ edges.<br>Cayley's formula: The number of all trees with vertex set $$[n]$$ is $$A_n = n^{n-2}$$<br>A connected graph $$G$$ can be drawn on a plane so that no two edges intersect, then G is a planar graph.<br>A connected planar graph or convex polyhedron satisfies: Vertices + Faces = Edges + 2<br>$$K_{3,3}$$ and $$K_5$$ are not planar, nor is any graph with a subgraph that is edge-equivalent to them.<br>A convex Polyhedron with V vertices, E edges, and F faces satisfies: $$3F \le 3E$$, $$3V \le 3E$$, $$E \le 3V-6$$, $$E \le 3F-6$$, one face has at most 5 edges, and one vertex has at most degree 5.<br>$$K_{3,3}$$ has 6 vertices and 9 edges<br>A graph on $$n$$-vertices has $$\binom{n}{2}$$ possible edges.<br>If a graph is planar, then $$E \le 3V - 6$$. However, some non-planar graphs, like $$K_{3,3}$$, satisfy this too.<br>Prüfer code: Remove the smallest vertex from the graph, write down only its neighbor's value, repeat.<br><br><span style=font-size:120%><b>Math 462 - Combinatorial Theory II</b></span><br>All labeled graphs on $$n$$ vertices:<div style=display:inline-block;margin-right:2em>\[ 2^{\binom{n}{2}} \]</div><br>There are $$(n-1)!$$ ways to arrange $$n$$ elements in a cycle.<br>Given $$h_n=3 h_{n-1} - 2 h_{n-2}$$, change to $$h_{n+2}=3 h_{n+1} - 2 h_n$$, then $$h_{n+2}$$ is $$n=2$$ so multiply by $$x^{n+2}$$<br><div style=display:inline-block;margin-right:2em>\[ \sum h_{n+2}x^{n+2} = 3 \sum h_{n+1}x^{n+2} - 2 \sum h_n x^{n+2} \]</div>Then normalize to<div style=display:inline-block;margin-right:2em>\[ \sum_{n=2}^{\infty}h_n x^n \]</div>by subtracting $$h_0$$ and $$h_1$$<br><div style=display:inline-block;margin-right:2em>\[ \sum h_n x^n - x h_1 - h_0 = 3 x \sum h_{n+1}x^{n+1} - 2 x^2 \sum h_n x^n \]</div><br><div style=display:inline-block;margin-right:2em>\[ G(x) - x h_1 - h_0 = 3 x (G(x)-h_0) - 2 x^2 G(x) \]</div>Solve for:<div style=display:inline-block;margin-right:2em>\[ G(x) = \frac{1-x}{2x^2-3x+1} = \frac{1}{1-2x} = \sum (2x)^n \]</div><br><div style=display:inline-block;margin-right:2em>\[ G(x) = \sum(2x)^n = \sum 2^n x^n = \sum h_n x^n \rightarrow h_n=2^n \]</div><br><div style=display:inline-block;margin-right:2em>\[ \sum_{n=0}^{\infty} \frac{x^n}{n!} = e^x \]</div><div style=display:inline-block;margin-right:2em>\[ \sum_{n=0}^{\infty} (cx)^n = \frac{1}{1-cx} \]</div><br>Also,<div style=display:inline-block;margin-right:2em>\[ 2e_1 + 5e_2 = n \rightarrow \sum h_n x^n = \sum x^{2e_1+5e_2} = \sum x^{2e_1} x^{5e_2} = \left(\sum^{\infty} x^{2e_1}\right)\left(\sum^{\infty} x^{5e_2}\right)=\frac{1}{(1-x^2)(1-x^5)}\]</div><br><br>$$S(n,k)$$: A Stirling number of the second kind is the number of nonempty partitions of $$[n]$$ into k blocks where the order of the blocks doesn't matter. $$S(n,k)=S(n-1,k-1)+k S(n-1,k)$$, $$S(n,n-2) = \binom{n}{3} + \frac{1}{2}\binom{k}{2}\binom{n+2}{2}$$<br>Bell Numbers:<div style=display:inline-block;margin-right:2em>\[ B(n) = \sum_{i=0}^n S(n,i) \]</div>, or the number of all partitions of $$[n]$$ into nonempty parts (order doesn't matter).<br>Catalan Number $$C_n$$:<div style=display:inline-block>\[ \frac{1}{n+1}\binom{2n}{n} \]</div>derived from<div style=display:inline-block;margin-right:2em>\[ \sum_{n \ge 1} c_{n-1} x^n = x C(x) \rightarrow C(x) - 1 = x C(x)\cdot C(x) \rightarrow C(x) = \frac{1 - \sqrt{1-4x}}{2x} \]</div><br>Products: Let $$A(n) = \sum a_n x^n$$ and $$B(n) = \sum b_n x^n$$. Then $$A(x)B(x)=C(x)=\sum c_n x^n$$ where<div style=display:inline-block>\[ c_n = \sum_{i=0}^n a_i b_{n-i} \]</div><br><br>Cycles: The number of $$n$$-permuatations with $$a_i$$ cycles of length $$i \in [n]$$ is \frac{n!}{a_1!a_2!...a_n!1^{a_1}2^{a_2}...n^{a_n}}<br>The number of n-permutations with only one cycle is $$(n-1)!$$<br>$$c(n,k)$$: The number of $$n$$-permutations with $$k$$-cycles is called a signless stirling number of the first kind.<br>$$c(n,k) = c(n-1,k-1) + (n-1) c(n-1,k)$$<div style=display:inline-block;margin-left:2em>\[ c(n,n-2)= 2\binom{n}{3} + \frac{1}{2}\binom{n}{2}\binom{n-2}{2} \]</div><br>$$s(n,k) = (-1)^{n-k} c(n,k)$$ and is called a signed stirling number of the first kind.<br><br>Let $$i \in [n]$$, then fro all $$k \in [n]$$, there are $$(n-1)!$$ permutations that contain $$i$$ in a $$k$$-cycle.<br><br><div style=display:inline-block;margin-right:2em>\[ T(n,k)=\frac{k-1}{2k}n^2 - \frac{r(k-r)}{2k} \]</div>Let Graph $$G$$ have $$n$$ vertices and more than $$T(n,k)$$ edges. Then $$G$$ contains a $$K_{k+1}$$ subgraph, and is therefore not $$k$$-colorable.<br><br>$$N(T)$$ = all neighbors of the set of vertices $$T$$<br>$$a_{s,d} = \{ s,s+d,s+2d,...,s+(n-1)d \}$$<br>$$\chi(H)$$: Chromatic number of Graph $$H$$, or the smallest integer $$k$$ for which $$H$$ is $$k$$-colorable.<br>A 2-colorable graph is bipartite and can divide its vertices into two disjoint sets.<br>A graph is bipartite if and only if it does not contain a cycle of odd length.<br>A bipartite graph has at most $$\frac{n^2}{4}$$ edges if $$n$$ is even, and at most $$\frac{n^2 - 1}{4}$$ edges if $$n$$ is odd.<br>If graph $$G$$ is not an odd cycle nor complete, then $$\chi(G) \le$$ the largest vertex degree $$\ge 3$$.<br>A bipartite graph on $$n$$ vertices with a max degree of $$k$$ has at most $$k\cdot (n-k)$$ edges.<br>A tree is always bipartite (2-colorable).<br><br>Philip Hall's Theorem: Let a bipartite graph $$G=(X,Y)$$. Then $$X$$ has a perfect matching to $$Y$$ if and only if for all $$T \subset X, |T| \le |N(T)|$$<br><br>$$R(k,l):$$ The Ramsey Number $$R(k,l)$$ is the smallest integer such that any 2-coloring of a complete graph on $$R(k,l)$$ vertices will contain a red $$k$$-clique or blue $$l$$-clique. A $$k$$-clique is a complete subgraph on $$k$$ vertices.<br><div style=display:inline-block;margin-right:2em>\[ R(k,l) \le R(k,l-1) + R(k-1,l) \]</div><div style=display:inline-block;margin-right:2em>\[ R(k,k) \le 4^{k-1} \]</div><div style=display:inline-block;margin-right:2em>\[ R(k,l) \le \binom{k+l-2}{l-1} \]</div><br><br>Let $$P$$ be a partially ordered set (a "poset"), then:<br>1) $$\le$$ is reflexive, so $$x \le x$$ for all $$x \in P$$<br>2) $$\le$$ is transitive, so that if $$x \le y$$ and $$y \le z$$ then $$x \le z$$<br>3) $$\le$$ is anti-symmetric, such that if $$x\le y$$ and $$y \le x$$, then $$x=y$$<br><br>Let $$P$$ be the set of all subsets of $$[n]$$, and let $$A \le B$$ if $$A \subset B$$. Then this forms a partially ordered set $$B_n$$, or a Boolean Algebra of degree $$n$$.<br><br><div style=display:inline-block;margin-right:2em>\[ E(X)=\sum_{i \in S} i\cdot P(X=i) \]</div><br>A chain is a set with no two incomparable elements. An antichain has no comparable elements.<br>Real numbers are a chain. $$\{ (2,3), (1,3), (3,4), (2,4) \}$$ is an antichain in $$B_4$$, since no set contains another.<br>Dilworth: In a finite partially ordered set $$P$$, the size of any maximum antichain is equal to the number of chains in any chain cover.<br><br>Weak composition of n into 4 parts: $$a_1 + a_2 + a_3 + a_4 = n$$<br>Applying these rules to the above equation: $$a_1 \le 2, a_2\text{ mod }2 \equiv 0, a_3\text{ mod }2 \equiv 1, a_3 \le 7, a_4 \ge 1$$<br>Yields the following:<div style=display:inline-block;margin-right:2em>\[ a_1 + 2a_2 + (2a_3 + 1) + a_4 = n \]</div><div style=display:inline-block;margin-right:2em>\[(\sum_0^2 x^{a_1})(\sum_0^{\infty} x^{2a_2})(\sum_0^3 x^{2a_3 + 1})(\sum_1^{\infty} x^{a_4})=\frac{1+x+x^2}{1-x^2}(x+x^3+x^5+x^7)\left(\frac{1}{1-x} - 1\right)\]</div><br><br><span style=font-size:120%><b>Math 394 - Probability I</b></span><br>both E and F:<div style=display:inline-block;margin-right:2em>\[ P(EF) = P(E\cap F) \]</div>If E and F are independent, then<div style=display:inline-block;margin-right:2em>\[ P(E\cap F) = P(E)P(F) \]</div><br><br>$$P(F) = P(E) + P(E^c F)$$ $$P(E\cup F) = P(E) + P(F) - P(EF)$$<br>$$P(E\cup F \cup G) = P(E) + P(F) + P(G) - P(EF) - P(EG) - P(FG) + P(EFG)$$<br><br>E occurs given F:<div style=display:inline-block;margin-right:2em>\[P(E|F)=\frac{P(EF)}{P(F)} \]</div>$$P(EF)=P(FE)=P(E)P(F|E)$$<br>Bayes formula:<div style=display:inline-block;margin-right:2em>\[ P(E)=P(EF)+P(EF^c) \]</div><div style=display:inline-block;margin-right:2em>\[P(E)=P(E|F)P(F) + P(E|F^c)P(F^c) \]</div><div style=display:inline-block;margin-right:2em>\[P(E)=P(E|F)P(F) + P(E|F^c)[1 - P(F)] \]</div><br><div style=display:inline-block;margin-right:2em>\[ P(B|A)=P(A|B)\frac{P(B)}{P(A)} \]</div><br>The odds of A:<div style=display:inline-block;margin-right:2em>\[ \frac{P(A)}{P(A^c)} = \frac{P(A)}{1-P(A)} \]</div><div style=display:inline-block;margin-right:2em>\[P[\text{Exactly } k \text{ successes}]=\binom{n}{k}p^k(1-p)^{n-k} \]</div><br><div style=display:inline-block;margin-right:2em>\[ P(n \text{ successes followed by } m \text{ failures}) = \frac{p^{n-1}(1-q^m)}{p^{n-1}+q^{m-1}-p^{n-1}q^{m-1}} \]</div>where $$p$$ is the probability of success, and $$q=1-p$$ for failure.<br><br><div style=display:inline-block;margin-right:2em>\[ \sum_{i=1}^{\infty}p(x_i)=1 \]</div>where $$x_i$$ is the $$i^{\text{th}}$$ value that $$X$$ can take on.<br><div style=display:inline-block;margin-right:2em>\[ E[X] = \sum_{x:p(x) > 0}x p(x) \text{ or }\sum_{i=1}^{\infty} x_i p(x_i) \]</div><div style=display:inline-block;margin-right:2em>\[ E[g(x)]=\sum_i g(x_i)p(x_i) \]</div><div style=display:inline-block;margin-right:2em>\[ E[X^2] = \sum_i x_i^2 p(x_i) \]</div><br><div style=display:inline-block;margin-right:2em>\[ Var(X) = E[X^2] - (E[X])^2 \]</div><div style=display:inline-block>\[ Var(aX+b) = a^2 Var(X) \]</div>for constant $$a,b$$<br><div style=display:inline-block;margin-right:2em>\[ SD(X) = \sqrt{Var(X)} \]</div><br><br>Binomial random variable $$(n,p)$$:<div style=display:inline-block>\[ p(i)=\binom{n}{i}p^i(1-p)^{n-i}\; i=0,1,...,n\]</div>where $$p$$ is the probability of success and $$n$$ is the number of trials.<br>Poisson:<div style=display:inline-block;margin-right:2em>\[ E[X] = np \]</div><div style=display:inline-block;margin-right:2em>\[ E[X^2] = \lambda(\lambda + 1) \]</div><div style=display:inline-block;margin-right:2em>\[ Var(X)=\lambda \]</div><div style=display:inline-block;margin-right:2em>\[ P[N(t)=k)] = e^{-\lambda t}\frac{(\lambda t)^k}{k!} \: k=0,1,2,... \]</div>where $$N(t)=[s,s+t]$$<br><div style=display:inline-block;margin-right:2em>\[ E[X] = \int_{-\infty}^{\infty} x f(x) \,dx \]</div><div style=display:inline-block;margin-right:2em>\[ P\{X \le a \} = F(a) = \int_{-\infty}^a f(x) \,dx \]</div><div style=display:inline-block;margin-right:2em>\[ \frac{d}{da} F(g(a))=g'(a)f(g(a))\]</div><div style=display:inline-block;margin-right:2em>\[ E[g(X)]=\int_{-\infty}^{\infty} g(x) f(x) \,dx \]</div><div style=display:inline-block;margin-right:2em>\[ P\{ a \le X \le b \} = \int_a^b f(x) \,dx \]</div><br>Uniform:<div style=display:inline-block;margin-right:2em>\[ f(x) = \frac{1}{b-a} \text{ for } a\le x \le b \]</div><div style=display:inline-block;margin-right:2em>\[ E[X] = \frac{a+b}{2} \]</div><div style=display:inline-block;margin-right:2em>\[ Var(X) = \frac{(b-a)^2}{12}\]</div><br>Normal:<div style=display:inline-block;margin-right:2em>\[ f(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \text{ for } -\infty \le x \le \infty \]</div><div style=display:inline-block;margin-right:2em>\[ E[X] = \mu \]</div><div style=display:inline-block;margin-right:2em>\[ Var(X) = \sigma^2 \]</div><div style=display:inline-block;margin-right:2em>\[ Z = \frac{X-\mu}{\sigma} \]</div><br><div style=display:inline-block;margin-right:2em>\[ P\left[a \le X \le b\right] = P\left[ \frac{a - \mu}{\sigma} &lt; \frac{X - \mu}{\sigma} < \frac{b - \mu}{\sigma}\right] = \phi\left(\frac{b-\mu}{\sigma}\right) - \phi\left(\frac{a-\mu}{\sigma}\right) \]</div><br><br><br><div style=display:inline-block;margin-right:2em>\[ \phi(x) = GRAPH \]</div><div style=display:inline-block;margin-right:2em>\[ P[X \le a]=\phi\left(\frac{a-\mu}{\sigma}\right) \]</div><div style=display:inline-block;margin-right:2em>\[ P[Z > x] = P[Z \le -x] \]</div><div style=display:inline-block;margin-right:2em>\[ \phi(-x)=1-\phi(x) \]</div></br><div style=display:inline-block;margin-right:2em>\[ Y=f(X) \]</div><div style=display:inline-block;margin-right:2em>\[ F_Y=P[Y\le a]= P[f(X) \le a] = P[X \le f^{-1}(a)]=F_x(f^{-1}(a)) \]</div><div style=display:inline-block;margin-right:2em>\[ f_Y=\frac{d}{da}(f^{-1}(a))f_x(f^{-1}(a)) \]</div><div style=display:inline-block;margin-right:2em>\[ Y = X^2 \]</div><div style=display:inline-block;margin-right:2em>\[ F_Y = P[Y \le a] = P[X^2 \le a] = P[-\sqrt{a} \le X \le \sqrt{a}] = \int_{-\sqrt{a}}^{\sqrt{a}} f_X(x) dx \]</div><div style=display:inline-block;margin-right:2em>\[ f_Y = \frac{d}{da}(F_Y) \]</div><br><br><div style=display:inline-block;margin-right:2em>\[N(k) \ge x \rightarrow 1 - N(k) &lt; x \]</div><div style=display:inline-block;margin-right:2em>\[ P(A \cap N(k) \ge x) = P(A) - P(A \cap N(k) < x) \]</div><br><br><br><br>Discrete:<div style=display:inline-block;margin-right:2em>\[ P(X=1) = P(X \le 1) - P(X &lt; 1) \]</div>Continuous:<div style=display:inline-block;margin-right:2em>\[ P(X \le 1) = P(X &lt; 1) \]</div><br><br>Exponential:<div style=display:inline-block;margin-right:2em>\[ f(x) = \lambda e^{-\lambda x} \text{ for } x \ge 0 \]</div><div style=display:inline-block;margin-right:2em>\[ E[X] = \frac{1}{\lambda} \]</div><div style=display:inline-block;margin-right:2em>\[ Var(X) = \frac{1}{\lambda^2} \]</div><br><br>Gamma:<div style=display:inline-block;margin-right:2em>\[ f(x) = \frac{\lambda e^{-\lambda x} (\lambda x)^{\alpha - 1}}{\Gamma(\alpha)} \]</div><div style=display:inline-block;margin-right:2em>\[ \Gamma(\alpha)=\int_0^{\infty} e^{-x}x^{\alpha-1} \,dx \]</div><div style=display:inline-block;margin-right:2em>\[ E[X] = \frac{\alpha}{\lambda} \]</div><div style=display:inline-block;margin-right:2em>\[ Var(X) = \frac{\alpha}{\lambda^2} \]</div><br><div style=display:inline-block;margin-right:2em>\[ E[X_iX_j] = P(X_i = k \cap X_j=k) = P(X_i = k)P(X_j = k|X_i=k) \]</div><br><br><span style=font-size:120%><b>Stat 390 - Probability and Statistics for Engineers and Scientists</b></span><br><div style=display:inline-block;margin-right:2em>\[ p(x) = \text{categorical (discrete)} \]</div><div style=display:inline-block>\[ f(x) = \text{continuous} \]</div><div style=float:right><table cellpadding=4><tr><td></td><td>$$\mu_x=E[x]$$</td><td>$$\sigma_x^2 = V[x]$$</td></tr><tr><td>Binomial</td><td>$$n\pi$$</td><td>$$n\pi (1-\pi)$$</td></tr><tr><td>Normal</td><td>$$\mu$$</td><td>$$\sigma^2$$</td></tr><tr><td>Poisson</td><td>$$\lambda$$</td><td>$$\lambda$$</td></tr><tr><td>Exponential</td><td>$$\frac{1}{\lambda}$$</td><td>$$\frac{1}{\lambda^2}$$</td></tr><tr><td>Uniform</td><td>$$\frac{b+a}{2}$$</td><td>$$\frac{(b-a)^2}{12}$$</td></tr></table></div><table cellpadding=0><tbody><tr><td>Binomial</td><td>\[ p(x) = \binom{n}{x} \pi^x (1 - \pi)^{n-x}\]</td></tr><tr><td>Poisson</td><td>\[ p(x) = \frac{e^{-\lambda} \lambda^x}{x!} \]</td></tr><tr><td>Normal</td><td>\[ f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2} \left(\frac{x - \mu}{\sigma} \right)}\]</td></tr><tr><td>Exponential</td><td>\[ f(x) = \lambda e^{-\lambda x} \]</td></tr><tr><td>Uniform</td><td>\[ f(x) = \frac{1}{b-a}\]</td></tr></tbody></table><div style=display:inline-block;margin-right:2em>$$\pi = p =$$ proportion</div><div style=display:inline-block;margin-right:2em>$$n\pi = C = \lambda =$$ mean</div><div style=display:inline-block;margin-right:2em>$$\mu = n \pi$$</div><div style=display:inline-block;margin-right:2em>$$\sigma^2 = n \pi (1 - \pi)$$</div><table cellpadding=5><tbody><tr><td>Sample mean: $$\bar{x} =$$</td><td>\[ \frac{1}{n} \sum_{i=1}^n x_i \]</td><td>Sample median:</td><td>\[\tilde{x} = \text{if } n \text{ is odd,} \left(\frac{n+1}{2}\right)^{\text{th}} \text{ value} \] \[ \text{if } n \text{ is even, average} \frac{n}{2} \text{ and } \frac{n}{2}+1\]</td></tr></tbody></table><table cellpadding=5><tbody><tr><td>Sample variance:</td><td>\[s^2 = \frac{1}{n-1}\sum (x_i - \bar{x})^2 = \frac{S_{xx}}{n-1} = \sum x_i^2 - \frac{1}{n} \left( \sum x_i \right)^2\]</td><td>Standard deviation: $$s = \sqrt{s^2}$$</td></tr></tbody></table><br>low/high quartile: median of lower/upper half of data. If $$n$$ is odd, include median in both.<br><table cellpadding=5><tbody><tr><td>low = 1<sup>st</sup> quartile</td><td>high = 3<sup>rd</sup> quartile</td><td>median = 2<sup>nd</sup> quartile</td></tr></tbody></table><br>IQR: high - low quartiles<br>range: max - min<br>Total of something: $$\bar{x} n$$<br>An outlier is any data point outside the range defined by IQR $$\cdot 1.5$$<br>An <i>extreme</i> is any data point outside the range defined by IQR $$\cdot 3$$<br><br><div style=display:inline-block;margin-right:2em>\[ \mu_{\bar{x}} = \mu = \bar{x} \]</div><div style=display:inline-block;margin-right:2em>\[ \sigma_{\bar{x}} = \frac{\sigma}{\sqrt{n}} \]</div><div style=display:inline-block;margin-right:2em>\[ \mu_p = p = \pi \]</div><div style=display:inline-block;margin-right:2em>\[ \sigma_p = \sqrt{\frac{p(1-p)}{n}} \]</div><br><table cellpadding=5><tbody><tr><td>$$p(x)$$ distribution<br><i>[discrete]</i></td><td>mean: $$\mu_x = \sum x \cdot p(x) = E[x]$$<br>variance: $$\sigma_x^2 = \sum(x-\mu_x)^2 p(x) = V[x]$$</td></tr><tr><td>$$f(x)$$ distribution<br><i>[continuous]</i></td><td>mean:<div style=display:inline-block;margin-right:2em>\[ \mu_x = \int_{-\infty}^{\infty} x \cdot f(x) \]</div><br>median:<div style=display:inline-block;margin-right:2em>\[ \tilde{\mu} \rightarrow \int_{-\infty}^{\tilde{\mu}} f(x) = \int_0^{\tilde{\mu}} f(x) = 0.5 \]</div><br>variance:<div style=display:inline-block;margin-right:2em>\[ \sigma^2 = \int_{-\infty}^{\infty} (x =\mu_x)\cdot f(x) \]</div></td></tr><tr><td>Normal</td><td><div style=display:inline-block;margin-right:2em>\[ k = \frac{\mu + k \sigma - \mu}{\sigma} \]</div>standardize:<div style=display:inline-block;margin-right:2em>\[ \frac{x - \mu}{\sigma} \]</div><div style=display:inline-block;margin-right:2em>\[ -k = \frac{\mu - k \sigma - \mu}{\sigma} \]</div>upper quartile:<div style=display:inline-block;margin-right:2em>\[ \mu + 0.675\sigma \]</div>lower quartile:<div style=display:inline-block;margin-right:2em>\[ \mu - 0.675\sigma \]</div></td></tr><tr><td>Exponential</td><td><div style=display:inline-block;margin-right:2em>\[ -ln(c) \cdot \frac{1}{\lambda} \text{ where c is the quartile }(0.25,0.5,0.75) \]</div></td></tr></tbody></table><div style=display:inline-block;margin-right:2em>\[ S_{xx} = \sum x_i^2 - \frac{1}{n}\left(\sum x_i\right)^2 = \sum(x_i - \bar{x})^2 \]</div><div style=display:inline-block>\[ S_{yy} = \sum y_i^2 - \frac{1}{n}\left(\sum y_i\right)^2 = \sum(y_i - \bar{y})^2 \]</div>\[ S_{xy} = \sum{x_i y_i} - \frac{1}{n}\left(\sum x_i\right)\left(\sum y_i\right) \] \[ \text{SSResid} = \text{SSE (error sum of squares)} = \sum(y_i - \hat{y}_i)^2 = S_{yy} - b S_{xy} \]<div style=display:inline-block;margin-right:2em>\[ \text{SSTo} = \text{SST} = S_{yy} = \text{Total sum of squares} = \text{SSRegr} + \text{SSE} = \text{SSTr} + \text{SSE} = \sum_i^k \sum_j^n (x_{ij} - \bar{\bar{x}})^2 \]</div><div style=display:inline-block>\[ \text{SSRegr} = \text{regression sum of squares} \]</div><div style=display:inline-block;margin-right:2em>\[ r^2 = 1 - \frac{\text{SSE}}{\text{SST}} = \frac{\text{SSRegr}}{\text{SST}} = \text{coefficient of determination} \]</div><br><div style=display:inline-block;margin-right:2em>\[ r = \frac{S_{xy}}{\sqrt{S_{xx}}\sqrt{S_{yy}}} \]</div><div style=display:inline-block;margin-right:2em>\[ \hat{\beta} = \frac{S_{xy}}{S_{xx}} \]</div><div style=display:inline-block;margin-right:2em>\[ \hat{\alpha} = \bar{y} - \beta\bar{x} \]</div>prediction:<div style=display:inline-block;margin-right:2em>\[ \hat{\alpha} = \bar{y} - \beta\bar{x} \]</div>Percentile ($$\eta_p$$):<div style=display:inline-block;margin-right:2em>\[ \int_{-\infty}^{\eta_p} f(x) = p \]</div><br>MSE (Mean Square Error) =<div style=display:inline-block;margin-right:2em>\[ \frac{1}{n} SSE = \frac{1}{n}\sum (y_i-\hat{y}_i)^2 = \frac{1}{n} \sum (y_i - \alpha - \beta x_i)^2 \]</div><br>MSTr (Mean Square for treatments)<br><br><ins>ANOVA</ins><br><div style=display:inline-block;margin-right:2em>\[ SST = SS_{\text{explained}} + SSE \]</div><div style=display:inline-block;margin-right:2em>\[ R^2 = 1 - \frac{SSE}{SST} \]</div><div style=display:inline-block;margin-right:2em>\[ \sum(y_i - \bar{y})^2 = \sum(\hat{y}_i - \bar{y})^2 + \sum(y_i - \hat{y}_i)^2 \]</div><div style=display:inline-block;margin-right:2em>\[ s_e^2 = \frac{SSE}{n-2} \text{ or } \frac{SSE}{n-(k+1)} \]</div><div style=display:inline-block;margin-right:2em>\[ R_{adj}^2 = 1-\frac{SSE(n-1)}{SST(n-(k+1))}\]</div><div style=display:inline-block;margin-right:2em>\[ H_0: \mu_1 = \mu_2 = .. = \mu_k \]</div><div style=display:inline-block;margin-right:2em>\[ \bar{\bar{x}} = \left(\frac{n_1}{n}\right)\bar{x}_1 + \left(\frac{n_2}{n}\right)\bar{x}_2 + ... + \left(\frac{n_k}{n}\right)\bar{x}_k \]</div><div style=display:inline-block;margin-right:2em>\[ SSTr = n_1(\bar{x}_1 - \bar{\bar{x}})^2 + n_2(\bar{x}_2 - \bar{\bar{x}})^2 + ... + n_k(\bar{x}_k - \bar{\bar{x}})^2 \]</div><br><div style=float:right><table cellpadding=4 cellspacing=0><tr><td style="border-right:1px solid #000;border-bottom:1px solid #000">Sources</td><td style="border-bottom:1px solid #000">df</td><td style="border-bottom:1px solid #000">SS</td><td style="border-bottom:1px solid #000">MS</td><td style="border-bottom:1px solid #000">F</td></tr><tr><td style="border-right:1px solid #000">Between Samples</td><td>$$k-1$$</td><td>SSTr</td><td>MSTr</td><td>$$\frac{\text{MSTr}}{\text{MSE}}$$</td></tr><tr><td style="border-right:1px solid #000">Within Samples</td><td>$$n-k$$</td><td>SSE</td><td>MSE</td></tr><tr><td style="border-right:1px solid #000">Total</td><td>$$n-1$$</td><td>SST</td></tr></table></div><br>One-sample t interval:<div style=display:inline-block;margin-right:2em>\[ \bar{x} \pm t^* \frac{s}{\sqrt{n}} \]</div>(CI)<br>Prediction interval:<div style=display:inline-block;margin-right:2em>\[ \bar{x} \pm t^* s\sqrt{1+\frac{1}{n}} \]</div>(PI)<br>Tolerance interval:<div style=display:inline-block;margin-right:2em>\[ \bar{x} \pm k^* s \]</div><br>Difference t interval:<div style=display:inline-block;margin-right:2em>\[ \bar{x}_1 - \bar{x}_2 \pm t^*\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}} \]</div><br>Adjusted<div style=display:inline-block;margin-right:2em>\[ R^2 = 1 - \left(\frac{n-1}{n-(k+1)}\right) \frac{SSE}{SST} \]</div><br>Type I error: Reject $$H_0$$ when true; If the F-test p-value is small, it's useful.<br>Type II error: Don't reject $$H_0$$ when false.<br>B(Type II) =<div style=display:inline-block;margin-right:2em>\[ z^* \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}} \]</div><br><br>Simple linear regression: $$y = \alpha + \beta x$$<br>General multiple regression: $$y = \alpha + \beta_1 x_1 + ... + \beta_k x_k$$<br>Prediction error: $$\hat{y} - y^* = \sqrt{s_{\hat{y}}^2 + s_e^2} \cdot t$$<div style=display:inline-block;margin-right:2em>\[ t = \frac{\hat{y}-y^*}{\sqrt{s_{\hat{y}}^2 + s_e^2}} \]</div><br><div style=display:inline-block;margin-right:2em>\[ P(\hat{y} - y^* > 11) = P\left(\sqrt{s_{\hat{y}}^2 + s_e^2} \cdot t > 11\right) = P\left(t > \frac{11}{\sqrt{s_{\hat{y}}^2 + s_e^2}}\right) \]</div><br><div style=display:inline-block;margin-right:2em>\[ \mu_{\hat{x}_1 - \hat{x}_2} = \mu_{\hat{x}_1} - \mu_{\hat{x}_2} = \mu_1 - \mu_2 \]</div><div style=display:inline-block;margin-right:2em>\[ \sigma_{\hat{x}_1 - \hat{x}_2} = \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}} \]</div><div style=display:inline-block;margin-right:2em>\[ \hat{x}_1 - \hat{x}_2 \pm z^* \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}} \]</div><br><br>To test if $$\mu_1 = \mu_2$$, use a two-sample t test with $$\delta = 0$$<br>If you're looking for the true average, it's a CI, not the standard deviation about the regression.<br>A large n value may indicate a z--test, but you must think about whether or not its paired or not paired.<br>A hypothesis is only valid if it tests a <i>population</i> parameter. Do <b>not</b> extrapolate outside of a regression analysis unless you use a future predictor.<br><br><ins>F-Test</ins><br><div style=display:inline-block;margin-right:2em>\[ F = \frac{MSRegr}{MSE} \]</div><div style=display:inline-block;margin-right:2em>\[ MSRegr = \frac{SSRegr}{k} \]</div><div style=display:inline-block;margin-right:2em>\[ MSResid = \frac{SSE}{n - (k+1)} \]</div><div style=display:inline-block;margin-right:2em>\[ H_0: \beta_1=\beta_2=...=\beta_k=0 \]</div><br><br><ins>Chi-Squared ($$\chi^2$$)</ins><br><div style=display:inline-block;margin-right:2em>\[ H_0: \pi_1 = \frac{\hat{n}_1}{n_1},\pi_2 = \frac{\hat{n}_2}{n_2}, ..., \pi_k = \frac{\hat{n}_k}{n_k} \]</div><div style=display:inline-block;margin-right:2em>\[ \chi^2 = \sum_{i=1}^k \frac{(n_i - \hat{n}_i)^2}{\hat{n}_i} = \sum \left(\frac{\text{observed - expected}}{\text{expected}}\right) \]</div><br><br><ins>Linear Association Test</ins><br><div style=display:inline-block;margin-right:2em>\[ H_0: p=0 \]</div><div style=display:inline-block;margin-right:2em>\[ t = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}} \]</div><div style=display:inline-block;margin-right:2em>\[ \sigma_{\hat{y}} = \sigma \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{S_{xx}} } \]</div><div style=display:inline-block;margin-right:2em>\[ \mu_{\hat{y}} = \hat{y} = \alpha + \beta x^* \]</div><div style=display:inline-block;margin-right:2em>\[ \hat{y} \pm t^* s_{\hat{y}} df = n-2 \text{ for a mean y value (population)} \]</div><div style=display:inline-block;margin-right:2em>\[ \hat{y} \pm t^* \sqrt{s_e^2 + s_{\hat{y}}^2} df=n-2 \text{ for single future y-values (PI)} \]</div><br><br><ins>Paired T-test</ins><br><div style=display:inline-block;margin-right:2em>\[ \bar{d} = \bar{(x-y)} \]</div><div style=display:inline-block;margin-right:2em>\[ t = \frac{\bar{d} - \delta}{\frac{s_d}{\sqrt{n}}} \]</div><div style=display:inline-block;margin-right:2em>\[ \sigma_d = \sigma \text{ of x-y pairs } = s_d \]</div><br><br>Large Sample:<div style=display:inline-block;margin-right:2em>\[ z = \frac{\bar{x} - 0.5}{\frac{s}{\sqrt{n}}} \]</div><div style=display:inline-block;margin-right:2em>\[ \text{P-value } \le \alpha \]</div><br>Small Sample:<div style=display:inline-block;margin-right:2em>\[ z = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}} \]</div><div style=display:inline-block;margin-right:2em>\[ df = n-1 \]</div><br><br>Difference:<br><div style=display:inline-block;margin-right:2em>\[ H-0: \mu_1 - \mu_2 = \delta \]</div><div style=display:inline-block;margin-right:2em>\[ t = \frac{\bar{x}_1 - \bar{x}_2 - \delta}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \]</div><div style=display:inline-block;margin-right:2em>\[ df = \frac{\left(\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)}{\frac{1}{n_1-1} \left(\frac{s_1^2}{n_1}\right)^2 + \frac{1}{n_2-1} \left(\frac{s_2^2}{n_2}\right)^2 } \]</div></div></article></div></section><section><div class=dim><aside><h2>Contact</h2><ul></ul></aside><article><blockquote>*"We must be the change we want to see in the world." - Mahatma Gandhi*</blockquote>Given the enormous influence of The Matrix on a lot of what I make, many people assume that it is my favorite movie. My favorite movie of all time is actually [Contact](https://en.wikipedia.org/wiki/Contact_(film)).<p>Many people assume the message behind Contact is that Religion and Science aren&rsquo;t mutually exclusive, but this is only part of the equation. Contact is really about <em>faith</em>. It&rsquo;s about never losing faith in your own passion, regardless of what it is. It can be science, religion, art, music, it doesn&rsquo;t matter. All that matters is that you find your passion, and you have faith in it, and no matter what happens, no matter what stands in your way, you fight for it.</p><p>Naturally, in real life, we don&rsquo;t have a Dues Ex Hadden Industries to magic away all our problems, but the movie does an excellent job of making sure the drama that plays out on screen does not undermine it&rsquo;s purpose. Even with virtually unlimited funding, Ellie finds out she&rsquo;s being forcefully kicked out of the Very Large Array. No matter what anyone says to her, she refuses to let it go. At that point in the movie, it is abundantly clear that absolutely nothing will stop this woman from pursuing her passion, no matter how insane or misunderstood it is, no matter what obstacles are put in her path, no matter what friends she may lose along the way. At this darkest hour, after everything seems to be falling apart, she goes to a cliffside to think.</p><p>And then she comes back.</p><p>She comes back, and she starts listening again, because she isn&rsquo;t going to give up, and <em>that</em> is when she hears the signal. <em>That</em> is when our hollywood movie is yanked out of reality and brings us on a fantastic journey through the stars. It happens after Ellie <strong>refuses to give up</strong>, even after the universe itself seems to be conspiring to ruin her.</p><p>Then, the project is stolen from her repeatedly, and she has to fight to keep it. Then, she is denied her desire to be the one to ride through the machine. Then the machine is completely destroyed and everything she fought for is taken away from her.</p><p><em>Then</em>, our little Dues Ex Hadden comes back. Hadden gave Ellie her funding because of her emotional outburst at the investor meeting. Hadden saw the passion she had, the unstoppable fire and unshakable <em>faith</em> she had in her science project, and <em>that</em> is when he decided to fund her. Once again, Hadden moves us into the realm of fiction, and gives Ellie a second chance.</p><p>Contact is brilliant because it begins firmly anchored in reality, and only later, with a few nudges in the right direction, throws us into a world of make-believe. Carl Sagan is trying to make us ask, what if? What if we could fly? What if we could go to the moon? What if we colonized Mars? What if aliens contacted us? Contact shows us how beautiful life can be. It reminds us of all the amazing things that human beings are capable of when we ask that simple question, <em>What If?</em></br></br>To answer it, we just need a little faith.</p></article></div></section><section><div class=dim><aside><h2>The Dark Side of Web Development</h2><ul></ul></aside><article><p>I am, by no means, a very good web developer. I am, however, a highly experienced C++ developer. The end result is that people usually ask me why I like C++ so much when everyone else hates it, and everyone is supposedly in agreement that C++ is the worst language ever made and unless you are writing drivers you should never, ever use it. I mean, you can write games in Python now, right?</p><p>While these kinds of ignorant opinions are frustrating for me and my fellow C++ developers who actually need efficient cross-platform programs, that&rsquo;s not what I&rsquo;m here to debate. In fact, if you think C++ is a complete piece of shit, I won&rsquo;t argue with you about that at all. For all I know, you may be right!</p><p>Unfortunately for you, <strong>web development is just as bad as C++</strong>.</p><p>Of course, I just said that I&rsquo;m not very good at web development, so doesn&rsquo;t this disqualify me from having an opinion about it? On the contrary, the fact that I&rsquo;m not very good at web development is extremely important. The reason is that one of the major complaints about C++ is that <em>good C++ code is hard to write if you are an inexperienced developer</em>. I can certainly attest to that fact, it&rsquo;s taken me years to develop my C++ skills to this point. The fact that Python code is so easy and natural to write is one of its major selling points.</p><p>The thing is, <em>good HTML/CSS code is also hard to write if you are an inexperienced developer</em>. I can attest to this too, because I&rsquo;m an inexperienced developer, and its <em>really goddamn hard</em> to write good HTML/CSS code. Let me clarify what I mean by &ldquo;good&rdquo; here: by &ldquo;good&rdquo; I mean code that works on all platforms, doesn&rsquo;t blow up, and doesn&rsquo;t contain strange edge cases that will cause it to fail for no reason. Many developers preach the joys of functional programming, which eliminates most edge cases by prohibiting functions that have side-effects.</p><p>Everyone knows about the subtle, nasty bugs that can crop up in C++, often as a result of doing something that was intuitive, yet wrong. This is rehashed in the &ldquo;C++ is bad&rdquo; arguments over and over and over without end, <em>especially</em> with the functional programming zealots. The problem is that HTML/CSS has <em>all sorts of really stupid crap</em> that can happen when you do something subtly wrong with the HTML that is not immediately obvious to a new developer.</p><p>It took me a while to learn that you shouldn&rsquo;t be putting block level elements inside inline elements in your HTML - it will <em>almost always</em> work, until it doesn&rsquo;t, and it won&rsquo;t be obvious what exactly is going wrong. Many people new to HTML aren&rsquo;t even aware of HTML resets, so they&rsquo;ll have to try and remember which elements are block-level by default and which aren&rsquo;t. Then the new standard introduced a bunch of psuedo-block level elements that try to fix some of these problems, except they aren&rsquo;t supported by everything because <em>MICROSOFT</em>, so now you&rsquo;re stuck with a bunch of workarounds for IE that take forever to develop.</p><p>Just recently I discovered an amazingly weird bug with my webpage, wherein upon first loading it, all my buttons would be stacked vertically. It appeared that Chrome was ignoring <code>float:left</code> on those elements&mldr; until the page was refreshed or any other part of the website was visited, at which point everything started working again! Firefox, of course, didn&rsquo;t have any issues with it. The problem was that I had a <code>&lt;p></code> element defined as a giant button, then put <code>&lt;a href="">&lt;/a></code> around it because I wanted the entire button to be a link. This, of course, violates the inline/block element mandate I outlined above, despite it being a very natural thing to do - you want the entire button to link somewhere? Put a link around it. It&rsquo;ll <em>almost always</em> work, until it doesn&rsquo;t.</p><p>Trying to write perfectly standards conformant HTML is exceedingly difficult because half the time what you want to do isn&rsquo;t in the standard or is some hack that works almost all the time but not quite. The rest of the time, the &ldquo;standard&rdquo; way of doing things is completely and utterly bizarre, like using <code>margin:auto 0;</code> to center elements. But wait, that only works on horizontal elements! If you want to vertically center an arbitrary element, you have to use <a href=http://www.jakpsatweb.cz/css/css-vertical-center-solution.html>a lot of weird crap</a> just to get it to work on everything. Or you could just use a table. But tables are bad, so you should never use them, even though they actually solve a whole lot of problems new developers have because their non-table solutions are completely unintuitive, because we&rsquo;re all trying to use a <em>document layout engine</em> to make <em>GUIs</em>, for crying out loud.</p><p>Many web developers argue that these problems are going away now that we have a better standard. Sadly, they are just getting started - WebGL will become exceedingly important in the next 5 years, and its standardization is an <em><a href=http://codeflow.org/entries/2013/feb/22/how-to-write-portable-webgl/#why-portable>absolute mess</a></em> almost to the point of it being unusable. Then there&rsquo;s the sordid situation with HTML5 audio and video, which is only just starting to get tolerable. These problems are not going away - they are simply being replaced by different problems. Of course, some stuff actually is becoming easier in HTML - just like a lot of stuff is becoming easier in C++11. So either you ignore both the new C++ features and the new HTML features, or you admit that C++ has become less horrible recently.</p><p>Of course, C++ is still way too easy to write unmaintainable code in, and HTML/CSS code is clearly self-documenting! Except it obviously isn&rsquo;t, given the endless horror stories of terrifying CSS dependencies with random <code>!important</code> keywords flung around in a giant mess that&rsquo;s just as impossible to understand as templatized C++ hell.</p><p>But wait, if you just write proper HTML, it won&rsquo;t have that problem! Well, <em>duh</em>, if you write proper C++, you don&rsquo;t have that problem either. I&rsquo;m sorry, if you are going to hate something, you can&rsquo;t have your cake and eat it too. If C++ supporting features that can be abused to make code impossible to read, like operator overloading, makes it a bad language, then CSS is a bad language, because there is an endless list of obscure, bizarre syntax you can use in your CSS style sheets (like <code>!important</code>) that will make your HTML almost impossible to read. But wait, C++ is also incredibly hard to parse! HTML being easy to parse must be why Opera recently gave up trying to maintain its own HTML layout engine&mldr; Oh wait. Well at least javascript is a consistent, easy to understand language that doesn&rsquo;t have any weird quirks&mldr; <a href="http://www.youtube.com/watch?v=_yZHbh396rc">JUST KIDDING!</a></p><p>So it seems modern technology has succeeded in replacing a very fast, difficult to use, inconsistent language with 3 different, very slow, difficult to use, inconsistent languages.</p><p>Now, if you want to believe that HTML/CSS is still good when used correctly, then all I ask is that you grudgingly admit that, maybe, <em>just maybe</em>, C++ is also good when used correctly. If you still think C++ is an abomination from the depths of cthulu, I hope you can admit that web development therefore must also be an abomination.</p><p>Or we can just keep arguing with each other, because that&rsquo;s always productive.</p></article></div></section><section><div class=dim><aside><h2>Windows Breaks assert() Inside WM_CANCELMODE</h2><ul></ul></aside><article><p>So I have this dll that&rsquo;s doing a bunch of horrible WinAPI calls for me. It&rsquo;s designed to abstract away all the pain and unholy functions feeding on innocent blood. Little did I know that trying to cage WinAPI into a more manageable corner would be my undoing.</p><p>The window is created and assigned a WndProc callback function inside this DLL, as per the various absurd requirements involving how you create windows in Windows. Everything works just fine and dandy until the application window suddenly closes, an error &ldquo;ding&rdquo; is heard, and the program silently crashes without any sort of error message whatsoever.</p><p>What just happened?</p><p>Well, I&rsquo;m afraid you just triggered an assertion. But instead of doing what an assertion is supposed to do, which is immediately crash the program so you know what the fuck just happened, it instead somehow manages to create an infinite callback loop caused by the operating system actually trying to <em>assign the assertion dialog to the window</em>. You know, the same window that was part of an application that&rsquo;s supposed to be in the middle of <em>CRASHING</em>?! This causes another message to be sent, thus causing another assertion to be violated, et cetera until the stack overflows, except it doesn&rsquo;t actually tell you the stack overflows, it just crashes with absolutely no explanation. Even while in a debugger, which is truly amazing.</p><p>Specifically, the assertion triggers a dialog box to be displayed, but this dialog box forcibly sends another WM_CANCELMODE message to the application, apparently completely bypassing the entire point of having a message queue, because the program never gets to go back to it. It&rsquo;s WndProc is simply called with WM_CANCELMODE because <em>fuck you</em>, and so if your assertion fails when you&rsquo;re processing WM_CANCELMODE, it will simply do this over and over and over until the entire window tree collapses in on itself from the intense gravitational pull of <em>astronomical amounts of stupidity</em>.</p><p>The resulting black hole sucks in all nearby sources of sanity, which include some sort of error message about the stack overflowing, or perhaps the <em>actual fucking assertion error</em>, and then explodes, leaving you without a goddamn clue about what just happened. This is usually followed by copious amounts of screaming, then disbelief, then finally assuming the fetal position and praying for forgiveness for attempting to touch the windows API, as the Microsoft gods laugh and drag another developer&rsquo;s soul into their black circle of hell.</p><p>Somehow, every time I touch the windows API, it always ends with me curled into a ball, moaning &ldquo;what the fuck&rdquo; over and over again while crying.</p></article></div></section><section><div class=dim><aside><h2>The Productivity Fallacy</h2><ul></ul></aside><article><p>Technology tends to serve one of two purposes - to make us more efficient at some task, or to entertain us in our resulting free time. However, when we fixate on productivity to the exclusion of everything else, we often forget about the big picture. Perhaps the best example of this are people insisting that real coders need to use Vim to be productive due to it&rsquo;s unmatched text editing powers.</p><p>This is totally absurd. I spend maybe 10% of my time actually writing code, 30% debugging that code, and the remaining 60% trying to solve a problem. Even if I could write all my code instantly, I have only improved my productivity by 10%. I&rsquo;ve found that changing my code patterns to let me catch bugs faster and earlier has had a much more significant impact on my coding speed, because taking a chunk out of 30% has a much greater effect on my overall productivity.</p><p>But what about the 60%? I&rsquo;m sure I could make some of that go away with more powerful visualization tools and intense mental training, but when I hit a problem that&rsquo;s simply really hard to solve, nothing short of a cybernetic implant that makes my brain bigger is going to make a dent in how much time I spend thinking about something unless I want to make a stupid mistake and regret it later.</p><p>The issue that&rsquo;s arising from our hyperproductive tools is that our productivity is beginning to outstrip our ability to think. We are so productive <em>we can&rsquo;t think fast enough to utilize it</em>. Vim may be the most amazing text editor ever, but it doesn&rsquo;t matter because I spend more time thinking than I do actually editing text. We&rsquo;re so focused on making everyone super productive we seem to forget that we are beginning to receive diminishing returns from some of our efforts.</p><p>One consequence of this is that, obviously, we should be focusing on tools to help us think faster. We need to do profiling of people&rsquo;s lives to find the chokepoints and focus on those instead of doing the equivalent of micro-optimizations in C code that&rsquo;s only called every 5 minutes. That, however, does not concern me. What does concern me is the repeated mistakes futurists make when attempting to predict the future of technology.</p><p><a href="http://www.youtube.com/watch?v=a6cNdhOKwi0">This Microsoft video</a> is a superb example of good technology predictions implemented in the worst way possible. The entire video treats human beings as machinery that needs to complete various tasks in the quickest way possible, instead of recognizing that human beings are, in fact, <em>people</em>. Many of the human interactions feel fake because all the interactions are treated simply as tasks that must be completed efficiently, regardless of how beneficial the time saved actually is. Productivity is not important, the way it <em>feels</em> is important.</p><p>Futuristic architecture often makes this same mistake, creating cold, bland environments that, while they do feel futuristic, are not things anyone would want to live or work in. We need environments that feel warm, inviting, and natural. When building futuristic environments, we must be extremely careful about where the future is peeking in, because there is such a thing as too much future in one room.</p><p>We make things look like wood simply because we like how wood looks, not because we need to build anything out of wood anymore. We like trees growing around our houses. We make our lights look like the sun instead of actually being white. We are constantly making arbitrary choices that have nothing to do with productivity and everything to do with making us feel comfortable. Until designers recognize this and stop sucking the life out of everything in an effort to make it more &ldquo;productive&rdquo;, they will inevitably be shunned in favor of slightly less efficient, but more inviting designs.</p><p>Design is an optimization problem that must maximize both productivity <em>and</em> feel, not one or the other. Some people actually like color in their IDEs and webpages that consist of more than flat text, faint lines and whitespace.</p></article></div></section><section><div class=dim><aside><h2>C# to C++ Tutorial - Part 4: Operator Overload</h2><ul></ul></aside><article><p>[ <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-1-basics-of-syntax/>1</a> · <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-2-pointers/>2</a> · <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-3-classes-and/>3</a> · <strong>4</strong> · <span style=color:#aaa>5 · 6 · 7</span> ]</p><p>If you are familiar with C#, you should be familiar with the difference between C#&rsquo;s <code>struct</code> and <code>class</code> declarations. Namely, a <code>struct</code> is a value type and a <code>class</code> is a reference type, meaning that if you pass a struct to a function, its default behavior is for the entire struct to be <strong>copied</strong> into the function&rsquo;s parameter, so any modifications made to it won&rsquo;t affect whatever was passed in. On the flip side, a class is a reference value, so a <strong>reference</strong> is passed into the function, and any changes made to that reference will be reflected in the object that was originally passed into the function.</p><p><pre class=language-csharp><code>// Takes an integer, or a basic value type
public static int add(int v)
{
  v+=3;
  return 4+v;
}

public struct Oppa
{
  public string gangnam;
}

// Takes a struct, or a complex value type
public static Oppa style(Oppa g)
{
  g.gangnam=&#34;notstyle&#34;;
  return g;
}

public class Psy
{
  public int style;
}

// Takes a class, or a reference type
public static void change(Psy psy)
{
  psy.style=5;
}

// Takes an integer, but forces it to be passed by reference instead of by value.
public static int addref(ref int v)
{
  v+=3;
  return 4+v;
}

int a = 0;
int b = add(a);
// a is still 0
// b is now 7

int c = addref(a);
// a is now 3, because it was passed by reference
// c is now 7

Oppa s1;
s1.gangnam=&#34;style&#34;;
Oppa s2 = style(s1);
//s1.gangnam is still &#34;style&#34;
//s2.gangnam is now &#34;notstyle&#34;

Psy psy = new Psy();
psy.style=0;
change(psy);
// psy.style is now 5, because it was passed by reference
</code></pre>C++ also lets you pass in parameters by reference and by value, however it is more explicit about what is happening, so there is no default behavior to know about. If you simply declare the type itself, for example <code>(myclass C, int B)</code>, then it will be passed by value and copied. If, however, you use the reference symbol that we&rsquo;ve used before in variable declarations, it will be passed by reference. This happens no matter what. If a reference is passed into a function that takes a value, it will still have a copy made.</p><p><pre class=language-cpp><code>// Integer passed by value
int add(int v)
{
  v+=3;
  return 4+v;
}

class Psy
{
public:
  int style;
};

// Class passed by value
Psy change(Psy psy)
{
  psy.style=5;
  return psy;
}

// Integer passed by reference
int addref(int&amp; v)
{
  v+=3;
  return 4+v;
}

// Class passed by reference
Psy changeref(Psy&amp; psy)
{
  psy.style=5;
  return psy;
}

int horse = 2;
int korea = add(horse);
// horse is still 2
// korea is now 9

int horse2 = 2;
int korea2 = addref(horse2);
// horse2 is now 5
// korea2 is now 9

Psy psy;
psy.style = 0;
Psy ysp = change(psy);
// psy.style is still 0
// ysp.style is now 5

Psy psy2;
psy2.style = 0;
Psy ysp2 = changeref(psy2);
// psy2.style is now 5
// ysp2.style is also 5
</code></pre>However, in order to copy something, C++ needs to know how to properly copy your class. This gives rise to the <strong>copy constructor</strong>. By default, the compiler will automatically generate a copy constructor for your class that simply invokes all the default copy constructors of whatever member variables you have, just like C#. If, however, your class is holding on to a pointer, then this is going to cause a giant mess when two classes are pointing to the same thing and one of the deletes what it&rsquo;s pointing to! By specifying a copy constructor, we can deal with the pointer properly:</p><p><pre class=language-cpp><code>class myString
{
public:
  // The copy constructor, which copies the string over instead of copying the pointer
  myString(const myString&amp; copy)
  {
    size_t len = strlen(copy._str)+1; //+1 for null terminator
    _str=new char[len];
    memcpy(_str,copy._str,sizeof(char)*len);
  }
  // Normal constructor
  myString(const char* str)
  {
    size_t len = strlen(str);
    _str=new char[len];
    memcpy(_str,str,sizeof(char)*len);
  }
  // Destructor that deallocates our string
  ~myString()
  {
    delete [] _str;
  }

private:
  char* _str;
};

</code></pre>This copy constructor can be invoked manually, but it will simply be implicitly called whenever its needed. Of course, that isn&rsquo;t the only time we need to deal with our rogue pointer that screws things up. What happens when we set our class equal to another class? Remember, <strong>a reference cannot be changed after it is created</strong>. Observe the following behavior:</p><p><pre class=language-cpp><code>int a = 3;
int b = 2;
int&amp; ra = a;
int* pa = &amp;a;

b = a; //b is now 3
a = 0; //b is still 3, but a is now 0
b = ra; // b is now 0
a = 5; // b is still 0 but now a is 5
b = *pa; // b is now 5
b = 8; // b is now 8 but a is still 5

ra = b; //a is now 8! This assigns b&#39;s values to ra, it does NOT change the reference!
ra = 9; //a is now 9, and b is still 8! ra STILL refers to a, and NOTHING can change that.

pa = &amp;b; // Now pa points to to b
a = *pa; // a is now 8, because pointers CAN be changed.
*pa = 7; // Now b is 7, but a is still 8

int*&amp; rpa = pa; //Now we have a reference to a pointer (C++11)
//rpa = 5; // COMPILER ERROR, rpa is a reference to a POINTER
int** ppa = &amp;pa;
//rpa = ppa; // COMPILER ERROR, rpa is a REFERENCE to a pointer, not a pointer to a pointer!
rpa = &amp;a; //now pa points to a again. This does NOT change the reference!
b = *pa; // Now b is 8, the same as a.
</code></pre>So somehow, we have to overload the assignment operator! This brings us to <strong>Operator Overloading</strong>. <a href="http://msdn.microsoft.com/en-us/library/aa288467(v=vs.71).aspx">C# operator overloading</a> works by defining <em>global</em> operator overloads, ones that take a left and a right argument, and are static functions. By default, C++ operator overloading only take the <em>right argument</em>. The left side of the equation is implied to be the class itself. Consequently, <strong>C++ operators are not static</strong>. C++ does have global operators, but they are defined <strong>outside the class</strong>, and the assignment operator isn&rsquo;t allowed as a global operator; you have to define it inside the class. All the overload-able operators are shown below with appropriate declarations:</p><p><pre class=language-cpp><code>class someClass
{
someClass operator =(anything b); // me=other
someClass operator +(anything b); // me+other
someClass operator -(anything b); // me-other
someClass operator +(); // +me
someClass operator -(); // -me (negation)
someClass operator *(anything b); // me*other
someClass operator /(anything b); // me/other
someClass operator %(anything b); // me%other
someClass&amp; operator ++(); // ++me
someClass&amp; operator ++(int); // me++
someClass&amp; operator --(); // --me
someClass&amp; operator --(int); // me--
// All operators can TECHNICALLY return any value whatsoever, but for many of them only certain values make sense.
bool operator ==(anything b); 
bool operator !=(anything b);
bool operator &gt;(anything b);
bool operator &lt;(anything b);
bool operator &gt;=(anything b);
bool operator &lt;=(anything b);
bool operator !(); // !me
// These operators do not usually return someClass, but rather a type specific to what the class does.
anything operator &amp;&amp;(anything b); 
anything operator ||(anything b);

anything operator ~();
anything operator &amp;(anything b);
anything operator |(anything b);
anything operator ^(anything b);
anything operator &lt;&lt;(anything b);
anything operator &gt;&gt;(anything b);
someClass&amp; operator +=(anything b); // Should always return *this;
someClass&amp; operator -=(anything b);
someClass&amp; operator *=(anything b);
someClass&amp; operator /=(anything b);
someClass&amp; operator %=(anything b);
someClass&amp; operator &amp;=(anything b);
someClass&amp; operator |=(anything b);
someClass&amp; operator ^=(anything b);
someClass&amp; operator &lt;&lt;=(anything b);
someClass&amp; operator &gt;&gt;=(anything b);
anything operator [](anything b); // This will almost always return a reference to some internal array type, like myElement&amp;
anything operator *();
anything operator &amp;();
anything* operator -&gt;(); // This has to return a pointer or some other type that has the -&gt; operator defined.

anything operator -&gt;*(anything a);
anything operator ()(anything a1, U a2, ...);
anything operator ,(anything b);
operator otherThing(); // Allows this class to have an implicit conversion to type otherThing
void* operator new(size_t x); // These are called when you write new someClass()
void* operator new[](size_tx); // new someClass[num]
void operator delete(void*x); // delete pointer_to_someClass
void operator delete[](void*x); // delete [] pointer_to_someClass

};

// These are global operators that behave more like C# operators, but must be defined outside of classes, and a few operators do not have global overloads, which is why they are missing from this list. Again, operators can technically take or return any value, but normally you only override these so you can handle some other type being on the left side.
someClass operator +(anything a, someClass b);
someClass operator -(anything a, someClass b);
someClass operator +(someClass a);
someClass operator -(someClass a);
someClass operator *(anything a, someClass b);
someClass operator /(anything a, someClass b);
someClass operator %(anything a, someClass b);
someClass operator ++(someClass a);
someClass operator ++(someClass a, int); // Note the unnamed dummy-parameter int - this differentiates between prefix and suffix increment operators.
someClass operator --(someClass a);
someClass operator --(someClass a, int); // Note the unnamed dummy-parameter int - this differentiates between prefix and suffix decrement operators.

bool operator ==(anything a, someClass b);
bool operator !=(anything a, someClass b);
bool operator &gt;(anything a, someClass b);
bool operator &lt;(anything a, someClass b);
bool operator &gt;=(anything a, someClass b);
bool operator &lt;=(anything a, someClass b);
bool operator !(someClass a);
bool operator &amp;&amp;(anything a, someClass b);
bool operator ||(anything a, someClass b);

someClass operator ~(someClass a);
someClass operator &amp;(anything a, someClass b);
someClass operator |(anything a, someClass b);
someClass operator ^(anything a, someClass b);
someClass operator &lt;&lt;(anything a, someClass b);
someClass operator &gt;&gt;(anything a, someClass b);
someClass operator +=(anything a, someClass b);
someClass operator -=(anything a, someClass b);
someClass operator *=(anything a, someClass b);
someClass operator /=(anything a, someClass b);
someClass operator %=(anything a, someClass b);
someClass operator &amp;=(anything a, someClass b);
someClass operator |=(anything a, someClass b);
someClass operator ^=(anything a, someClass b);
someClass operator &lt;&lt;=(anything a, someClass b);
someClass operator &gt;&gt;=(anything a, someClass b);
someClass operator *(someClass a);
someClass operator &amp;(someClass a);

someClass operator -&gt;*(anything a, someClass b);
someClass operator ,(anything a, someClass b);
void* operator new(size_t x);
void* operator new[](size_t x);
void operator delete(void* x);
void operator delete[](void*x);
</code></pre>We can see that the assignment operator mimics the arguments of our copy constructor. For the most part, it does the exact same thing; the only difference is that existing values must be destroyed, an operation that should mostly mimic the destructor. We extend our previous class to have an assignment operator accordingly:</p><p><pre class=language-cpp><code>class myString
{
public:
  // The copy constructor, which copies the string over instead of copying the pointer
  myString(const myString&amp; copy)
  {
    size_t len = strlen(copy._str)+1; //+1 for null terminator
    _str=new char[len];
    memcpy(_str,copy._str,sizeof(char)*len);
  }
  // Normal constructor
  myString(const char* str)
  {
    size_t len = strlen(str);
    _str=new char[len];
    memcpy(_str,str,sizeof(char)*len);
  }
  // Destructor that deallocates our string
  ~myString()
  {
    delete [] _str;
  }

  // Assignment operator, does the same thing the copy constructor does, but also mimics the destructor by deleting _str. NOTE: It is considered bad practice to call the destructor directly. Use a Clear() method or something equivalent instead.
  myString&amp; operator=(const myString&amp; right)
  {
    delete [] _str;
    size_t len = strlen(right._str)+1; //+1 for null terminator
    _str=new char[len];
    memcpy(_str,right._str,sizeof(char)*len);
  }

private:
  char* _str;
};
</code></pre>These operations take an instance of the class and copy it&rsquo;s values to our instance. Consequently, these are known as <em>copy semantics</em>. If this was 1998, we&rsquo;d stop here, because for a long time, C++ only had copy semantics. Either you passed around references to objects, or you copied them. You could also pass around pointers to objects, but remember that pointers are value types just like integers and floats, so you are really just copying them around too. In fact, until recently, you were <em>not allowed to have references to pointers</em>. Pointers were the one data type that had to be passed by value. Provided you are using a C++0x-compliant compiler, <strong>this is no longer true</strong>, as you may remember from our <a href>first examples</a>. The new standard <a href=en.wikipedia.org/wiki/C%2B%2B11>released in 2011</a> allows references to pointers, and introduces <em>move semantics</em>.</p><p>Move semantics are designed to solve the following problem. If we have a series of dynamic string objects being concatenated, with normal copy constructors we run into a serious problem:</p><p><pre class=language-cpp><code>std::string result = std::string(&#34;Oppa&#34;) + std::string(&#34; Gangnam&#34;) + std::string(&#34; Style&#34;) + std::string(&#34; by&#34;) + std::string(&#34; Psy&#34;);
// This is evaluated by first creating a new string object with its own memory allocation, then deallocating both &#34; by&#34; and &#34; Psy&#34; after copying their contents into the new one
//std::string result = std::string(&#34;Oppa&#34;) + std::string(&#34; Gangnam&#34;) + std::string(&#34; Style&#34;) + std::string(&#34; by Psy&#34;);
// Then another new object is made and &#34; by Psy&#34; and &#34; Style&#34; are deallocated
//std::string result = std::string(&#34;Oppa&#34;) + std::string(&#34; Gangnam&#34;) + std::string(&#34; Style by Psy&#34;);
// And so on and so forth
//std::string result = std::string(&#34;Oppa&#34;) + std::string(&#34; Gangnam Style by Psy&#34;);
//std::string result = std::string(&#34;Oppa Gangnam Style by Psy&#34;);
// So just to add 5 strings together, we&#39;ve had to allocate room for 5 additional strings in the middle of it, 4 of which are then simply deallocated!
</code></pre>This is terribly inefficient; it would be much more efficient if we could utilize the temporary objects that are going to be destroyed anyway instead of reallocating a bunch of memory over and over again only to delete it immediately afterwards. This is where move semantics come in to play. First, we need to define a &ldquo;temporary&rdquo; object as one whose scope is entirely contained on <em>the right side of an expression</em>. That is to say, given a single assignment statement <code>a=b</code>, if an object is both created and destroyed inside <code>b</code>, then it is considered temporary. Because of this, these temporary values are also called <em>rvalues</em>, short for &ldquo;right values&rdquo;. C++0x introduces the syntax <code>variable&&</code> to designate an rvalue. This is how you declare a move constructor:</p><p><pre class=language-cpp><code>class myString
{
public:
  // The copy constructor, which copies the string over instead of copying the pointer
  myString(const myString&amp; copy)
  {
    size_t len = strlen(copy._str)+1; //+1 for null terminator
    _str=new char[len];
    memcpy(_str,copy._str,sizeof(char)*len);
  }
  // Move Constructor
  myString(myString&amp;&amp; mov)
  {
    _str = mov._str;
    mov._str=NULL;
  }
  // Normal constructor
  myString(const char* str)
  {
    size_t len = strlen(str);
    _str=new char[len];
    memcpy(_str,str,sizeof(char)*len);
  }
  // Destructor that deallocates our string
  ~myString()
  {
    if(_str!=NULL) // Make sure we only delete _str if it isn&#39;t NULL!
      delete [] _str;
  }

  // Assignment operator, does the same thing the copy constructor does, but also mimics the destructor by deleting _str. NOTE: It is considered bad practice to call the destructor directly. Use a Clear() method or something equivalent instead.
  myString&amp; operator=(const myString&amp; right)
  {
    delete [] _str;
    size_t len = strlen(right._str)+1; //+1 for null terminator
    _str=new char[len];
    memcpy(_str,right._str,sizeof(char)*len);
    return *this;
  }

private:
  char* _str;
};</code></pre><strong>NOTE: Observe that our destructor functionality was changed! Now that _str can be NULL, we have to check for that before deleting the object.</strong></p><p>The idea behind a move constructor is that, instead of <em>copying</em> the values into our object, we <em>move</em> them into our object, setting the source to some <code>NULL</code> value. Notice that this can only work for pointers, or objects containing pointers. Integers, floats, and other similar types can&rsquo;t really be &ldquo;moved&rdquo;, so instead their values are simply copied over. Consequently, move semantics is only beneficial for types like strings that involve dynamic memory allocation. However, because we must set the source pointers to 0, that means we can&rsquo;t use <code>const myString&&</code>, because then we wouldn&rsquo;t be able to modify the source pointers! This is why a move constructor is declared without a const modifier, which makes sense, since we intend to modify the object.</p><p>But wait, just like a copy constructor has an assignment copy operator, a move constructor has an equivalent assignment move operator. Just like the copy assignment, the move operator behaves exactly like the move constructor, but must destroy the existing object beforehand. The assignment move operator is declared like this:</p><pre class=language-cpp><code>myString&amp; operator=(myString&amp;&amp; right)
  {
    delete [] _str;
    _str=right._str;
    right._str=0;
    return *this;
  }
</code></pre><p>Move semantics can be used for some interesting things, like unique pointers, that <em>only</em> have move semantics - by disabling the copy constructor, you can create an object that is impossible to copy, and can therefore only be moved, which guarantees that there will only be one copy of its contents in existence. <code>std::unique_ptr</code> is an implementation of this provided in C++0x. Note that if a data structure requires copy semantics, <code>std::unique_ptr</code> will throw a compiler error, instead of simply mysteriously failing like the deprecated <code>std::autoptr</code>.</p><p>There is an important detail when you are using inheritance or objects with move semantics:</p><p><pre class=language-cpp><code>class Substring : myString
{
  Substring(Substring&amp;&amp; mov) : myString(std::move(mov))
  {
    _sub = std::move(mov._sub);
  }

  Substring&amp; operator=(Substring&amp;&amp; right)
  {
    myString::operator=(std::move(right));
    _sub = std::move(mov._sub);
    return *this;
  }

  myString _sub;
};
</code></pre>Here we are using <code>std::move()</code>, which takes a variable (that is either an rvalue or a normal reference) and returns an rvalue for that variable. This is because rvalues <em>stop being rvalues</em> the instant they are passed into a different function, which makes sense, since they are no longer on the right-hand side anymore. Consequently, if we were to pass <code>mov</code> above into our base class, it would trigger the <em>copy constructor</em>, because <code>mov</code> would be treated as <code>const Substring&</code>, instead of <code>Substring&&</code>. Using <code>std::move</code> lets us pass it in as <code>Substring&&</code> and properly trigger the move semantics. As you can see in the example, you must use <code>std::move</code> when moving any complex object, using base class constructors, or base class assignment operators. Note that <code>std::move</code> allows you to force an object to be moved to another object regardless of whether or not its actually an rvalue. This would be particularly useful for moving around <code>std::unique_ptr</code> objects.</p><p>There&rsquo;s some other weird things you can do with move semantics. This most interesting part is the strange behavior of <code>&&</code> when it is appended to existing references.</p><ul><li><code>A& &</code> becomes <code>A&</code></li><li><strong><code>A& &&</code> becomes <code>A&</code></strong></li><li><code>A&& &</code> becomes <code>A&</code></li><li><strong><code>A&& &&</code> becomes <code>A&&</code></strong></li></ul><p>By taking advantage of the second and fourth lines, we can perform <em>perfect forwarding</em>. Perfect forwarding allows us to pass an argument as either a normal reference (<code>A&</code>) or an rvalue (<code>A&&</code>) and then <em>forward it</em> into another function, preserving its status as an rvalue or a normal reference, including whether or not it&rsquo;s <code>const A&</code> or <code>const A&&</code>. Perfect forwarding can be implemented like so:</p><p><pre class=language-cpp><code>template&lt;typename U&gt;
void Set(U &amp;&amp; other)
{
  _str=std::forward&lt;U&gt;(other);
}
</code></pre>Notice that this allows us to assign our data object using either the copy assignment, or the move assignment operator, by using <code>std::forward&lt;U>()</code>, which transforms our reference into either an rvalue if it was an rvalue, or a normal reference if it was a normal reference, much like <code>std::move()</code> transforms everything into an rvalue. However, this requires a template, which may not always be correctly inferred. A more robust implementation uses two separate functions forwarding their parameters into a helper function:</p><p><pre class=language-cpp><code>class myString
{
public:
  // The copy constructor, which copies the string over instead of copying the pointer
  myString(const myString&amp; copy)
  {
    size_t len = strlen(copy._str)+1; //+1 for null terminator
    _str=new char[len];
    memcpy(_str,copy._str,sizeof(char)*len);
  }
  // Move Constructor
  myString(myString&amp;&amp; mov)
  {
    _str = mov._str;
    mov._str=NULL;
  }
  // Normal constructor
  myString(const char* str)
  {
    size_t len = strlen(str);
    _str=new char[len];
    memcpy(_str,str,sizeof(char)*len);
  }
  // Destructor that deallocates our string
  ~myString()
  {
    if(_str!=NULL) // Make sure we only delete _str if it isn&#39;t NULL!
      delete [] _str;
  }
  void Set(myString&amp;&amp; str)
  {
    _set&lt;myString&amp;&amp;&gt;(std::move(str));
  }
  void Set(const myString&amp; str)
  {
    _set&lt;const myString&amp;&gt;(str);
  }


  // Assignment operator, does the same thing the copy constructor does, but also mimics the destructor by deleting _str. NOTE: It is considered bad practice to call the destructor directly. Use a Clear() method or something equivalent instead.
  myString&amp; operator=(const myString&amp; right)
  {
    delete [] _str;
    size_t len = strlen(right._str)+1; //+1 for null terminator
    _str=new char[len];
    memcpy(_str,right._str,sizeof(char)*len);
    return *this;
  }

private:
  template&lt;typename U&gt;
  void _set(U &amp;&amp; other)
  {
    _str=std::forward&lt;U&gt;(other);
  }

  char* _str;
};
</code></pre>Notice the use of <code>std::move()</code> to transfer the rvalue correctly, followed by <code>std::forward&lt;U>()</code> to forward the parameter. By using this, we avoid redundant code, but can still build move-aware data structures that efficiently assign values with relative ease. Now, its on to <a href=#>Part 5: Delegated Llamas</a>! Or, well, delegates, function pointers, and lambdas. Possibly involving llamas. Maybe.</p></article></div></section><section><div class=dim><aside><h2>7 Problems Raytracing Doesn't Solve</h2><ul></ul></aside><article><p>I see a lot of people get excited about extreme concurrency in modern hardware bringing us closer to the magical holy grail of <em>raytracing</em>. It seems that everyone thinks that once we have raytracing, we can fully simulate entire digital worlds, everything will be photorealistic, and graphics will become a &ldquo;solved problem&rdquo;. This simply isn&rsquo;t true, and in fact highlights several fundamental misconceptions about the problems faced by modern games and other interactive media.</p><p>For those unfamiliar with the term, <em>raytracing</em> is the process of rendering a 3D scene by tracing the path of a beam of light after it is emitted from a light source, calculating its properties as it bounces off various objects in the world until it finally hits the virtual camera. At least, you hope it hits the camera. You see, to be perfectly accurate, you have to cast a bajillion rays of light out from the light sources and then see which ones end up hitting the camera at some point. This is obviously a problem, because most of the rays don&rsquo;t actually hit the camera, and are simply wasted. Because this brute force method is so incredibly inefficient, many complex algorithms (such as <a href=http://en.wikipedia.org/wiki/Photon_mapping>photon-mapping</a> and <a href=http://en.wikipedia.org/wiki/Metropolis_light_transport>Metropolis light transport</a>) have been developed to yield approximations that are thousands of times more efficient. These techniques are almost always focused on attempting to find paths from the light source to the camera, so rays can be cast in the reverse direction. Some early approximations actually cast rays out from the camera until they hit an object, then calculated the lighting information from the distance and angle, disregarding other objects in the scene. While highly efficient, this method produced extremely inaccurate results.</p><p>It is with a certain irony that raytracing is touted as being a precise, super-accurate rendering method when all raytracing is actually done via approximations in the first place. Pixar uses photon-mapping for its movies. Most raytracers operate on stochastic sampling approximations. We can already do raytracing in realtime, if we get approximate enough, it just looks <a href="http://www.youtube.com/watch?v=h5mRRElXy-w&amp;t=0m50s">boring</a> and is extremely limited. Graphics development doesn&rsquo;t just stop when someone develops realtime raytracing, because there will always be room for a better approximation.</p><h5 id=1-photorealism>1. Photorealism</h5><p>The meaning of <em>photorealism</em> is difficult to pin down, in part because the term is inherently subjective. If you define photorealism as being able to render a virtual scene such that it precisely matches a photo, then it is almost impossible to achieve in any sort of natural environment where the slightest wind can push a tree branch out of alignment.</p><p>This quickly gives rise to defining photorealism as rendering a virtual scene such that it is indistinguishable from a photograph of a similar scene, even if they aren&rsquo;t exactly the same. This, however, raises the issue of just how indistinguishable it needs to be. This seems like a bizarre concept, but there are different degrees of &ldquo;indistinguishable&rdquo; due to the differences between people&rsquo;s observational capacities. Many people will never notice a slightly misaligned shadow or a reflection that&rsquo;s a tad too bright. For others, they will stand out like sore thumbs and completely destroy their suspension of disbelief.</p><p>We have yet another problem in that the entire concept of &ldquo;photorealism&rdquo; has nothing to do with how humans see the world in the first place. Photos are inherently linear, while human experience a much more dynamic, log-based lighting scale. This gives rise to <a href=http://en.wikipedia.org/wiki/High_dynamic_range_imaging>HDR photography</a>, which actually has almost nothing to do with the HDR implemented in games. Games simply change the brightness of the entire scene, instead of combining the brightness of multiple exposures to brighten some areas and darken others in the <a href=http://en.wikipedia.org/wiki/File:BrnoSunsetHDRExampleByIgor.jpg>same photo</a>. If all photos are not created equal, then exactly which photo are we talking about when we say &ldquo;photorealistic&rdquo;?</p><h5 id=2-complexity>2. Complexity</h5><p>Raytracing is often cited as allowing an order of magnitude more detail in models by being able to efficiently process many more polygons. This is only sort of true in that raytracing is not subject to the same computational constraints that rasterization is. Rasterization must render every single triangle in the scene, whereas raytracing is only interested in whether or not a ray hits a triangle. Unfortunately, it still has to navigate through the scene representation. Even if a raytracer could handle a scene with a billion polygons efficiently, this raises completely unrelated problems involving RAM access times and cache pollution that suddenly become actual performance bottlenecks instead of micro-optimizations.</p><p>In addition, raytracing approximation algorithms almost always take advantage of rays that degrade quickly, such that they can only bounce 10-15 times before becoming irrelevant. This is fine and dandy for walking around in a city or a forest, but what about a kitchen? Even though raytracing is much better at handling reflections accurately, highly reflective materials cripple the raytracer, because now rays are bouncing hundreds of times off a myriad of surfaces instead of just 10. If not handled properly, it can absolutely devastate performance, which is catastrophic for game engines that must maintain constant render times.</p><h5 id=3-scale>3. Scale</h5><p>How do you raytrace stars? Do you simply wrap a sphere around the sky and give it a &ldquo;star&rdquo; material? Do you make them all point sources infinitely far away? How does this work in a space game, where half the stars you see can actually be visited, and the other half are entire galaxies? How do you accurately simulate an entire solar system down to the surface of a planet, as the <a href=http://kerbalspaceprogram.com/forum/entry.php/54-Scaled-Space-Now-with-100-more-Floating-Origin%21>Kerbal Space Program developers</a> had to? Trying to figure out how to represent that kind of information in a meaningful form with only 64 bits of precision, if you are lucky, is a problem completely separate from raytracing, yet of increasingly relevant concern as games continue to expand their horizons more and more. How do we simulate an entire galaxy? How can we maintain meaningful precision when faced with astronomical scales, and how does this factor in to our rendering pipeline? These are problems that arise in any rendering pipeline, regardless of what techniques it uses, due to fundamental limitations in our representations of numbers.</p><h5 id=4-materials>4. Materials</h5><p>Do you know what methane clouds look like? What about writing an <a href=http://en.wikipedia.org/wiki/File:Aerogel_nasa.jpg>aerogel</a> shader? Raytracing, by itself, doesn&rsquo;t simply figure out how a given material works, you have to tell it how each material behaves, and its accuracy is wholly dependent on how accurate your description of the material is. This isn&rsquo;t easy, either, it requires advanced mathematical models and heaps of data collection. In many places we&rsquo;re actually <a href=http://blog.selfshadow.com/publications/s2012-shading-course/>still trying to figure out</a> how to build physically correct material equations in the first place. Did you know that Dreamworks had to rewrite part of their cloud shader<sup><a href=#r1>1</a></sup> for How To Train Your Dragon? It turns out that getting clouds to look good when your character is flying directly beneath them with a hand raised is <em>really hard</em>.</p><p>This is just for <em>common lighting phenomena!</em> How are you going to write shaders for things like pools of magic water and birefringent <a href=http://en.wikipedia.org/wiki/Birefringence>calcite crystals</a>? How about trying to accurately simulate <a href=http://en.wikipedia.org/wiki/File:CircularPolarizer.jpg>circular polarizers</a> when most raytracers don&rsquo;t even know what polarization is? Does being photorealistic require you to simulate the <a href=http://en.wikipedia.org/wiki/Tyndall_effect>Tyndall Effect</a> for caustics in <a href=http://en.wikipedia.org/wiki/File:Why_is_the_sky_blue.jpg>crystals</a> and <a href=http://en.wikipedia.org/wiki/File:WaterAndFlourSuspensionLiquid.jpg>particulate matter</a>? There are <em>so many tiny little details</em> all around us that affect everything from the <a href=http://en.wikipedia.org/wiki/Tyndall_scattering#Blue_irises>color of our iris</a> to the <a href=http://en.wikipedia.org/wiki/Rainbow#Number_of_colours_in_spectrum_or_rainbow>creation of rainbows</a>. Just how much does our raytracer need to simulate in order to be photorealistic?</p><h5 id=5-physics>5. Physics</h5><p>What if we ignored the first four problems and simply assumed we had managed to make a perfect, magical photorealistic raytracer. Congratulations, you&rsquo;ve managed to co-opt the entirety of your CPU for the task of rendering a static 3D scene, leaving nothing left for the physics. All we&rsquo;ve managed to accomplish is taking the &ldquo;interactive&rdquo; out of &ldquo;interactive media&rdquo;. Being able to influence the world around us is a key ingredient to immersion in games, and this requires more and more accurate physics, which are arguably just as difficult to calculate as raytracing is. The most advanced real-time physics engine to-date is the <a href=http://www.gameranx.com/updates/id/707/article/mind-blowing-physics-engine-demonstration/>Lagoa Multiphysics</a>, and it can only just <em>barely</em> simulate a tiny scene in a well-controlled environment before it completely decimates a modern CPU. This is without any complex rendering at all. Now try doing that for a scene with a radius of several miles. Oh, and remember our <a href=#h3>issue with scaling</a>? This applies to physics too! Except with physics, its an <em>order of magnitude even more difficult</em>.</p><h5 id=6-content>6. Content</h5><p>As many developers have been discovering, procedural generation is not magic pixie dust you can sprinkle on problems to make them go away. Yet, without advances in content generation, we are forced to hire armies of artists to create the absurd amounts of detail required by modern games. Raytracing doesn&rsquo;t solve this problem, it makes it <em>worse</em>. In any given square mile of a human settlement, there are <em>billions</em> of individual objects, ranging from pine cones, to rocks, to TV sets, to <em>crumbs</em>, all of which technically have physics, and must be kept track of, and rendered, and even more importantly, <em>modeled</em>.</p><p>Despite <a href=http://en.wikipedia.org/wiki/Spore>multiple attempts</a> at leveraging procedural generation, the content problem has simply refused to go away. Until we can effectively harness the power of procedural generation, augmented artistic tools, and automatic design morphing, the advent of fully photorealistic raytracing will be useless. The best graphics engine in the world is nothing without art.</p><h5 id=7-ai>7. AI</h5><p><div style=margin-left:3em>&lt;Patrician|Away> what does your robot do, sam
&lt;bovril> it collects data about the surrounding environment, then discards it and drives into walls
— <i>Bash.org quote [#240849](http://bash.org/?240849)</i></div>Of course, while we&rsquo;re busy desperately trying to raytrace supercomplex scenes with advanced physics, we haven&rsquo;t even left any CPU time to calculate the AI! The AI in games is so consistently terrible its turned into <a href=http://tvtropes.org/pmwiki/pmwiki.php/Main/ArtificialStupidity>its own trope</a>. The game industry spends all its computational time trying to render a scene, leaving almost nothing left for the AI routines, forcing them to rely on <a href=http://en.wikipedia.org/wiki/A*_search_algorithm>techniques from 1968</a>. Think about that - we are approaching the point where AI in games comes down to a 50-year old technique that was considered hopelessly outdated before I was even born. Oh, and I should also point out that Graphics, Physics, Art, and AI are all completely separate fields with fundamentally different requirements that all have to work together in a coherent manner just so you can shoot headshots in Call of Duty 22.</p><p>I know that raytracing is exciting, sometimes simply as a demonstration of raw computational power. But it always disheartens me when people fantasize about playing amazingly detailed games indistinguishable from real life when that simply isn&rsquo;t going to happen, even with the inevitable development<sup>2</sup> of realtime raytracing. By the time it becomes commercially viable, it will simply be yet another incremental step in our eternal quest for infinite realism. It is an important step, and one we should strive for, but it alone is not sufficient to spark a revolution.</p><span style=font-size:80%><br><sup><a name=r1>1</a></sup> Found on the special features section of the How To Train Your Dragon DVD.<br><sup>2</sup> Disclaimer: I've been trying to develop an efficient raytracing algorithm for ages and haven't had much luck. <a href=http://www.tml.tkk.fi/~samuli/publications/lehtinen2012siggraph_paper.pdf>These guys are faring much better</a>.</span></article></div></section><section><div class=dim><aside><h2>Analyzing XKCD: Click and Drag</h2><ul></ul></aside><article><p>Today, xkcd <a href=http://xkcd.com/1110/>featured a comic</a> with a <em>comically</em> large image that is navigated by clicking and dragging. In the interests of SCIENCE (and possibly accidentally DDoSing Randall&rsquo;s image server - sorry!), I created a static HTML file of the <strong><a href=http://blackspherestudios.com/storage/xkcd_huge_static.html>entire composite image</a></strong>.<sup>1</sup></p><p>The collage is made up of 225 images<sup>2</sup> that stretch out over a total image area 79872 pixels high and 165888 pixels wide. The images take up 5.52 MB of space and are named with a simple naming scheme <code>"ydxd.png"</code> where d represents a cardinal direction appropriate for the axis (n for north, s for south on the y axis and e for east, w for west on the x axis) along with the tile coordinate number; for example, <code>"1n1e.png"</code>. Tiles are 2048x2048 png images with an average size of 24.53 KB. If you were to try and represent this as a single, uncompressed 32-bit 79872x165888 image file, it would take up 52.99 GB of space.</p><p>Assuming a human&rsquo;s average height is 1.8 meters, that would give this image a scale of about 1 meter per 22 pixels. That means the total composite image is approximately 3.63 kilometers high and 7.54 kilometers wide. It would take an average human 1.67 hours to walk from one end of the image to the other. Note that the characters at the far left say they&rsquo;ve been walking for 2 miles - they are 67584 pixels from the starting point, which translates to 3.072 km or ~1.9 miles, so this seems to indicate my rough estimates here are reasonably accurate.</p><p>If Randall spent, on average, one hour drawing each frame, it would take him 9.375 days of constant, nonstop work to finish this. If he instead spent an average of 10 minutes per frame, it would take ~37.5 hours, or almost an entire 40-hour work week.</p><p>Basically I&rsquo;m saying Randall Munroe is fucking insane.</p><span style=font-size:80%><sup>1</sup> If you are on firefox or chrome, right-clicking and selecting "Save as" will download the HTML file along with all 225 images into a separate folder.
<sup>2</sup> There are actually 3159 possible images (39 x 81), but all-white and all-black images are not included, instead being replaced by either the default white background or a massive black &lt;div> representing the ground, located 28672 pixels from the top of the image, with a height of 51200.</span></article></div></section><section><div class=dim><aside><h2>Coordinate Systems And Cascading Stupidity</h2><ul></ul></aside><article><p>Today I learned that there are way too many coordinate systems, and that I&rsquo;m an idiot (but that was already well-established). I have also learned to not trust graphics tutorials, but the reasons for that won&rsquo;t become apparent until the end of this article.</p><p>There are <a href=http://en.wikipedia.org/wiki/Cartesian_coordinate_system#Orientation_and_handedness>two types of coordinate systems</a>: left-handed and right-handed coordinate systems. By convention, most everyone in math and science uses right-handed coordinate systems with positive x going to the right, positive y going up, and positive z coming out of the screen. A left-handed coordinate system is the same, but positive z instead points into the screen. Of course, there are many other possible coordinate system configurations, each either being right or left-handed; some modern CAD packages have y pointing into the screen and z pointing up, and screen-space in graphics traditionally has y pointing down and z pointing into the screen.</p><p>If you start digging through DirectX and OpenGL, the handedness of the coordinate systems being used are ill-defined due to its reliance on various perspective transforms. Consequently, while DirectX traditionally uses a left-handed coordinate system and OpenGL uses a right-handed coordinate system, you can simply use <code>D3DPerspectiveMatrixRH</code> to give DirectX a right-handed coordinate system, and openGL actually uses a left-handed coordinate system by default on its shader pipeline - but all of these are entirely dependent on the handedness of the projection matrices involved. So, technically the coordinate system is whichever one you choose, but unlike the rest of the world, computer graphics has no real standard on which coordinate system to use, and so its just a giant mess of various coordinate systems all over the place, which means you don&rsquo;t know what handedness a given function is for until things start getting funky.</p><p>I discovered all this, because today I found out that, for the past 6 or so years (the entire time my graphics engine has ever existed in any shape or form), it has been <strong>rotating everything backwards</strong>. I didn&rsquo;t notice.</p><p>This happened due to a number of unfortunate coincidences. For many years, I simply didn&rsquo;t notice because I didn&rsquo;t know what direction the sign of a given rotation was supposed to rotate in, and even if I did I would have assumed this to be the default for graphics for some strange reason (there are a <em>lot</em> of weird things in graphics). The first hint was when I was integrating with Box2D and I had to reverse the rotation of its bodies to match up with my images. This did trigger an investigation, but I mistakenly concluded that it was <em>Box2D</em> that had it wrong, not me, because I was using <code>atan2</code> to check coordinates, and I was passing them in as <code>atan2(v.x,v.y)</code>. The problem is that <code>atan2</code> is defined as <code>float atan2(float y, float x)</code>, which means my coordinates were reversed and I was getting nonsense angles.</p><p>Now, here you have to understand that I was currently using a standard left-handed coordinate system, with y pointing up, x pointing right and z into the screen. The thing is, I wanted a coordinate system where y pointed <em>down</em>, and so I did as a tutorial instructed me to and reversed all of my y coordinates on the low-level drawing functions.</p><p>So, when <code>atan2(x,y)</code> gave me bad results, I mistakenly thought &ldquo;Oh, i forgot to reverse the y coordinate!&rdquo; Suddenly <code>atan2(x,-y)</code> was giving me angles that matched what my images were doing. The thing is, if you switch x and y and negate y, <code>atan2(x,-y)==-atan2(y,x)</code>. One mistake had been incorrectly validated by yet another mistake, caused by <em>yet another mistake!</em></p><p>You see, by inverting those y coordinates, I was accidentally reversing the result of my rotation matrices, which caused them to rotate everything backwards. This was further complicated by how the camera rotates things - if your camera is fixed, how do you make it appear that it is rotating? You rotate everything else <em>in the opposite direction!</em> Hence even though my camera was rotating backwards despite looking like it was rotating forwards, it was actually being rotated the right way for the wrong reason.</p><p>While I initially thought the fix for this would require some crazy coordinate system juggling, the actual solution was fairly simple. The fact was, a coordinate system with z pointing into the screen and y pointing down <em>is still right-handed</em>, which means it should play nicely with rotations from a traditional right-handed system. Since the handedness of a coordinate system is largely determined by the perspective matrix, reversing y-coordinates in the drawing functions was actually reversing them too late in the pipeline. Hence, because I used <code>D3DXMatrixPerspectiveLH</code>, I had a left-handed coordinate system, and my rotations ended up being reversed. <code>D3DXMatrixPerspectiveRH</code> negates the z-coordinate to switch the handedness of the coordinate system, but I like positive z pointing into the screen, so I instead hacked the left-handed perspective matrix itself and negated the y-scaling parameter in cell [2,2], then undid all the y-coordinate inversion insanity that had been inside my drawing functions (you could also negate the y coordinate in any world transform matrix sufficiently early in the pipeline by specifying a negative y scaling in [2,2]). Suddenly everything was consistent, and rotations were happening in the right direction again. Now the Camera rotation actually required the negative rotation, as one would expect, and I still got to use a coordinate system with y pointing down. Unfortunately it also reversed several rotation operations throughout the engine, some of which were functions that had been returning the wrong value <em>this whole time</em> so as to match up with the incorrect rotation of the engine - something that will give me nightmares for weeks, probably involving a crazed rabbit hitting me over the head with a carrot screaming <em><strong>&ldquo;STUPID STUPID STUPID STUPID!&rdquo;</strong></em></p><p>What&rsquo;s truly terrifying that all of this was indirectly caused by reversing the y coordinates in the first place. Had I instead flipped them in the perspective matrix itself (or otherwise properly transformed the coordinate system), I never would have had to deal with negating y coordinates, I never would have mistaken <code>atan2(x,-y)</code> as being valid, and I never would have had rotational issues in the first place.</p><p>All because of that one stupid tutorial.</p><p><sub>P.S. the moral of the story isn&rsquo;t that tutorials are bad, it&rsquo;s that you shouldn&rsquo;t be a stupid dumbass and not write unit tests or look at function definitions.</sub></p></article></div></section><section><div class=dim><aside><h2>How Joysticks Ruined My Graphics Engine</h2><ul></ul></aside><article><p>It&rsquo;s almost a tradition.</p><p>Every time my graphics engine has been stuck in maintenence mode for 6 months, I&rsquo;ll suddenly realize I need to push out an update or implement some new feature. I then realize that I haven&rsquo;t actually paid attention to any of my testing programs, or their speed, in months. This is followed by panic, as I discover my engine running at half speed, or worse. Having made an infinite number of tiny tweaks that all could have caused the problem, I am often thrown into temporary despair only to almost immediately find some incredibly stupid mistake that was causing it. One time it was because I left the profiler on. Another time it was caused by calling the Render function twice. I&rsquo;m gearing up to release the <a href=http://blackspherestudios.com>first public beta</a> of my graphics engine, and this time is no different.</p><p>I find an old backup distribution of my graphics engine and run the tests, and my engine is running at 1100 FPS instead of 3000 or 4000 like it should be. Even the stress test is going at only ~110 FPS instead of 130 FPS. The strange part was that for the lightweight tests, it seemed to be hitting a wall at about 1100 FPS, whereas normally it hits a wall around 4000-5000 due to CPU⇒GPU bottlenecks. This is usually caused by some kind of debugging, so I thought I&rsquo;d left the profiler on again, but no, it was off. After stepping through the rendering pipeline and finding nothing, I knew I had no chance of just guessing what the problem was, so I turned on the profiler and checked the results. Everything seemed relatively normal except-<pre class=language-cil><code>PlaneShader::cEngine::_callpresent       1.0    144.905 us   19%        145 us   19%
PlaneShader::cEngine::Update             1.0     12.334 us    2%        561 us   72%
PlaneShader::cEngine::FlushMessages    1.0    546.079 us   70%        549 us   71%
</code></pre><strong><em>What the FUCK?!</em></strong> Why is <em>70%</em> of my time being spent in <code>FlushMessages()</code>? All that does is process window messages! It shouldn&rsquo;t take any time at all, and here it is taking longer to process messages than it does to render an entire frame!<pre class=language-cpp><code>bool cEngine::FlushMessages()
{
PROFILE_FUNC();
_exactmousecalc();
//windows stuff
MSG msg;

while(PeekMessageW(&amp;msg, NULL, 0, 0, PM_REMOVE))
{
TranslateMessage(&amp;msg);
DispatchMessageW(&amp;msg);

    if(msg.message == WM_QUIT)
    {
      _quit = true;
      return false;
    }

}

_joyupdateall();
return !_quit; //function returns opposite of quit
}
</code></pre>Bringing up the function, there don&rsquo;t seem to be many opportunities for it to fail. I go ahead and comment out <code>_exactmousecalc()</code> and <code>_joyupdateall()</code>, wondering if, perhaps, something in the joystick function was messing up? Lo and behold, my speeds are back to normal! After re-inserting the exact mouse calculation, it is, in fact, <code>_joyupdateall()</code> causing the problem. This is the start of the function:<pre class=language-cpp><code>void cInputManager::_joyupdateall()
{
JOYINFOEX info;
info.dwSize=sizeof(JOYINFOEX);
info.dwFlags= \[ Clipped... \];

for(unsigned short i = 0; i &lt; _maxjoy; ++i)
{
if(joyGetPosEx(i,&amp;info)==JOYERR_NOERROR)
{
if(_allbuttons\[i\]!=info.dwButtons)
{
</code></pre>Well, shit, there isn&rsquo;t really any possible explanation here other than <em>something</em> going wrong with <code>joyGetPosEx</code>. It turns out that calling <code>joyGetPosEx</code> when there isn&rsquo;t a joystick plugged in takes a whopping 34.13 µs (microseconds) on average, which is almost as long as it takes me to render a single frame (43.868 µs). There&rsquo;s probably a good reason for this, but evidently it is not good practice to call it unnecessarily. Fixing this was relatively easy - just force an explicit call to look for active joystick inputs and only poll those, but its still one of the weirdest performance bottlenecks I&rsquo;ve come up against.</p><p>Of course, if you aren&rsquo;t developing a graphics engine, consider that 1/60 of a second is 16666 µs - 550 µs leaves a lot of breathing room for a game to work with, but a graphics engine must not force <em>any</em> unnecessary cost on to a program that is using it unless that program explicitly allows it, hence the problem.</p><p>Then again, <a href=http://stackoverflow.com/questions/1528727/why-is-sse-scalar-sqrtx-slower-than-rsqrtx-x>calculating invsqrt(x)*x is faster than sqrt(x)</a>, so I guess anything is possible.</p></article></div></section><section><div class=dim><aside><h2>Multithreading Problems In Game Design</h2><ul></ul></aside><article><p>A couple years ago, when I first started designing a game engine to unify Box2D and my graphics engine, I thought this was a superb opportunity to join all the cool kids and multithread it. I mean all the other game developers were talking about having a thread for graphics, a thread for physics, a thread for audio, etc. etc. etc. So I spent a lot of time teaching myself various lockless threading techniques and building quite a few iterations of various multithreading structures. Almost all of them failed spectacularly for various reasons, but in the end they were all too complicated.</p><p>I eventually settled on a single worker thread that was sent off to start working on the physics at the beginning of a frame render. Then at the beginning of each subsequent frame I would check to see if the physics were done, and if so sync the physics and graphics and start up another physics render iteration. It was a very clean solution, but fundamentally flawed. For one, it introduces an inescapable frame of input lag.</p><pre>Single Thread Low Load
&nbsp;&nbsp;FRAME 1&nbsp;&nbsp; +----+
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;|
. Input1 -> |&nbsp;&nbsp;&nbsp;&nbsp;|
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|[__]| Physics&nbsp;&nbsp; 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|[__]| Render&nbsp;&nbsp;&nbsp;&nbsp;
. FRAME 2&nbsp;&nbsp; +----+ INPUT 1 ON BACKBUFFER
. Input2 -> |&nbsp;&nbsp;&nbsp;&nbsp;|
. Process ->|&nbsp;&nbsp;&nbsp;&nbsp;|
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|[__]| Physics
. Input3 -> |[__]| Render
. FRAME 3&nbsp;&nbsp; +----+ INPUT 2 ON BACKBUFFER, INPUT 1 VISIBLE
.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;|
.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;|
. Process ->|[__]| Physics
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|[__]| Render
&nbsp;&nbsp;FRAME 4&nbsp;&nbsp; +----+ INPUT 3 ON BACKBUFFER, INPUT 2 VISIBLE

Multi Thread Low Load
&nbsp;&nbsp;FRAME 1&nbsp;&nbsp; +----+
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;| 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;|
. Input1 -> |&nbsp;&nbsp;&nbsp;&nbsp;| 
.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |[__]| Render/Physics START&nbsp;&nbsp;
. FRAME 2&nbsp;&nbsp; +----+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
. Input2 -> |____| Physics END
.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;|
.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;| 
. Input3 -> |[__]| Render/Physics START
. FRAME 3&nbsp;&nbsp; +----+ INPUT 1 ON BACKBUFFER
.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |____|
.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;| PHYSICS END
.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;| 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|____| Render/Physics START
&nbsp;&nbsp;FRAME 4&nbsp;&nbsp; +----+ INPUT 2 ON BACKBUFFER, INPUT 1 VISIBLE</pre><p>The multithreading, by definition, results in any given physics update only being reflected in the next rendered frame, because the entire point of multithreading is to immediately start rendering the current frame as soon as you start calculating physics. This causes a number of latency issues, but in addition it requires that one introduce a separated &ldquo;physics update&rdquo; function to be executed only during the physics/graphics sync. As I soon found out, this is a massive architectural complication, especially when you try to put in scripting or let other languages use your engine.</p><p>There is another, more subtle problem with dedicated threads for graphics/physics/audio/AI/anything. <strong>It doesn&rsquo;t scale.</strong> Let&rsquo;s say you have a platformer - AI will be contained inside the game logic, and the absolute vast majority of your CPU time will either be spent in graphics or physics, or possibly both. That means your game effectively only has two threads that are doing any meaningful amount of work. Modern processors have <strong>8 logical cores</strong><sup><a href=#ft_1>1</a></sup>, and the best one currently available has <strong>12</strong>. You&rsquo;re using <em>two of them</em>. You introduced all this complexity and input lag just so you could use 16.6% of the processor instead of 8.3%.</p><p>Instead of trying to create a thread for each individual component, you <a href=http://inception.davepedu.com/>need to go deeper</a>. You have to parallelize each individual component separately, then tie them together in a single-threaded design. This has the added bonus of being vastly more friendly to single-threaded CPUs that <em>can&rsquo;t</em> thread things (like certain phones), because the parallization goes on at a lower level and is invisible to the overall architecture of the library. So instead of having a graphics thread and a physics thread, you simply call the physics update, then call the graphics update, and inside each physics and graphics update you spawn enough worker threads to match the number of cores you have to work with and concurrently process as much stuff as possible. This eliminates latency problems, complicated library designs, and it <em>scales forever</em>. Even if your initial implementation of concurrency won&rsquo;t handle 32 cores, because the concurrency is encapsulated inside the engine, you can just go back and change it later <em>without ever having to modify any programs that use the graphics engine</em>.</p><p>Consequently, don&rsquo;t try to multithread your games. It isn&rsquo;t worth it. Separately parallelize each individual component instead and write your game engine single-threaded; only use additional threads for asynchronous activities like resource loading.</p><p><span style=font-size:80%><br><sup><a name=ft_1>1</a></span> The processors actually only have 4 or 6 <em>physical</em> cores, but use hyperthreading techniques so that 8 or 12 logical cores are presented to the operating system. From a software point of view, however, this is immaterial.</span></p></article></div></section><section><div class=dim><aside><h2>Stop Following The Rules</h2><ul></ul></aside><article><p>The fact that math, for most people, is about a set of rules, exemplifies how terrible our attempts at teaching it are. A disturbing amount of programming education is also spent hammering proper coding guidelines into students&rsquo; heads. Describing someone as a cowboy programmer is often derisive, and wars between standards, rules and languages rage like everlasting fires. It is into these fires we throw the burnt-out husks that were once our imaginations. We have taken our holy texts and turned them into weapons to crush any remnants of creativity that might have survived our childhood&rsquo;s educational incarceration.</p><p>Math and programming are not sets of rules to be followed. Math is a language - an incredibly dense, powerful way of conveying ideas about abstraction and generalization taken to truly astonishing levels. Each theorem is another note added to a chord, and as the chords play one after another, they build on each other, across instruments, to form a grand symphony. Math, in the right hands, is the language of problem solving. Most people know enough math to get by. It&rsquo;s like knowing enough French to say hello, order food, and call a taxi. You don&rsquo;t really know the language, you&rsquo;re just repeating phrases to accomplish basic tasks. Only when you have mastered a certain amount of fluency can you construct your own epigraphs, and taste the feeling of putting thoughts into words.</p><p>With the proper background, Math becomes a box of legos. Sometimes you use the legos to solve problems. Other times you just start playing around and see what you can come up with. Like any language, Math can do simple things, like talk about the weather. Or, you can write a beautiful novel with words that soar through the reader&rsquo;s imagination. There are many ways to say things in Math. Perhaps you want to derive the formula for the <a href=http://en.wikipedia.org/wiki/Sphere#Volume_of_a_sphere>volume of a sphere</a>? You can use geometry, or perhaps calculus, or maybe it would be easier with spherical coordinates. Math even has dialects, there being many ways of writing a <a href=http://en.wikipedia.org/wiki/Derivative#Notations_for_differentiation>derivative</a>, or even a <a href=http://en.wikipedia.org/wiki/Partial_derivative#Notation>partial derivative</a> (one of my professors once managed to use three in a single lecture). As our mathematical vocabulary grows, we can construct more and more elegant sentences and paragraphs, refining the overall structure of our abstract essay.</p><p>Programming too, is just a language, one of concurrency, functions and flow-control. Programming could be considered a lingual descendant of Math. Just as English is Latin-based, so is programming a Math-based language. We can use it to express many arcane designs in an efficient manner. Each problem has many different solutions in many different dialects. There&rsquo;s functional programming and procedural programming and object-oriented programming. But the programming community is obsessed with solving boring problems and writing proper code. Too overly concerned about maintainability, naming conventions and source control. What constitutes &ldquo;common sense&rdquo; varies wildly depending on your chosen venue, and then everyone starts arguing about semicolons.</p><p>Creativity finds little support in modern programming. Anything not adhering to strict protocols is considered useless at best, and potentially damaging at worst. Programming education is infused with corporate policy, designed to teach students how to behave and not get into trouble. Even then, its terribly inconsistent, with multiple factions warring with each other over whose corporate policies are superior. Programming languages are treated more like religions than tools.</p><p>The issue is that solving new problems, by definition, requires creative thinking. Corporate policy designed to stamp out anything not adhering to &ldquo;best practices&rdquo; is shooting itself in the foot, because it is incapable of solving new classes of problems that didn&rsquo;t exist 5 years ago. Companies that finally succeed in beating the last drop of creativity out of their employees suddenly need to hire college graduates to solve new problems they don&rsquo;t know how to deal with, and the cycle starts again. We become so obsessed with enforcing proper code etiquette that we forget how to play with the language. We think we&rsquo;re doing ourselves a favor by ruthlessly enforcing strict coding guidelines, only to realize our code has already become irrelevant.</p><p>We need to separate our mathematical language from the proof. Just as there is more to English than writing technical specifications, there is more to Math than formal research papers, and more to programming than writing mission-critical production code. Rules and standards are part of a healthy programming diet, but we must remember to take everything in moderation. We can&rsquo;t be so obsessed with writing standardized code that we forget to teach students all the wonderful obscurities of the language. We can&rsquo;t be telling people to never use a feature of a programming language because they&rsquo;ll never use it properly. Of course they won&rsquo;t use it properly if they can&rsquo;t even try! We should not only be teaching programmers the importance of formality, but <em>where</em> it&rsquo;s important, and where it&rsquo;s <em>not</em>. We should encourage less stringent rules on non-critical code and small side projects.</p><p>In mathematics, one never writes a proof from beginning to finish. Often you will work backwards, or take shortcuts, until you finally refine it to a point where you can write out the formal specification. When messy code is put into production, it&rsquo;s not the programmer&rsquo;s fault for being creative, it&rsquo;s the idiot who didn&rsquo;t refactor it first. Solving this by removing all creativity from the entire pipeline is like banning cars to lower the accident rate.</p><p>Corporate policy is for corporate code, not experimental features. Don&rsquo;t let your creativity die. Stop following the rules.</p></article></div></section><section><div class=dim><aside><h2>Why Windows 8 Does The Right Thing The Wrong Way</h2><ul></ul></aside><article><p>Yesterday, I saw a superb presentation called &ldquo;<a href=http://www.slideshare.net/bcousins/when-the-consoles-die-what-comes-next>When The Consoles Die, What Comes Next?</a>&rdquo; by Ben Cousins. It demonstrates that mobile gaming is behaving as a disruptive technology, and is causing the same market decline in consoles that consoles themselves did to arcades in the 1990s. He also demonstrates how TV crushed cinema in a similar manner - we just don&rsquo;t think of it like that because we don&rsquo;t remember back when almost <strong>60% of the population</strong> was going to the movie theaters on a weekly basis. Today, most people tend to go to the movie theater as a special occasion, so the theaters didn&rsquo;t completely die out, they just lost their market dominance. The role the movie theater played changed as new technology was introduced.</p><p>The game industry, and in fact the software industry as a whole, is in a similar situation. Due to the mass adoption of iPads and other tablets, we now have a mobile computing experience that is distinct from that of say, a console, or even a PC. Consequently, the role of consoles and PCs will shift in response to this new technology. However, while many people are eager to jump on the bandwagon (and it&rsquo;s a <a href=http://en.wikipedia.org/wiki/Angry_Birds>very lucrative bandwagon</a>), we are already losing sight of what will happen to stabilize the market.</p><p>People who want to sound futuristic and smart are talking about the &ldquo;Post-PC Era&rdquo;, which is a very inaccurate thing to say. PCs are clearly very useful for some tasks, and its unlikely that they will be entirely replaced by mobile computing, especially when screen real-estate is so important to development and productivity, and the difficulty of replicating an ergonomic keyboard. The underlying concept of a PC, in that you sit down at it, have a keyboard and mouse and a large screen to work at, is unlikely to change significantly. The mouse will probably be replaced by adaptive touch solutions and possibly gestures, and the screen might very well turn into a giant glass slab with OLEDs on it, or perhaps simply exist as the wall, but the underlying idea is not going anywhere. It will simply <em>evolve</em>.</p><p>Windows 8 is both a surprisingly prescient move on part of Microsoft, and also (not surprisingly) a horrible train wreck of an OS. The key concept that Microsoft correctly anticipated was the <strong>unification of operating systems</strong>. It is foolish to think that we will continue on with this brick wall separating tablet devices and PCs. The difference between tablets and PCs is simply one of both user interface and user experience. These are both managed by the highest layers of complexity in an operating system, such that it can simply adapt its presentation to suit whatever device it is currently on. It will <em>have to</em> once we introduce monitors the size of walls and OLED cards with embedded microchips. There will be such a ridiculous number of possible presentation mediums, that the idea of a presentation medium must be generalized such that a single operating system can operate on a stupendous range of devices.</p><p>This has important consequences for the future of software. Currently we seem to think that there should be &ldquo;tablet versions&rdquo; of software. This is silly and inconvenient. If you buy a piece of software, it should <strong>just work</strong>, no matter what you put it on. If it finds itself on a PC, it will analyze the screen size and behave appropriately. If its on a tablet, it will enable touch controls and reorganize the UI appropriately. More importantly, you shouldn&rsquo;t have to <em>buy</em> a version for each of your devices, because eventually there won&rsquo;t be anything other than a computer we carry around with us that plugs into terminals or interacts with small central servers at a company.</p><p>If someone buys a game I make, they <em>own</em> a copy of that game. That means they need to be able to get a copy of that game on all their devices without having to buy it 2 or 3 times. The act of buying the game should make it available to install on any interactive medium they want, and my game should simply adapt itself to whatever medium is being used to play it. The difference between PC and tablet will become blurred as they are reduced to simply being different modes of interaction, with the same underlying functionality.</p><p>This is what Microsoft is attempting to anticipate, by building an operating system that can work on both a normal computer and a tablet. They even introduce a Windows App Store, which is a crucial step towards allowing you to buy a program for both your PC and your tablet <em>in a single purchase</em>. Unfortunately, the train-wreck analogy is all too appropriate for describing the state of Windows 8. Rather than presenting an elegant, unified tablet and PC experience, they smash together two completely incompatible interfaces in an incoherent disaster. You are either presented with a metro interface, or a traditional desktop interface, with no in-between. The transition is about as smooth as your head smashing against a brick wall. They don&rsquo;t even properly account for the fact that their new metro start menu is terribly optimized for a mouse, but try to make you use it anyway. It does the right thing, the wrong way.</p><p>The game industry has yet to catch on to this, since one designs either a &ldquo;PC game&rdquo; or a &ldquo;mobile game&rdquo;. When a game is released on a tablet, it&rsquo;s a special &ldquo;mobile version&rdquo;. FL Studio has a special mobile version. There is no unification anywhere, and the two are treated as separate walled gardens. While this is currently an advantage during a time where tablets don&rsquo;t have the kind of power a PC does, it will quickly become a disadvantage. The convenience of having familiar interfaces on all your devices, with all of the same programs, will trump isolated functionality. There will always be games and programs more suited to consoles, or to PCs, or to tablets, but unless we stop thinking of these as separate devices, and instead one of many possible user experiences that we must adapt our creations to, we will find ourselves on the wrong side of history.</p></article></div></section><section><div class=dim><aside><h2>Visual Studio Broke My Computer</h2><ul></ul></aside><article><p>So I&rsquo;d been using the developer preview of VS11 and liked some of its improvements. When the desaturated VS11 beta came out, <a href=https://erikmcclure.com/blog/implicit-ui-design/>I hated the color scheme</a> but decided I still wanted the upgraded components, so I went to install VS11 beta. Unfortunately the beta only lets you change its install location if the preview developer preview isn&rsquo;t installed, and the developer preview had installed itself into C:\ without ever letting me change the path, which was annoying. So I took the opportunity to fix things and uninstalled the developer preview, then installed the beta of VS11.</p><p>Everything was fine and dandy until I discovered that VS11 wasn&rsquo;t compiling C++ DLLs that worked on XP. I don&rsquo;t know how it managed to do this, since the DLL had no dependencies whatsoever, and that bug was only supposed to affect MFC and other windows related components and hence there was no windows flag for me to specify which version I wanted, but just to be sure I decided to try and compile it in VS2010. It was at this point I discovered that VS2010 could no longer open any projects at all. It was broken. Further investigation revealed that uninstalling VS11 developer preview will break VS2010. Now, I had an ultimate version of VS2010 I&rsquo;ve had sitting around for a while I got from Dreamspark, so I figured I could just uninstall VS2010 and then reinstall the ultimate version and that would kill any remaining problems the pesky 2011 beta introduced.</p><p>The thing is, I can&rsquo;t uninstall the SP1 update from VS2010. Not before I uninstalled VS2010, not after I uninstalled it, not even after I installed the ultimate version. It just gave me this:</p><blockquote>*The removal of Microsoft Visual Studio 2010 Service Pack 1 may put this computer in an state in which projects cannot be loaded and Service Pack 1 cannot be reinstalled. For instructions about how to correct the problem, see the readme on the Microsoft Download Center website.*</blockquote><p>So I just had to leave the Service Pack alone and attempted to re-apply it after installing VS2010 Ultimate, but the online installer failed. So then I downloaded the SP1 iso file and installed that. It failed too, but this time I could fix the problem - someone had forgotten to copy the F#_redist MSI file to the TEMP directory, instead only copying the CAB file. Note that I don&rsquo;t even have F# installed.</p><p>I was able to resolve that problem and finished installing the service pack, but to no avail. Both the VS2010 installation and the service pack had forgotten to install the C++ standard library headers, which, as you can imagine, are kind of important. I searched around for a solution, but the only guy who had the same problem as me had simply <a href=http://social.msdn.microsoft.com/Forums/pl/vssetup/thread/86743068-8e78-4f20-bb3e-44ff0e5170c0>reformatted and reinstalled windows</a> (note the moderator&rsquo;s excellent grasp of english grammar). The only thing I had to go off of was using a <a href=http://archive.msdn.microsoft.com/vs2010uninstall>special utility</a> they built to uninstall all traces of VS2010 from your computer. Unfortunately, the utility doesn&rsquo;t actually succeed in uninstalling everything, and also doesn&rsquo;t uninstall SP1, so you have to uninstall SP1 first before running the utility. The problem is, I can&rsquo;t uninstall SP1 or I&rsquo;ll never be able to install it again.</p><p>At this point it appears I am pretty much fucked. How does Microsoft consider this an acceptable scenario? I worked as an intern at Microsoft once, I <em>know</em> they use their own development tools. I used tools that hadn&rsquo;t even been released yet. There was one guy on our team whose entire job was just the setup. And yet, through a series of astonishingly bad failures, any one of which being fixed would have prevented this scenario, my computer is now apparently fucked, and I&rsquo;m going to have to upgrade my windows installation to 64 bit a lot sooner than I wanted.</p><p><strong>EDIT:</strong> After using the uninstall tool to do a full uninstall and uninstalling SP1 and manually finding any VC10 related registry entries in the registry and deleting them, then reinstalling everything from scratch, I solved the header file problem (but had to reinstall SP1 or it wouldn&rsquo;t let me open my project files). However then the broken VCTargetsPath problem showed up again, which a repair didn&rsquo;t fix. I finally fixed the issue by finding someone else with a working installation of VC10, having them export their MSBuild registry key and manually merging it into my registry. If you have this problem, I&rsquo;ve uploaded the registry key (which should be the same for any system, XP or 7) <a href=http://www.mediafire.com/?91yph5n2704hvzi>here</a>. If you have a 64-bit machine, you may need to copy its values into the corresponding WoW64 nodes (just search for a second instance of MSBuild in your registry).</p></article></div></section><section><div class=dim><aside><h2>Implicit UI Design</h2><ul></ul></aside><article><p>For a long time, I have been frustrated with the poor UI design that is rampant in the software industry. As a consequence, many Linux enthusiasts have pointed out how productive you can be with Emacs, VIM, and other keyboard-shortcut/terminal oriented software. The UI design has gotten so bad, I have to agree that in comparison to recent user interface designs, keyboard shortcuts are looking rather appealing. This, however, doesn&rsquo;t mean that one approach is inherently better than another, simply that modern user interfaces <em>suck</em>.</p><p>In this blog, I&rsquo;m going to outline several improvements to user interfaces and the generalized alternative design philosophy that is behind many of them. To start, let&rsquo;s look at <a href=http://blogs.msdn.com/b/visualstudio/archive/2012/02/23/introducing-the-new-developer-experience.aspx>this recent article</a> about Visual Studio 11, which details Microsoft&rsquo;s latest strategy to ruin their products by sucking all the color out of them:</p><p><div style=text-align:center;overflow:hidden><a href=http://blogs.msdn.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-01-29-92-metablogapi/1667.dev10toolbars_5F00_4D41B521.png><img src=http://blogs.msdn.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-01-29-92-metablogapi/1667.dev10toolbars_5F00_4D41B521.png alt="Visual Studio 2010"></a><i>Visual Studio 2010</i></div><div style=text-align:center;overflow:hidden><a href=http://blogs.msdn.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-01-29-92-metablogapi/1667.dev10toolbars_5F00_4D41B521.png><img src=http://blogs.msdn.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-01-29-92-metablogapi/8117.dev11toolbars_5F00_2190EB25.png alt="Visual Studio 11"></a><i>Visual Studio 11</i></div></p><p>See, color is kind of important. Notably, color is <em>how I find icons in your toolbar</em>. To me, color is a kind of filter. If I know what color(s) are most prominent in a given icon, I can mentally filter out everything else and only have to browse through a much smaller subset of your icons. Combined with having a vague notion of where a given icon is, this usually reduces the number of icons I have to mentally sift through to only one or two.</p><div style=text-align:center;overflow:hidden><a href=/img/qem4h.png><img src=/img/qem4h.png alt="Color Filtering"></a><i>Mental color and spatial filtering</i></div><p>If you remove color, I am left to only my spatial elimination, which can make things extremely annoying when there are a bunch of icons right next to each other. Microsoft claims to have done a study that shows that this new style does not harm being able to identify a given icon in terms of speed, but fails to take into account the <em>mental tax</em> that goes into finding an icon without color.<blockquote><em>While we understand that opinions on this new style of iconography may vary, an icon recognition study conducted with 76 participants, 40 existing and 36 new VS users, showed no negative effect in icon recognition rates for either group due to the glyph style transition. In the case of the new VS users they were able to successfully identify the new VS 11 icons faster than the VS 2010 icons (i.e., given a command, users were faster at pointing to the icon representing that command).</em></blockquote>You may still be able to find the icon, especially after you&rsquo;ve been forced to memorize its position just to find it again, and do it reasonably fast, but whether you like it or not, the absence of color will force your brain to process more possible icons before settling down on the one you actually want. When I&rsquo;m compiling something, I only have a vague notion of where the start button actually is. I don&rsquo;t need to know exactly where it is or even what it looks like; it&rsquo;s extremely easy to find since its practically the only green button on the entire toolbar. Same goes for save and open. The uniform color makes the icons very easy to spot, and all the other icons are immediately discarded.</p><p>That said, there are many poorly design icons in Visual Studio. Take this group of icons:
<a href=/img/mIcIL.png target=_blank><img src=/img/mIcIL.png alt="Bad Icons!"></a></p><p>I don&rsquo;t really know what any of those buttons do. One of them is some sort of toolbox and one is probably the properties window, but they have <em>too many colors</em>. It becomes, again, mentally taxing when even looking at those icons because there is too much irrelevant information being processed. In this scenario, the colors are detrimental to the identification of the buttons. This is because there is no single dominant color, like on the play button, or the save button. Hence, the best solution is to design icons with principle colors, such that some or all the icon is one color and the rest is greyscale. The brain edits out the greyscale and allows identification by color, followed by location, followed by actual glyph shape. To avoid overloading the user with a rainbow of colors, consider scaling the amount of color to how likely a given button is to be used. Desaturate or shrink the colored area of buttons that are much less important. Color is a tool that can be used correctly or incorrectly - that doesn&rsquo;t mean you should just throw it away the first time you screw up.</p><p>We can make better decisions by taking into account how the user is going to use the GUI. By developing an awareness of how a user interface is normally used, we develop vastly superior interactions that accelerate, rather than impede, workflow. To return to visual studio, let&rsquo;s take a look at a feature in the VS11: pinning a variable preview.
<a href=/img/c6j3X.png target=_blank><img src=/img/c6j3X.png alt="Variable Preview"></a><a href=/img/fskyt.png target=_blank><img src=/img/fskyt.png alt="Pinned Variable Preview"></a></p><p>This is a terrible implementation for a number of reasons. First, since the pin button is all the way on the other end, it is a moving target and you&rsquo;ll never really be sure where it is until you need to pin something. Furthermore, you can drag the pinned variable around, and you&rsquo;ll want to after Visual Studio moves it to a seemingly random location that is almost always annoying (but only after the entire IDE locks up for 3 seconds because you haven&rsquo;t done it recently). When would a user be dragging a variable around? Only when its pinned. A better implementation is to make a handle on the left side of any variable preview. If you click the handle (and optionally drag the variable around), it is implicitly converted to a pinned variable without changing anything else, and a close button appears to the left of the handle.</p><p><a href=/img/yXQ12.png target=_blank><img src=/img/yXQ12.png alt="Better Variable Preview"></a><a href=/img/YOXVA.png target=_blank><img src=/img/YOXVA.png alt="Better Pinned Preview"></a></p><p>This is much easier to use, because it eliminates a mouse click and prevents the variable from moving to some random location you must then locate afterwards to move it to your actual desired location. By shifting the close button to the left, it is no longer a moving target. To make this even better, you should make sure previews snap to each other so you can quickly build a list of them, and probably include a menu dropdown by the close button too.</p><p>We have just used <em>Implicit UI Design</em>, where instead of forcing the user to explicitly specify what they want to have happen, we can use contextual clues to <em>imply</em> a given action. We knew that the user could not possibly move a variable preview without wanting to pin it, so we simply made the act of moving the preview, pin it. Another example is docking windows. Both Adobe and Visual Studio are guilty of trying to make everything dockable everywhere without realizing that this is usually just extremely annoying, not helpful. I mean really, why would I want to <em>dock</em> the find window?</p><div style=text-align:center;overflow:hidden><a href=/img/odtyz.png><img style=height:380px src=/img/odtyz.png alt="Bad Find Window!"></a></br><i>Goddamn it, not again</i></div><p>Even if I was doing a lot of find and replace, it just isn&rsquo;t useful. You can usually cover up half your code while finding and replacing without too much hassle. The only thing this does is make it really hard to move the damn window anywhere because if you aren&rsquo;t careful you&rsquo;ll accidentally dock it to the toolbar and then you have to pull the damn thing out and hope nothing else blew up, and if your really unlucky you&rsquo;ll have to reconstruct the whole freaking solution window.</p><p>That isn&rsquo;t helpful. The fact that the act of docking and undocking can be excruciatingly slow makes things even worse and is inexcusable. Only a UI that is bloated beyond my ability to comprehend could possibly have such a difficult time docking and undocking things, no doubt made worse by their fetishization of docking windows. Docking windows correctly requires that you account for the extremely common mistake of accidentally undocking or docking a window where you didn&rsquo;t want it. If a mistake is so common and so annoying, you should make it either much harder to do, or make it very easy to undo. In this case, you should remember where the window was docked last (or not docked), and make a space for it to be dropped into, instead of forcing the user to figure out which magic location on the screen actually docks the window to the right position (which sometimes involves differences of 2 or 3 pixels, which is incredibly absurd).</p><div style=text-align:center;overflow:hidden><a href=/img/ZKhiJ.png><img src=/img/ZKhiJ.png alt="Spot Holding"></a><br><i>Spot Holding</i></div><p>Panels are related to docking (usually you can dock something into a collapsible panel), but even these aren&rsquo;t done very efficiently. If you somehow manage to get the window you want docked into a panel, it defaults to pinning <em>the entire group of panels</em>, regardless of whether they were pinned before or not. I just wanted to pin one!</p><div style=text-align:center;overflow:hidden><a href=/img/A4Q2w.png><img src=/img/A4Q2w.png / alt="Pin Disaster"></a><br><i>You have to drag the thing out and redock it to pin just that one panel.</i></div><p>There is a better way to do this. If we click on a collapsible panel, we know the user wants to show it. However, the only reason they even need to click on the panel is because we don&rsquo;t show it immediately after the mouse hovers over the button. This time should be less than a tenth of a second, and it should immediately close if the mouse gets too far away. It should stay open if the mouse is close enough that the user might have temporarily left the window but may want to come back in. Hovering over another panel button <em>immediately</em> replaces the current panel (in this case, at least), and dragging the panel title bar or the panel button lets you dock or undock.</p><p>Now the user will never need to click the panel to make it show up, so we can make that operation do something else. Why not make clicking a panel pin it open? And don&rsquo;t do any of that &ldquo;pin the entire bunch of panels&rdquo; crap either, just pin that one panel and have it so the other panels can still pop up over it. Then, if you click the panel button again, it&rsquo;s unpinned. This is <em>so much better</em> than the clunky UI interfaces we have right now, and we did it by thinking about <em>Implicit UI Design</em>. By making the mouse click redundant, we could take that to <em>imply</em> that the user wants the panel to be pinned. Moving the mouse far away from the panel <em>implies</em> that the panel is no longer useful. To make sure a mistake is easy to correct, pinning a panel should be identical to simply having it be hovered over indefinitely, and should not change the surrounding UI in any way. Then a mistake can simply be undone by clicking the panel button again, which is a much larger target than a tiny little pin icon. Combine this with our improved docking above, so that a mistakenly undocked panel, when clicked and dragged again, has its old spot ready and waiting in case you want to undo your mistake.</p><div style=overflow:hidden><a href=/img/1j4kq.png><img src=/img/1j4kq.png alt="Panel holding"></a></div><p>It&rsquo;s 2012. I think its high time our user interfaces reflected that.</p></article></div></section><section><div class=dim><aside><h2>Linux Mint 12 KDE</h2><ul></ul></aside><article><p>Over the course of 3 hours spent trying to figure out why my Linux Mint 12 KDE installation would to go to a permanent black screen on boot, I managed to overheat part of my computer (at least that is the only thing that could explain this) to the point where it&rsquo;d lock up on the POST and had to give up until this morning, where I managed to figure out that I could delete the xorg.conf file in Mint to force it to go to default settings. This finally got Mint 12 to show up, and I later confirmed that the nvidia drivers were broken. I then discovered that the nvidia drivers in the distribution for apt-get sources are almost <strong>120 versions</strong> behind the current release (295), but the installer for that kept failing despite my attempts to fix it and having to add source repositories to apt-get because apparently these are disabled by default, which confused me greatly for a short period whilst trying to install the Linux source. This ultimately proved futile since Nvidia can&rsquo;t be bothered to make anything remotely easy for Linux, so I&rsquo;m stuck with a half-broken default driver that can&rsquo;t use my second monitor with a mouse cursor that repeatedly blinks out of existence in a very aggravating manner.</p><p>Of course, the only reason I even have Linux installed is to compile things on Linux and make sure they work, so as long as my development IDE works, I should be fine! Naturally, it doesn&rsquo;t. Kdevelop4 is the least insultingly bad coding IDE available for Linux that isn&rsquo;t VIM or Emacs, which both follow in the tradition of being so amazingly configurable you&rsquo;ll spent half your development cycle trying to get them to work work and then learning all the arcane commands they use in order to have some semblance of productivity. Like most bizarre, functionality-oriented Linux tools, after about 6 months of torturing yourself with them, you&rsquo;ll probably be significantly more productive than someone on Visual Studio. Sadly, the majority of my time is not spent coding, it&rsquo;s spent sitting in front of a computer screen for hours trying to figure out how to get 50 lines of code to work properly. Saying that your text editor lets you be super productive assumes you are typing something all the time, and if you are doing that you are a codemonkey, not a programmer. Hence, I really don&rsquo;t give a fuck about how efficient a given text editor is so long as it has a couple commands I find useful in it. What&rsquo;s more important is that it works. KDevelop4 looked like it might actually do this, but sadly it can&rsquo;t find any include files. It also can&rsquo;t compile anything that isn&rsquo;t C++ because it builds everything with CMake and refuses to properly compile a C file. It has a bunch of hilariously bad user interface design choices, and basically just sucks.</p><p>So now i&rsquo;m back to the command line, editing my code in Kate, the default text editor for Mint 12, forced to compile my code with GCC from the terminal. This, of course, only works when I have a single source file. Now I need to learn how to write makefiles, which are notoriously confusing and ugly and require me to do all sorts of weird things and hope GCC&rsquo;s -M option actually generates the right rule because the compiler itself is too stupid to figure out dependencies, but I have no IDE to tell it what to do. Then I have to link everything, and then I have to debug my program from the terminal using command line gdb, which is one of the most incredibly painful experiences I have ever had. Meanwhile, every single user experience in Linux is still terribly designed, optimized for everything except what I want to do, difficult to configure because they let you configure too much stuff and are eager to tell you in 15000 lines of manual pages about every single esoteric command no one will ever use that makes it almost impossible to find anything until you find the exact sequence of letters that will actually let you find the command your looking for and not another one that looks almost like it but does something completely different. That, of course, is if you have a web browser. I don&rsquo;t know what the fuck you&rsquo;d even do with man. I assume you&rsquo;d have to just pipe the thing into grep just to find anything.</p><p>This is not elegant. It&rsquo;s a bit sad, but mostly it&rsquo;s just painful. I hate Linux. I don&rsquo;t hate the kernal. I don&rsquo;t hate all the functionality. It&rsquo;s just that the people who use Linux do not think about user experience. They think in terms of translating command line functions into GUIs and trying desperately to hang on to whatever pretty graphics are cool and when something doesn&rsquo;t work they tell you to stop using proprietary drivers from Nvidia, except the non-proprietary drivers <em>can&rsquo;t actually do 3D yet</em> but that&rsquo;s ok, no one needs that stuff. Never mind that Linux mint 12 doesn&rsquo;t actually come with any diagnostic or repair tools whatsoever. Never mind that every single distro I&rsquo;ve tried so far has been absolutely terrible one way or another. The more I am forced to use Linux, the more I crave for Windows and the fact that things tend to just <em>work</em> in Windows. Things don&rsquo;t work very well in Windows, but at this point that seems better than Linux&rsquo;s apparent preference of &ldquo;either it works really well or it doesn&rsquo;t work at all&rdquo;.</p><p>We could try to point fingers, but that usually doesn&rsquo;t solve anything. It&rsquo;s part nvidia&rsquo;s fault, it&rsquo;s part software vendors fault, its partly using a 40 year old window rendering engine that&rsquo;s so out of touch with reality it is truly painful, and it&rsquo;s partly the users either being too dumb to care about the broken things or too smart to use the broken things. It&rsquo;s a lot of people&rsquo;s fault. It&rsquo;s a lot of crap. I don&rsquo;t know how to fix it. I do know that it is crap, and it is broken, and it makes me want to punch the next person who says Linux is better than everything in the face, repeatedly, until he is a bloody mess on the ground begging for death to relieve his pain, because there are no words for expressing how much I hate this, and if you think I&rsquo;m not being fair, good for you, I don&rsquo;t give a fuck. But then I can never reason with people who are incapable of listening to alternative ideas, so its usually useless to bring the topic up anyway. I suggest hundreds of tweaks to things that need to do [x] or do [x] and people are like NO THAT ISN&rsquo;T NECESSARY GO AWAY YOU KNOW NOTHING. Fine. Fuck you. I&rsquo;m done with this shit.</p><p>Oh wait, I still have to do my Linux homework.</p><p>EDIT: Upon further inspection, Linux Mint 12 is melting my graphics card. The graphics card fan is on, all the time, and if I run mint for too long either installed or livecd or really anything, the fan will be on the whole time and upon restart the POST check locks up. However after turning off the computer for 10 minutes and going into windows, the temperature is at 46°C, which is far below dangerous levels, so either it has a very good heatsink or the card isn&rsquo;t actually melting, it&rsquo;s just being run improperly, which doesn&rsquo;t really make me feel any better. Either way, I am now in an even more serious situation, because I have homework to do but Linux Mint is literally <em>breaking my computer</em>. I&rsquo;d try to fix it by switching graphics drivers but at this point <em>every single driver available is broken</em>. <strong>ALL OF THEM.</strong> I don&rsquo;t even know what to do anymore.</p></article></div></section><section><div class=dim><aside><h2>'Programmer' is an Overgeneralization</h2><ul></ul></aside><article><blockquote>*"Beware of bugs in the above code; I have only proved it correct, not tried it."* - Donald Knuth</blockquote><p>Earlier today, I came across a post during a <em>google-fu</em> session that claimed that no one should use the C++ standard library function <code>make_heap</code>, because almost nobody uses it correctly. I immediately started mentally ranting about how utterly ridiculous this claim is, because anyone whose gone to a basic algorithm class would know how to properly use <code>make_heap</code>. Then I started thinking about all the programmers who <em>don&rsquo;t</em> know what a heap is, and furthermore probably don&rsquo;t even need to know.</p><p>Then I realized that both of these groups are still called programmers.</p><p>When I was a wee little lad, I was given a lot of very bad advice on proper programming techniques. Over the years, I have observed that most of the advice wasn&rsquo;t actually bad advice in-of-itself, but rather it was being given without context. The current startup wave has had an interesting effect of causing a lot of hackers to realize that &ldquo;performance doesn&rsquo;t matter&rdquo; is a piece of advice <em>riddled</em> with caveats and subtle context, especially when dealing with complex architectures that can interact in unexpected ways. While this <a href=http://en.wikipedia.org/wiki/Chinese_whispers>broken telephone effect</a> arising from the lack of context is a widespread problem on its own, in reality it is simply a symptom of an even deeper problem.</p><p>The word programmer covers a stupendously large spectrum of abilities and skill levels. On a vertical axis, a programmer could barely know how to use vbscript, or they could be writing compilers for Intel and developing scientific computation software for aviation companies. On a horizontal axis, they could be experts on databases, or weeding performance out of a GPU, or building concurrent processing libraries, or making physics engines, or doing image processing, or generating 3D models, or writing printer drivers, or using coffeescript, HTML5 and AJAX to build web apps, or using nginx and PHP for writing the LAMP stack the web app is sitting on, or maybe they write networking libraries or do artificial intelligence research. They are all programmers.</p><p><strong>This is insane.</strong></p><p>Our world is <a href=http://online.wsj.com/article/SB10001424053111903480904576512250915629460.html>being eaten by software</a>. In the future, programming will be a basic course alongside reading and math. You&rsquo;ll have four R&rsquo;s - Reading, &lsquo;Riting, &lsquo;Rithematic, and Recursion. Saying that one is a programmer will become meaningless because 10% or more of the population will be a programmer on some level. Already the word &ldquo;programmer&rdquo; has so many possible meanings it&rsquo;s like calling yourself a &ldquo;scientist&rdquo; instead of a physicist. Yet, what other choices do we have? The only current attempt at fixing this gave a paltry <a href=http://www.skorks.com/2010/03/the-difference-between-a-developer-a-programmer-and-a-computer-scientist/>3 options</a> that are just incapable of conveying the differences between me and someone who graduated from college with a PhD in artificial intelligence. They do multidimensional mathematical analysis and evaluation using functional languages I will never understand without years of research. I&rsquo;m supposed to write incredibly fast, clever C++ and HLSL assembly while juggling complex transformation matrices to draw pretty pictures on the screen. These jobs are both extremely difficult for completely different reasons, and neither person can do the other persons job. What is good practice for one is an abhorration for the other. We are both programmers. Even <em>within our own field</em>, we are simply graphics programmers or AI programmers or <code>[x]</code> programmers.</p><p>Do you know why we have pointless language wars, and meaningless arguments about what is good in practice? Do you know why nobody ever comes to a consensus on these views except in certain circles where &ldquo;practice&rdquo; means the same thing to everyone? Because we are overgeneralizing <em>ourselves</em>. We view ourselves as a bunch of programmers who happen to specialize in certain things, and we are making the mistake of thinking that our viewpoint applies outside of our area of expertise. We are industrial engineers trying to tell chemists how to run their experiments. We are architects trying to tell English majors how to design an essay because we both use lots of paper.</p><p>This attitude is deeply ingrained in the core of computer science. The entire point of computer science is that a bunch of basic data structures can do everything you will ever need to do. It is a fallacy to try and extend this to programming in general, because it simply is not true. We are forgetting that these data structures only do everything we need to do in the magical perfect land of mathematics, and ignore all the different implementations that are built for different areas of programming, for completely different uses. Donald Knuth understood the difference between theory and implementation - we should strive to recognize the difference between <em>theoretical</em> and <em>implementation-specific</em> advice.</p><p>It is no longer enough to simply ask someone if they are a programmer. Saying a programmer writes programs is like saying a scientist does science. The difference is that botanists don&rsquo;t design nuclear reactors.</p></article></div></section><section><div class=dim><aside><h2>The Great Mystery of Linear Gradient Lighting</h2><ul></ul></aside><article><p>A long, long time ago, in pretty much the same place I&rsquo;m sitting in right now, I was learning how one would do 2D lighting with soft shadows and discovered the age old adage in 2D graphics: linear gradient lighting looks better than mathematically correct inverse square lighting.</p><p>Strange.</p><p>I brushed it off as artistic license and perceptual trickery, but over the years, as I dug into advanced lighting concepts, nothing could explain this. It was a mystery. Around the time I discovered microfacet theory I figured it could theoretically be an attempt to approximate non-lambertanian reflectance models, but even that wouldn&rsquo;t turn an exponential curve into a linear one.</p><p>This bizarre law even showed up in my 3D lighting experiments. Attempting to invoke the inverse square law would simply result in extremely bright and dark areas and would look absolutely terrible, and yet the only apparent fix I saw anywhere was simply calculating light via linear distance in clear violation of observed light behavior. Everywhere I looked, people calculated light on a linear basis, everywhere, on everything. Was it the equations? Perhaps the equations being used operated on linear light values instead of exponential ones and so only output the correct value if the light was linear? No, that wasn&rsquo;t it. I couldn&rsquo;t figure it out. Years and years and years would pass with this discrepancy left unaccounted for.</p><p>A few months ago I noted an article on gamma correction and assumed it was related to color correction or some other post process effect designed to compensate for monitor behavior, and put it as a very low priority research point on my mental to-do-list. No reason fixing up minor brightness problems until your graphics engine can actually render everything properly. Yesterday, though, I happened across a <a href="http://news.ycombinator.com/item?id=3294840">Hacker News</a> posting about learning modern 3D engine programming. Curious if it had anything I didn&rsquo;t already know, I ran through its topics, and <a href=http://www.arcsynthesis.org/gltut/Texturing/Tutorial%2016.html>found this</a>. Gamma correction wasn&rsquo;t just making the scene brighter to fit with the monitor, it was compensating for the fact that <em>most images are actually already gamma-corrected</em>.</p><p>In a nutshell, the brightness of a monitor is exponential, not linear (with a power of about 2.2). The result is that a linear gradient displayed on the monitor is not actually increasing in brightness linearly. Because it&rsquo;s mapped to a curve, it will actually increase in brightness exponentially. This is due to the human visual system processing luminosity on a logarithmic scale. The curve in question is this:</p><p><a href=http://http.developer.nvidia.com/GPUGems3/elementLinks/24fig02.jpg target=_blank><img src=http://http.developer.nvidia.com/GPUGems3/elementLinks/24fig02.jpg alt="Gamma Response Curve"></a>
<span style=font-size:80%><br><i>Source: <a href=http://http.developer.nvidia.com/GPUGems3/gpugems3_ch24.html>GPU Gems 3 - Chapter 24: The Importance of Being Linear</a></i></span></p><p>You can see the effect in this picture, taken from the <a href=http://www.arcsynthesis.org/gltut/Texturing/Tutorial%2016.html>article I mentioned</a>:
<a href=http://www.arcsynthesis.org/gltut/Texturing/Gamma%20Ramp%20sRGB.png target=_blank><img src=http://www.arcsynthesis.org/gltut/Texturing/Gamma%20Ramp%20sRGB.png alt="Linear Curve"></a></p><p>The thing is, I always assumed the top linear gradient was a linear gradient. Sure it looks a little dark, but hey, I suppose that might happen if you&rsquo;re increasing at 25% increments, right? <strong>WRONG</strong>. The <em>bottom strip</em> is a true linear gradient<sup>1</sup>. The top strip is a literal assignment of linear gradient RGB values, going from 0 to 62 to 126, etc. While this is, digitally speaking, a mathematical linear gradient, what happens when it gets displayed on the screen? It gets distorted by the CRT Gamma curve seen in the above graph, which makes the end value <em>exponential</em>. The bottom strip, on the other hand, is gamma corrected - it is NOT a mathematical linear gradient. It&rsquo;s values go from 0 to <strong>134</strong> to <strong>185</strong>. As a result, when this exponential curve is displayed on your monitor, it&rsquo;s values are dragged down by the exact inverse exponential curve, resulting in a true linear curve. An image that has been &ldquo;gamma-corrected&rdquo; in this manner is said to exist in sRGB color space.</p><p>The thing is, most images <em>aren&rsquo;t linear</em>. They&rsquo;re actually in the sRGB color space, otherwise they&rsquo;d look totally wrong when we viewed them on our monitors. Normally, this doesn&rsquo;t matter, which is why most 2D games simply ignore gamma completely. Because all a 2D engine does is take a pixel and display it on the screen without touching it, if you enable gamma correction you will actually <em>over-correct</em> the image and it will look terrible. This becomes a problem with image editing, because digital artists are drawing and coloring things on their monitors and they try to make sure that everything looks good <em>on their monitor</em>. So if an artist were visually trying to make a linear gradient, they would probably make something similar to the already gamma-corrected strip we saw earlier. Because virtually no image editors linearize images when saving (for good reason), the resulting image an artist creates is <em>actually in sRGB color space</em>, which is why only turning on gamma correction will usually simply make everything look bright and washed out, since you are normally using images that are <em>already gamma-corrected</em>. This is actually good thing due to subtle precision issues, but it creates a serious problem when you start trying to do lighting calculations.</p><p>The thing is, lighting calculations are linear operations. It&rsquo;s why you use Linear Algebra for most of your image processing needs. Because of this, when I tried to use the inverse-square law for my lighting functions, the resulting value that I was multiplying on to the already-gamma-corrected image <em><strong>was not gamma corrected!</strong></em> In order to do proper lighting, you would have to first linearize the gamma-corrected image, perform the lighting calculation on it, and then re-gamma-correct the end result.</p><p>Wait a minute, what did we say the gamma curve value was? It&rsquo;s <span class=math>$$ x^{2.2} $$</span>, so <span class=math>$$ x^{0.45} $$</span> will gamma-correct the value <span class=math>$$ x $$</span>. But the inverse square law states that the intensity of a light is actually <span class=math>$$ \frac{1}{x^2} $$</span>, so if you were to gamma correct the inverse square law, you&rsquo;d end up with:<div class=math>\[ {\bigg(\frac{1}{x^2}}\bigg)^{0.45} = {x^{-2}}^{0.45} = x^{-0.9} ≈ x^{1} \]</div><em>That&rsquo;s almost linear!<sup>2</sup></em></p><p><strong><span style=font-size:200%>OH MY GOD<br></span></strong><a href=http://bucultureshock.com/wp-content/uploads/2011/10/mind-blown-11.jpeg target=_blank><img src=http://bucultureshock.com/wp-content/uploads/2011/10/mind-blown-11.jpeg alt="MIND == BLOWN"></a></p><p><em><strong>That&rsquo;s it!</strong></em> The reason I saw linear curves all over the place was because it was a rough approximation to gamma correction! The reason linear lighting looks good in a 2D game is because its actually an approximation to a <em>gamma-corrected inverse-square law!</em> <strong>Holy shit!</strong> Why didn&rsquo;t anyone ever explain this?!<sup>3</sup> Now it all makes sense! Just to confirm my findings, I went back to my 3D lighting experiment, and sure enough, after correcting the gamma values, using the inverse square law for the lighting gave correct results! <strong>MUAHAHAHAHAHAHA!</strong></p><p>For those of you using OpenGL, you can implement gamma correction as explained in the article <a href=http://www.arcsynthesis.org/gltut/Texturing/Tutorial%2016.html>mentioned above</a>. For those of you using DirectX9 (not 10), you can simply enable <code>D3DSAMP_SRGBTEXTURE</code> on whichever texture stages are using sRGB textures (usually only the diffuse map), and then enable <code>D3DRS_SRGBWRITEENABLE</code> during your drawing calls (a gamma-correction stateblock containing both of those works nicely). For things like GUI, you&rsquo;ll probably want to bypass the sRGB part. Like OpenGL, you can also skip <code>D3DRS_SRGBWRITEENABLE</code> and simply gamma-correct the entire blended scene using <code>D3DCAPS3_LINEAR_TO_SRGB_PRESENTATION</code> in the <code>Present()</code> call, but this has a lot of caveats attached. In DirectX10, you no longer use <code>D3DSAMP_SRGBTEXTURE</code>. Instead, you use an sRGB texture format (see <a href=http://download.microsoft.com/download/b/5/5/b55d67ff-f1cb-4174-836a-bbf8f84fb7e1/Picture%20Perfect%20-%20Gamma%20Through%20the%20Rendering%20Pipeline.zip>this presentation</a> for details).</p><span style=font-size:80%><br><sup>1</sup> or at least much closer, depending on your monitors true gamma response<br><sup>2</sup> In reality I'm sweeping a whole bunch of math under the table here. What you really have to do is move the inverse square curve around until it overlaps the gamma curve, then apply it, and you'll get something that is <a href="http://www.wolframalpha.com/input/?i=plot+%281%2F%28x-1.9%29%5E2+-+0.25%29%5E0.45+from+0+to+1">roughly linear</a>.<br><sup>3</sup> If this is actually standard course material in a real graphics course, and I am just *really bad* at finding good tutorials, I apologize for the palm hitting your face right now.</span></article></div></section><section><div class=dim><aside><h2>Signed Integers Considered Stupid (Like This Title)</h2><ul></ul></aside><article><p><em>Unrelated note: If you title your article &ldquo;[x] considered harmful&rdquo;, you are a horrible person with no originality. Stop doing it.</em></p><p>Signed integers have always bugged me. I&rsquo;ve seen quite a bit of signed integer overuse in C#, but it is most egregious when dealing with C/C++ libraries that, for some reason, insist on using <code>for(int i = 0; i &lt; 5; ++i)</code>. Why would you <em>ever</em> write that? <code>i</code> cannot possibly be negative and for that matter shouldn&rsquo;t be negative, ever. Use <code>for(unsigned int i = 0; i &lt; 5; ++i)</code>, for crying out loud.</p><p>But really, that&rsquo;s not a fair example. You don&rsquo;t really lose anything using an integer for the i value there because its range isn&rsquo;t large enough. The places where this become stupid are things like using an integer for height and width, or returning a signed integer count. Why on earth would you want to return a negative count? If the count fails, return an unsigned -1, which is just the maximum possible value for your chosen unsigned integral type. Of course, <em>certain people</em> seem to think this is a bad idea because then you will return the largest positive number possible. What if they interpret that as a valid count and try to allocate 4 gigs of memory? Well gee, I don&rsquo;t know, what happens when you try to allocate -1 bytes of memory? In both cases, something is going to explode, and in both cases, its because the person using your code is an idiot. Neither way is more safe than the other. In fact, signed integers cause far more problems then they solve.</p><p>One of the most painfully obvious issues here is that virtually every single architecture in the world uses the <a href="http://en.wikipedia.org/wiki/Two's_complement">two&rsquo;s complement</a> representation of signed integers. When you are using two&rsquo;s complement on an 8-bit signed integer type (a <code>char</code> in C++), the largest positive value is 127, and the largest negative value is -128. That means a signed integer can represent a negative number so large <em>it cannot be represented as a positive number</em>. What happens when you do <code>(char)abs(-128)</code>? It tries to return 128, which overflows back to&mldr; -128. This is the cause of a host of security problems, and what&rsquo;s hilarious is that a lot of people try to use this to fuel their argument that you should use C# or Java or Haskell or some other esoteric language that makes them feel smart. The fact is, any language with fixed size integers has this problem. That means C# has it, Java has it, most languages have it to some degree. This bug doesn&rsquo;t mean you should stop using C++, it means you need to <em>stop using signed integers in places they don&rsquo;t belong</em>. Observe the following code:</p><pre class=language-cpp><code>if (*p == &#39;*&#39;)
  {
    ++p;
    total_width += abs (va_arg (ap, int));
  }</code></pre><p>This is <em>retarded</em>. Why on earth are you interpreting an argument as a signed integer only to then immediately call <code>abs()</code> on it? So a brain damaged programmer can throw in negative values and not blow things up? If it can only possibly be valid when it is a positive number, interpret it as a <em><code>unsigned int</code></em>. Even if someone tries putting in a negative number, they will serve only to make the <code>total_width</code> abnormally large, instead of potentially putting in -128, causing <code>abs()</code> to return -128 and creating a <code>total_width</code> that is far too small, causing a buffer overflow and hacking into your program. And don&rsquo;t go declaring <code>total_width</code> as a signed integer either, because that&rsquo;s just stupid. Using an unsigned integer here closes a potential security hole and makes it even harder for a dumb programmer to screw things up<sup><a href=#foot1>1</a></sup>.</p><p>I can only attribute the vast overuse of <code>int</code> to programmer laziness. <code>unsigned int</code> is just too long to write. Of course, that&rsquo;s what <code>typedef</code>&rsquo;s are for, so that isn&rsquo;t an excuse, so maybe they&rsquo;re worried a programmer won&rsquo;t understand how to put a -1 into an <code>unsigned int</code>? Even if they didn&rsquo;t, you could still cast the <code>int</code> to an <code>unsigned int</code> to serve the same purpose and close the security hole. I am simply at a loss as to why I see <code>int</code>&rsquo;s all over code that could never possibly be negative. If it could never possibly be negative, you are therefore <em>assuming</em> that it won&rsquo;t be negative, so it&rsquo;s a much better idea to just make it <em>impossible</em> for it to be negative instead of giving hackers 200 possible ways to break your program.</p><span style=font-size:80%><sup><a name=foot1>1</a></sup> There's actually another error here in that <code>total_width</code> can overflow even when unsigned, and there is no check for that, but that's beyond the scope of this article.</span></article></div></section><section><div class=dim><aside><h2>C# to C++ Tutorial - Part 3: Classes and Structs and Inheritance (OH MY!)</h2><ul></ul></aside><article><p>[ <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-1-basics-of-syntax/>1</a> · <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-2-pointers/>2</a> · <strong>3</strong> · <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-4-operator-overload/>4</a> <span style=color:#aaa>&#183; 5 &#183; 6 &#183; 7</span> ]</p><p>Classes in C#, like most object-oriented languages, are very similar to their C++ counterparts. They are declared with <code>class</code>, exist between brackets and inherit classes using a colon <code>':'</code>. Note, however, that all classes in C++ <strong>must end with a semicolon!</strong> You will forget this semicolon, and then <em>all the things will break</em>. You can do pretty much everything you can do with a C# class in a C++ class, except that C++ does not have <code>partial</code> classes, and in C++ classes themselves cannot be declared <code>public</code>, <code>protected</code> or <code>private</code>. Both of these features don&rsquo;t exist because they are made irrelevant with how classes are declared in header files.</p><p>In C# you usually just have one code file with the class declared in it along with all the code for all the functions. You can just magically use this class everywhere else and everything is fun and happy with rainbows. As mentioned before, C++ uses header files, and they are heavily integrated into the class system. We saw before how in order to use a function somewhere else, its prototype must first be declared in the header file. This applies to both classes and pretty much everything else. You need to understand that unlike C#, C++ does not have magic dust in its compiler. In C++, it just goes down the list of .cpp files, does a bit of dependency optimization, and then simply compiles each .cpp file by taking all the content from all the headers that are included (including all the headers included in the headers) and pasting it before the actual code from the .cpp file, and compiling. This process is repeated separately for every single code file, and no order inconsistencies are allowed anywhere in the code, the headers, <em>or even the order that the headers are included in</em>. The compiler literally takes every single #include statement as it is and simply replaces it with the code of the header it points to, wherever this happens to be in the code. This can (and this has happened to me) result in certain configurations of header files working even though one header file is actually missing a dependency. For example:<pre class=language-cpp><code>//Rainbow.h

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
}; // DO NOT FORGET THE SEMICOLON
</code></pre><pre class=language-cpp><code>//Unicorn.h

class Unicorn
{
  int magic;
};
</code></pre><pre class=language-cpp><code>//main.cpp

#include &#34;Unicorn.h&#34;
#include &#34;Rainbow.h&#34;

int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre>Compiling main.cpp will succeed in this case, even though Rainbow.h is referencing the <code>Unicorn</code> class without it ever being declared. The reason behind this is what happens when the compiler expands all the includes. Right before compiling main.cpp (after the preprocessor has run), main.cpp looks like this:<pre class=language-cpp><code>//main.cpp

//Unicorn.h

class Unicorn
{
  int magic;
};
//Rainbow.h

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
}; // DO NOT FORGET THE SEMICOLON

int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre>It is now obvious that because Rainbow.h was included after Unicorn.h, the <code>Unicorn</code> reference was resolved since it was declared before <code>Rainbow</code>. However, had we reversed the order of the include files, we would have had an anachronism: an inconsistency in our chronological arrangement. It is very bad practice to construct headers that are dependent on the order in which they are included, so we usually resolve something like this by having Rainbow.h simply include Unicorn.h, and then it won&rsquo;t matter what order they are included in.<pre class=language-cpp><code>//Rainbow.h

#include &#34;Unicorn.h&#34;

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
};
</code></pre>Left as is, however, and we run into a problem. Lets try compiling main.cpp:<pre class=language-cpp><code>//main.cpp

#include &#34;Rainbow.h&#34;
#include &#34;Unicorn.h&#34;

int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre><pre class=language-cpp><code>//main.cpp

//Rainbow.h

#include &#34;Unicorn.h&#34;

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
};
//Unicorn.h

class Unicorn
{
  int magic;
};

int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre><pre class=language-cpp><code>//main.cpp

//Rainbow.h

//Unicorn.h

class Unicorn
{
  int magic;
};

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
};
//Unicorn.h

class Unicorn
{
  int magic;
};

int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre>We&rsquo;ve just declared <code>Unicorn</code> twice! Obviously one way to solve this in our very, very simplistic example is to just remove the spurious <code>#include</code> statement, but this violates the unwritten rule of header files - any header file should be able to be included anywhere in any order regardless of what other header files have been included. This means that, first, any header file should include all the header files that it needs to resolve its dependencies. However, as we see here, that simply makes it extremely likely that a header file will get included 2 or 3 or maybe hundreds of times. What we need is an <strong>include guard</strong>.<pre class=language-cpp><code>//Unicorn.h

#ifndef __UNICORN_H__
#define __UNICORN_H__

class Unicorn
{
  int magic;
};

#endif
</code></pre>Understanding this requires knowledge of the <strong>C Preprocessor</strong>, which is what goes through and <em>processes</em> your code before its compiled. It is very powerful, but right now we only need to know the basics. Any statement starting with <code>#</code> is a preprocessor command. You will notice that <code>#include</code> is itself a preprocessor command, which makes sense, since the preprocessor was replacing those <code>#include</code>&rsquo;s with the code they contained. <code>#define</code> lets you <em>define</em> a constant (or if you want to be technical, an <em>object-like macro</em>). It can be equal to a number or a word or just not be equal to anything and simply be in a <em>defined state</em>. <code>#ifdef</code> and <code>#endif</code> are just an if statement that allows the code inside of it to exist if the given constant is <em>defined</em>. <code>#ifndef</code> simply does the opposite - the code inside only exists if the given constant <em>doesn&rsquo;t</em> exist.</p><p>So, what we do is pick a constant name that probably will never be used in anything else, like <code>__UNICORN_H__</code>, and put in a check to see if it is defined. The first time the header is reached, it won&rsquo;t be defined, so the code inside <code>#ifndef</code> will exist. The next line tells the preprocessor to define<code> __UNICORN_H__</code>, the constant we just checked for. That means that the next time this header is included, <code>__UNICORN_H__</code> will have been defined, and so the code will be skipped over. Observe:<pre class=language-cpp><code>//main.cpp

#include &#34;Rainbow.h&#34;
#include &#34;Unicorn.h&#34;

int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre><pre class=language-cpp><code>//main.cpp

//Rainbow.h

#include &#34;Unicorn.h&#34;

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
};
//Unicorn.h

#ifndef __UNICORN_H__
#define __UNICORN_H__

class Unicorn
{
  int magic;
};

#endif

int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre><pre class=language-cpp><code>//main.cpp

//Rainbow.h

//Unicorn.h

#ifndef __UNICORN_H__
#define __UNICORN_H__

class Unicorn
{
  int magic;
};

#endif

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
};
//Unicorn.h

#ifndef __UNICORN_H__
#define __UNICORN_H__

class Unicorn
{
  int magic;
};

#endif

int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre><pre class=language-cpp><code>//main.cpp

//Rainbow.h

//Unicorn.h

#ifndef __UNICORN_H__
#define __UNICORN_H__

class Unicorn
{
  int magic;
};

#endif

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
};
//Unicorn.h

#ifndef __UNICORN_H__
#endif

int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre><pre class=language-cpp><code>//main.cpp

//Rainbow.h

//Unicorn.h


class Unicorn
{
  int magic;
};


class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
};
//Unicorn.h


int main(int argc, char *argv[]) 
{
  Rainbow rainbow;
}
</code></pre>Our problem is solved! However, note that <code>//Unicorn.h</code> was left in, because it was outside the include guard. It is <strong>absolutely critical</strong> that you put <em>everything</em> inside your include guard (ignoring comments), or it will either not work properly or be extremely inefficient.<pre class=language-cpp><code>//Rainbow.h

#include &#34;Unicorn.h&#34;

#ifndef __RAINBOW_H__ //WRONG WRONG WRONG WRONG WRONG
#define __RAINBOW_H__

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
};

#endif
</code></pre>In this case, the code still <em>compiles</em>, because the include guards prevent duplicate definitions, but its very taxing on the preprocessor that will repeatedly attempt to include <code>Unicorn.h</code> only to discover that it must be skipped over anyway. The preprocessor may be powerful, but it is also <strong>very dumb</strong> and is easily crippled. The thing is slow enough as it is, so try to keep its workload to a minimum by putting your <code>#include</code>&rsquo;s inside the include guard. Also, don&rsquo;t put semicolons on preprocessor directives. Even though almost everything else in the entire language wants semicolons, semicolons in preprocessor directives will either be redundant or considered a syntax error.<pre class=language-cpp><code>//Rainbow.h

#ifndef __RAINBOW_H__
#define __RAINBOW_H__

#include &#34;Unicorn.h&#34; // SMILES EVERYWHERE!

class Rainbow
{
  Unicorn _unicorns[5]; // 5 unicorns dancing on rainbows
};

#endif
</code></pre>Ok, so now we know how to properly use header files, but not how they are used to declare classes. Let&rsquo;s take a class declared in C#, and then transform it into an equivalent prototype in C++.<pre class=language-csharp><code>public class Pegasus : IComparable&lt;Pegasus&gt;
{
  private Rainbow rainbow;
  protected int magic;
  protected bool flying;

  const int ID=10;
  static int total=0;
  const string NAME=&#34;Pegasus&#34;;

  public Pegasus()
  {
    flying=false;
    magic=1;
    IncrementTotal();
  }
  ~Pegasus()
  {
    magic=0;
  }
  public void Fly()
  {
    flying=true;
  }
  private void Land()
  {
    flying=false;
  }
  public static string GetName()
  {
    return NAME;
  }
  private static void IncrementTotal()
  {
    ++total;
  }
  public int CompareTo(Pegasus other)
  {
    return 0;
  }
}
</code></pre><pre class=language-cpp><code>class Pegasus : public IComparable&lt;Pegasus&gt;
{
public:
  Pegasus();
  ~Pegasus();
  void Fly();
  virtual int CompareTo(Pegasus&amp; other);
  
  static const int ID=10;
  static int total;
  static const char* NAME;

  inline static void IncrementTotal() { ++total; }

protected:
  int magic;
  bool flying;

private:
  void Land();
  
  Rainbow rainbow;
};
</code></pre>Immediately, we are introduced to C++&rsquo;s method of dealing with <code>public</code>, <code>protected</code> and <code>private</code>. Instead of specifying it for each item, they are done in groups. The inheritance syntax is identical, and we&rsquo;ve kept the static variables, but now only one of them is being initialized in the class. In C++, you cannot initialize a static variable inside a class unless it is a <code>static const int</code> (or any other integral type). Instead, we will have to initialize <code>total</code> and <code>NAME</code> when we get around to implementing the code for this class. In addition, while most of the functions do not have code, as expected, <code>IncrementTotal</code> does. As an aside, C# does not have <code>static const</code> because it considers it redundant - all constant values are static. C++, however, allows you to declare a <code>const</code> variable that isn&rsquo;t static. While this would be useless in C#, there are certain situations where it is useful in C++.</p><p>If a given function&rsquo;s code doesn&rsquo;t have any dependencies unavailable in the header file the class is declared in, you can define that method in the class prototype itself. However, <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-1-basics-of-syntax/#header-code>as I mentioned before</a>, code in header files runs the danger of being compiled twice. While the compiler is usually good about properly instancing the class, it is usually a good idea to <code>inline</code> any functions defined in the header. Functions that are <code>inline</code>&rsquo;d are embedded inside code that calls them instead of being explicitly called. That means instead of pushing arguments on to the stack and returning, the compiler simply embeds the function inside of the code that called it, like so:<pre class=language-cpp><code>#include &#34;Pegasus.h&#34;

// Before compilation
int main(int argc, char *argv[]) 
{
  Pegasus::IncrementTotal()
}

// After compilation
int main(int argc, char *argv[]) 
{
  ++Pegasus::total;
}
</code></pre>The consequence of this means that the function itself is never actually instantiated. In fact the function might as well not exist - you won&rsquo;t be able to call it from a DLL because the function was simply embedded everywhere that it was used, kind of like a fancy macro. This neatly solves our issue with code in header files, and will be important later on. This also demonstrates how one accesses static variables and functions in a class. Just like before, the C# method of using <code>.</code> no longer works, you must use the Scope Resolution Operator (<code>::</code>) to access static members and functions of a class. This same operator is what allows us to declare the code elsewhere without confusing the compiler.<pre class=language-cpp><code>//Pegasus.cpp

#include &#34;Pegasus.h&#34;

int Pegasus::total = 0;
const char* Pegasus::NAME = &#34;Pegasus&#34;;

Pegasus::Pegasus() : IComparable&lt;Pegasus&gt;(), magic(1), flying(false)
{
  IncrementTotal();
}

Pegasus::~Pegasus()
{
  magic=0;
}

void Pegasus::Fly()
{
  flying=true;
}

void Pegasus::Land()
{
  flying=false;
}

string Pegasus::GetName()
{
  return NAME;
}

int Pegasus::CompareTo(Pegasus other)
{
  return 0;
}
</code></pre>This looks similar to what our C# class looked like, except the functions aren&rsquo;t in the class anymore. <code>Pegasus::</code> tells the compiler what class the function you are defining belongs in, which allows it to assign the class function prototype to the correct implementation, just like it did with normal functions before. Notice that <code>static</code> is not used when defining <code>GetName()</code> - All function decorations (<code>inline</code>, <code>static</code>, <code>virtual</code>, <code>explicit</code>, etc.) are <strong>only allowed on the function prototype</strong>. Note that all these rules apply to static variable initialization as well; both <code>total</code> and <code>NAME</code> are resolved using <code>Pegasus::</code> and <strong>don&rsquo;t</strong> have the <code>static</code> decorator, only their type. Even though we&rsquo;re using <code>const char*</code> instead of <code>string</code>, you can still initialize a constant value using <code>= "string"</code>.</p><p>The biggest difference here is in the constructor. In C#, the only things you bother with in the constructor after the colon is either initializing a subclass or calling another constructor. In C++, you can initialize any subclasses you have along with any variables you have, including passing arguments to whatever constructors your variables might have. Most notably is the ability to initialize constant values, which means you can have a constant integer that is set to a value passed through the constructor, or based off a function call from somewhere else. Unfortunately, C++ traditionally does not allow initializing any variables in any sub-classes, nor does it allow calling any of your own constructors. C++0x <a href=http://en.wikipedia.org/wiki/C%2B%2B0x#Object_construction_improvement>partially resolves this problem</a>, but it is not fully implemented in VC++ or other modern compilers. This blow, however, is mitigated by default arguments in functions (and by extension, constructors), which allows you to do more with fewer functions.</p><p>The order in which variables are constructed is occasionally important if there is an inter-dependency between them. While having such inter-dependencies are generally considered a bad idea, they are sometimes unavoidable, and you can take advantage of a compiler&rsquo;s default behavior of initializing the values in a left to right order. While this behavior isn&rsquo;t technically guaranteed, it is sufficiently reliable for you to take use of it in the occasional exceptional case, but always double-check that the compiler hasn&rsquo;t done crazy optimization in the release version (usually, though, this will just blow the entire program up, so it&rsquo;s pretty obvious).</p><p>Now, C# has another datatype, the <code>struct</code>. This is a limited datatype that cannot have a constructor and is restricted to value-types. It is also passed by-value through functions by default, unlike classes. This is very similar to how structs behaved in C, but have no relation to C++&rsquo;s <code>struct</code> type. In C++, a <code>struct</code> is <em>completely identical</em> to a <code>class</code> in every way, save for one minor detail: all members of a <code>class</code> are <code>private</code> by default, while all members of a <code>struct</code> are <code>public</code> by default. That&rsquo;s it. You can take <em>any</em> <code>class</code> and replace it with <code>struct</code> and the only thing that will change is the default access modifier.</p><p>Even though there is no direct analogue to C#&rsquo;s <code>struct</code>, there is an implicit equivalent. If a <code>class</code> or <code>struct</code> (C++ really doesn&rsquo;t care) meets the requirements of a traditional C <code>struct</code> (no constructor, only basic data types), then it&rsquo;s treated as Plain Old Data, and you are then allowed to skip the constructor and initialize its contents using the special bracket initialization that was <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-2-pointers/#struct-init>touched on before</a>. Yes, you can initialize constant variables using that syntax too.</p><p>One thing I&rsquo;ve skipped over is the <code>virtual</code> code decorator in the C++ prototype of <code>Pegasus</code>, which is not actually necessary, because the function is already attempting to override another virtual function declared in <code>IComparable</code>, which implicitly makes it virtual. However, in C#, <code>IComparable</code> is implemented as an <code>interface</code>, which is not present in C++. Of course, if you really think about it, an <code>interface</code> is kind of like a normal class, just with all <code>abstract</code> methods (ignore the inheritance issues with this for now). So, we could rewrite the C# implementation of <code>IComparable</code> as a class with abstract methods:<pre class=language-csharp><code>public class IComparable&lt;T&gt;
{
  public abstract int CompareTo(T other);
}
</code></pre>As it turns out, this has a direct C++ analogue:<br><pre class=language-csharp><code>template&lt;class T&gt;
class IComparable
{
public:
  virtual int CompareTo(T other)=0;
}
</code></pre>This <code>virtual</code> function, instead of being implemented, has an <code>=0</code> on the end of it. That makes the function <em>pure virtual</em>, which is just another way of saying <code>abstract</code>. So the C++ version of <code>abstract</code> is a pure virtual function, and a C++ version of interfaces is just a class made entirely out of pure virtual functions. Just as C# prevents you from instantiating an <code>abstract class</code> or <code>interface</code>, C++ considers any class that either declares or inherits pure virtual functions without giving them code as an <code>abstract class</code> that cannot be instantiated. Unfortunately C++ does not have anything like <code>sealed</code>, <code>override</code>, etc., so you are on your own there. Keep in mind that <code>public IComparable&lt;T></code> could easily be replaced with <code>protected</code> or <code>private</code> for more control.</p><p>The reason C# has interfaces at all is because C# only allows you to inherit a single class, regardless of whether or not its abstract. If its got code, you can only inherit it once. Interfaces, however, have no code, and so C# lets you pile them on like candy. This isn&rsquo;t done in C++, because C++ supports multiple inheritance. In C++ you can have any class inherit any other class, no matter what, but you can only instantiate a class if it provides implementations for all pure virtual functions somewhere along its inheritance line. Unfortunately, there are a lot of caveats about multiple inheritance, the most notorious being <a href=http://en.wikipedia.org/wiki/Diamond_problem>the Diamond Problem</a>.</p><p>Let&rsquo;s say you have a graphics engine that has an <code>Image</code> class, and that image class inherits from an <code>abstract class</code> that holds its position. Obviously, any image on the screen is going to have a position. Then, let&rsquo;s take a physics engine, with a basic object that also inherits from an abstract class that holds its position. Obviously any physics object must have a position. So, what happens when you have a game object that is both an image and a physics object? Since the image and the physics object are in fact the same thing, both of them must have the same position at all times, but both inherit the abstract class storing position separately, resulting in two positions. Which one is the right position? When you call SetPosition, which position are you talking about?</p><p>Virtual inheritance was introduced as an attempt to solve this problem. It works by creating a single instance of a derived class for the entire inheritance change, such that both the physics object and the image share the same position, as they are supposed to. Unfortunately, it can&rsquo;t resolve all the ambiguities, and it introduces a whole truckload of new problems. It has a nasty habit of being unable to resolve its own virtual functions properly and introducing all sorts of horrible weirdness. Most incredibly bizarre is a virtually inherited class&rsquo;s constructor - it must be initialized in the last class in the inheritance chain, and is one of the first classes to get its constructor called, regardless of where it might be in the hierarchy. It&rsquo;s destructor order is equally as bizarre. Virtual inheritance is sometimes useful for certain small utility classes that must be shared through a wide variety of situations, like a flag class. As a rule of thumb, you should only use virtual inheritance in a class that either relies on the default constructor or only offers a constructor that takes no arguments, and has no superclasses. This allows you to just slap the <code>virtual</code> keyword on and forget about all the wonky constructor details.<pre class=language-cpp><code>class Pegasus : virtual IComparable&lt;Pegasus&gt;</code></pre>If you ever think you need to use virtual inheritance on something more complicated, your code is broken and you need to rethink your program&rsquo;s architecture (and the compiler probably won&rsquo;t be able to do it properly anyway). On a side-note, the constructors for any given object are called from the top down. That is, when your object&rsquo;s constructor is called, it immediately calls all the constructors for all it&rsquo;s superclasses, usually before even doing any variable initialization, and then those object constructors immediately call all <em>their</em> superclass constructors, and so on until the first line of code executed in your program is whatever the topmost class was. This then filters down until control is finally returned to your original constructor, such that any constructor code is only executed <em>after</em> all of its base classes have been constructed. The exact reverse happens for destructors, with the lowest class destructor being executed first, and after its finished, the destructors for all its base classes are called, such that a class destructor is always called while all of its base classes still exist.</p><p>Hopefully you are familiar with C#&rsquo;s <code>enum</code> keyword. While it used to be far more limited, it has now been extended to such a degree it is <em>identical</em> to C++, even the syntax is the same. The only difference between the two is that the C++ version can&rsquo;t be declared <code>public</code>, <code>protected</code> or <code>private</code> and needs to have a semicolon on the end (like everything else). Like in C#, enums, classes and structs can be embedded in classes, except in C++ they can also be embedded in structs (because structs are basically classes with a different name). Also, C++ allows you to declare an <code>enum</code>/<code>class</code>/etc. and a variable inside the class at the same time using the following syntax:<pre class=language-cpp><code>class Pegasus
{
  enum Count { Uno=2, Dos, Tres, Quatro, Cinco } variable;
  enum { Uno=2, Dos, Tres, Quatro, Cinco } var2; //When used to immediately declare a variable, enums can be anonymous
}

//Same as above
class Pegasus
{
  enum Count { Uno=2, Dos, Tres, Quatro, Cinco }; //cannot be anonymous

  Count variable;
  Count var2;
}
</code></pre>Unions are exclusive to C++, and are a special kind of data structure where each element occupies the same address. To understand what that means, let&rsquo;s look at an example:<pre class=language-cpp><code>union //Unions are usually anonymous, but can be named
{
  struct { // The anonymity of this struct exposes its internal members.
    __int32 low;
    __int32 high;
  }
  __int64 full;
}
</code></pre>union<br><code>__int32</code> and <code>__int64</code> are simply explicitly declaring 32-bit and 64-bit integers. This <code>union</code> allows us to either set an entire 64-bit integer, or to only set its low or high portion. This happens because the data structure is laid out as follows:</p><a href=http://img705.imageshack.us/img705/2389/p3union1.png target=_blank><img src=http://img705.imageshack.us/img705/2389/p3union1.png alt="Integer Union Layout"></a><p>Both <code>low</code> and <code>full</code> are mapped to the <em>exact same place in memory</em>. The only difference is that <code>low</code> is a 32-bit integer, so when you set that to 0, only the first four bytes are set to zero. <code>high</code> is pointing to a location in memory that is exactly 4 bytes in front of <code>low</code> and <code>full</code>. So, if <code>low</code> and <code>full</code> were located at <code>0x000F810</code>, <code>high</code> would be located at <code>0x000F814</code>. Setting <code>high</code> to zero sets the last four bytes to zero, but doesn&rsquo;t touch the first four. Consequently, if you set <code>high</code> to 0, reading <code>full</code> would always return the same value as <code>low</code>, since it would essentially be constrained to a 32-bit integer. Unions, however, do not have to have matching memory layouts:<pre class=language-cpp><code>union //Unions are usually anonymous, but can be named
{
  char pink[5]
  __int32 fluffy;
  __int64 unicorns;
}
</code></pre>The layout of this union is:</p><a href=http://img851.imageshack.us/img851/1847/p3union2.png target=_blank><img src=http://img851.imageshack.us/img851/1847/p3union2.png alt="Jagged Union Layout"></a><p>Any unused space is simply ignored. This same rule would apply for any structs being used to group data. The size of the <code>union</code> is simply the size of its largest member. Setting all 5 elements of <code>pink</code> here would result in <code>fluffy</code> being equal to zero, and only the last 24-bits (or last 3 bytes) of <code>unicorns</code> be untouched. Likewise, setting <code>fluffy</code> to zero would zero out the first 4 elements in <code>pink</code> (indexes 0-3), leaving the 5<sup>th</sup> untouched. These unions are often used in performance critical areas where a single function must be able to recieve many kinds of data, but will only ever recieve a single group of data at a time, and so it would be more efficient to map all the possible memory configurations to a single data structure that is large enough to hold the largest group. Here is a real world example:<pre class=language-cpp><code>struct __declspec(dllexport) cGUIEvent
{
  cGUIEvent() { memset(this,0,sizeof(cGUIEvent)); }
  cGUIEvent(unsigned char _evt, const cVecT&lt;int&gt;* mousecoords, unsigned char _button, bool _pressed) : evt(_evt), subevt(0), mousecoords(mousecoords), button(_button), pressed(_pressed) {}
  cGUIEvent(unsigned char _evt, const cVecT&lt;int&gt;* mousecoords, unsigned short _scrolldelta) : evt(_evt), subevt(0), mousecoords(mousecoords), scrolldelta(_scrolldelta) {}
  union
 {
  struct
  {
   unsigned char evt;
   unsigned char subevt;
  };
  unsigned short realevt;
 };

  union
  {
    struct { const cVecT&lt;int&gt;* mousecoords; unsigned char button; bool pressed; };
    struct { const cVecT&lt;int&gt;* mousecoords; short scrolldelta; };
    struct { //the three ctrl/shift/alt bools (plus a held bool) here are compressed into a single byte
      bool down;
      unsigned char keycode; //only used by KEYDOWN/KEYUP
      char ascii; //Only used by KEYCHAR
      wchar_t unicode; //Only used by KEYCHAR
      char sigkeys;
    };
    struct { float value; short joyaxis; }; //JOYAXIS
    struct { bool down; short joybutton; }; //JOYBUTTON*
  };
};
</code></pre>Here, the GUI event is mapped to memory according to the needs of the event that it is representing, without the need for complex inheritance or wasteful memory usage. Unions are indispensable in such scenarios, and as a result are very common in any sort of message handling system.</p><p>One strange decorator that has gone unexplained in the above example is the <code>__declspec(dllexport)</code> class decorator. When creating a windows DLL, if you want anything to be usable by something inheriting the DLL, you have to export it. In VC++, this can be done with a <a href="http://msdn.microsoft.com/en-us/library/28d6s79h(v=vs.80).aspx">module definition file (.def)</a>, which is useful if you&rsquo;ll be using GetProcAddress manually, but if you are explicitly linking to a DLL, <code>__declspec(dllexport)</code> automatically exports the function for you when placed on a function. When placed on a class, it automatically exports the entire class. However, for anyone to utilize it, they have to have the header file. This arises to DLLs being distributed as DLLs, linker libraries (.lib), and sets of header files, usually in an &ldquo;include&rdquo; directory. In certain cases, only some portions of your DLL will be accessible to the outside, and so you&rsquo;ll want two collections of header files - outside header files and internal ones that no one needs to know about. Consequently, utilizing a large number of C++ DLLs usually involves substantial organization of a whole lot of header files.</p><p>Due to the compiler-specific nature of DLL management, they will be covered in <a href=#>Part 6</a>. For now, its on to operator overloading, copy semantics and move semantics!</p><p><a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-4-operator-overload/>Part 4: Operator Overload</a></p><iframe width=640 height=390 src="http://www.youtube-nocookie.com/embed/eWM2joNb9NE?rel=0&hd=1" frameborder=0 allowfullscreen></iframe></article></div></section><section><div class=dim><aside><h2>The Problem of Vsync</h2><ul></ul></aside><article><p>If you were to write directly to the screen when drawing a bouncing circle, you would run into some problems. Because you don&rsquo;t do any buffering, your user might end up with a quarter circle drawn for a frame. This can be solved through Double Buffering, which means you draw the circle on to a backbuffer, then &ldquo;flip&rdquo; (or copy) the completed image on to the screen. This means you will only ever send a completely drawn scene to the monitor, but you will still have tearing issues. These are caused by trying to update the monitor outside of its refresh rate, meaning you will have only finished drawing half of your new scene over the old scene in the monitor&rsquo;s video buffer when it updates itself, resulting in half the scanlines on the screen having the new scene and half still having the old scene, which gives the impression of tearing.</p><p>This can be solved with Vsync, which only flips the backbuffer right before the screen refreshes, effectively locking your frames per second to the refresh rate (usually 60 Hz or 60 FPS). Unfortunately, Vsync with double buffering is implemented by simply locking up the entire program until the next refresh cycle. In DirectX, this problem is made even worse because the API locks up the program with a 100% CPU polling thread, sucking up an entire CPU core just waiting for the screen to enter a refresh cycle, often for almost 13 milliseconds. So your program sucks up an entire CPU core when 90% of the CPU isn&rsquo;t actually doing anything but waiting around for the monitor.</p><p>This waiting introduces another issue - Input lag. By definition any input given during the current frame can only come up when the next frame is displayed. However, if you are using vsync and double buffering, the current frame on the screen was the LAST frame, and the CPU is now twiddling its thumbs until the monitor is ready to display the frame that you have <em>already finished rendering</em>. Because you already rendered the frame, the input now has to wait until the end of the frame being displayed on the screen, at which point the frame that was already rendered is flipped on to the screen and your program finally realizes that the mouse moved. It now renders yet another frame taking into account this movement, but because of Vsync that frame is blocked until the next refresh cycle. This means, if you were to press a key just as a frame was put up on the monitor, you would have <em>two full frames of input lag</em>, which at 60 FPS is <strong>33 ms</strong>. I can ping a server 20 miles away with a ping of 21 ms. You might as well be in the next city with that much latency.</p><p>There is a solution to this - Triple Buffering. The idea is a standard flip mechanism commonly used in dual-thread lockless synchronization scenarios. With two backbuffers, the application can write to one and once its finished, tell the API and it will mark it for flipping to the front-buffer. Then the application starts drawing on the second, after waiting for any flipping operation to finish, and once its done, marks that for flipping to the front-buffer and starts drawing on the first again. This way, the application can draw 2000 frames a second, but only 60 of those frames actually get flipped on to the monitor using what is essentially a lockless flipping mechanism. Because the application is now effectively rendering 2000 frames per second, there is no more input lag. Problem Solved.</p><p>Except not, because DirectX implements Triple Buffering in the most useless manner possible. DirectX just treats the extra buffer as a chain, and rotates through the buffers as necessary. The only advantage this has is that it avoids waiting for the backbuffer copy operation to finish before writing again, which is completely useless in an era where said copy operation would have to be measured in microseconds. Instead, it simply ensures that vsync blocks the program, which doesn&rsquo;t solve the input issue at all.</p><p>However, there is a flag, <code>D3DPRESENT_DONOTWAIT</code>, that forces vsync to simply return an error if the refresh cycle isn&rsquo;t available. This would allow us to implement a hack resembling what triple buffering should be like by simply rolling our own polling loop and re-rendering things in the background on the second backbuffer. Problem solved!</p><p><strong>Except not</strong>. It turns out the Nvidia and Intel don&rsquo;t bother implementing this flag, forcing Vsync to block no matter what you do, and to make matters worse, this feature doesn&rsquo;t have an entry in D3DCAPS9, meaning the DirectX9 API just assumes that it exists, and <em>there is no way to check if it is supported</em>. Of course, don&rsquo;t complain about this to anyone, because of the 50% of people who asked about this who weren&rsquo;t simply ignored, almost all of them were immediately accused of bad profiling, and that the <code>Present()</code> function couldn&rsquo;t possibly be blocking with the flag on. I question the wisdom of people who ignore the fact that the code executed its main loop 2000 times with vsync off and 60 times with it on and somehow come to the conclusion that <code>Present()</code> isn&rsquo;t blocking the code.</p><p>Either way, we&rsquo;re kind of screwed now. Absolutely no feature in DirectX actually does what its supposed to do, so there doesn&rsquo;t seem to be a way past this input lag.</p><p>There is, however, another option. Clever developers would note that to get around vsync&rsquo;s tendency to eat up CPU cycles like a pig, one could introduce a <code>Sleep()</code> call. So long as you left enough time to render the frame, you could recover a large portion of the wasted CPU. A reliable way of doing this is figuring out how long the last frame took to render, then subtracting that from the FPS you want to enforce and sleep in the remaining time. By enforcing an FPS of something like 80, you give yourself a bit of breathing room, but end up finishing rendering the frame around the same time it would have been presented anyway.</p><p>By timing your updates very carefully, you can execute a <code>Sleep()</code> call, <em>then</em> update all the inputs, <em>then</em> render the scene. This allows you to cut down the additional lag time by nearly 50% in ideal conditions, almost <em>completely eliminating</em> excess input lag. Unfortunately, if your game is already rendering at or below 100 FPS, it takes you 10 milliseconds to render a frame, allowing you only 2.5 milliseconds of extra time to look for input, which is of limited usefulness. This illustrates why Intel and Nvidia are unlikely to care about <code>D3DPRESENT_DONOTWAIT</code> - modern games will never render fast enough for substantial input lag reduction.</p><p>Remember when implementing the Yield that the amount of time it takes to render the frame should be the time difference between the two render calls, minus the amount of time spent sleeping, minus the amount of time <code>Present()</code> was blocking.</p></article></div></section><section><div class=dim><aside><h2>C# to C++ Tutorial - Part 2: Pointers Everywhere!</h2><ul></ul></aside><article><p>[ <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-1-basics-of-syntax/>1</a> · <strong>2</strong> · <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-3-classes-and/>3</a> · <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-4-operator-overload/>4</a> <span style=color:#aaa>· 5 · 6 · 7</span> ]</p><p>We still have a lot of ground to cover on pointers, but before we do, we need to address certain conceptual frameworks missing from C# that one must be intimately familiar with when moving to C++.</p><p>Specifically, in C# you mostly work with the <strong>Heap</strong>. The heap is not difficult to understand - its a giant lump of memory that you take chunks out of to allocate space for your classes. Anything using the <code>new</code> keyword is allocated on the heap, which ends up being almost everything in a C# program. However, the heap isn&rsquo;t the only source of memory - there is also the <strong>Stack</strong>. The Stack is best described as what your program lives inside of. I&rsquo;ve said before that everything takes up memory, and yes, that includes your program. The thing is that the Heap is inherently dynamic, while the Stack is inherently <em>fixed</em>. Both can be re-purposed to do the opposite, but trying to get the Stack to do dynamic allocation is extremely dangerous and is almost guaranteed to open up a mile-wide security hole.</p><p>I&rsquo;m going to assume that a C# programmer knows what a <a href=http://en.wikipedia.org/wiki/Stack_(data_structure)>stack</a> is. All you need to understand is that absolutely every single piece of data that isn&rsquo;t allocated on the heap is pushed or popped off your program&rsquo;s stack. That&rsquo;s why most debuggers have a &ldquo;stack&rdquo; of functions that you can go up and down. Understanding the stack in terms of how many functions you&rsquo;re inside of is ok, but in reality, there are also variables declared on the stack, including every single parameter passed to a function. It is important that you understand how variable scope works so you can take advantage of declaring things on the stack, and know when your stack variables will simply vanish into nothingness. This is where <code>{</code> and <code>}</code> come in.</p><p><pre class=language-cpp><code>int main(int argc, char *argv[])
{
  int bunny = 1;
  
  {
    int carrot=3;
    int lettuce=8;
    bunny = 2; // Legal
  }

  //carrot=2; //Compiler error: carrot does not exist
  int carrot = 3; //Legal, since the other carrot no longer exists
  
  {
    int lettuce = 0;

    { 
       //int carrot = 1; //Compiler error: carrot already defined
       int grass = 9;
       
       bunny = grass; //Still legal
       bunny = carrot; // Also legal
    }
    
    //bunny = grass; //Illegal
    bunny = lettuce; //Legal
  }
  
  //bunny = lettuce; //Illegal
}
</code></pre><code>{</code> and <code>}</code> define <strong>scope</strong>. Anything declared inside of them ceases to exist outside, but is still accessible to any additional layers of scope declared inside of them. This is a way to see your program&rsquo;s stack in action. When <code>bunny</code> is declared, its pushed on to the stack. Then we enter our first scope area, where we push <code>carrot</code> and <code>lettuce</code> on to the stack and set <code>bunny</code> to 2, which is legal because <code>bunny</code> is still on the stack. When the scope is then closed, however, anything declared inside the scope is popped from the stack <em>in the exact opposite order it was pushed on</em>. Unfortunately, compiler optimization might change that order behind the scenes, so <strong>don&rsquo;t rely on it</strong>, but it should be fairly consistent in debug builds. First <code>lettuce</code> is de-allocated (and its destructor called, if it has one), then <code>carrot</code> is de-allocated. Consequently, trying to set <code>carrot</code> to 2 outside of the scope will result in a compiler error, because it doesn&rsquo;t exist anymore. This means we can now declare an entirely new integer variable that is also called <code>carrot</code>, without causing an error. If we visualize this as a stack, that means <code>carrot</code> is now directly above <code>bunny</code>. As we enter a new scope area, <code>lettuce</code> is then put on top of <code>carrot</code>, and then <code>grass</code> is put on top of <code>lettuce</code>. We can still assign either <code>lettuce</code> or <code>carrot</code> to <code>bunny</code>, since they are all on the stack, but once we leave this inner scope, <code>grass</code> is popped off the stack and no longer exists, so any attempt to use it causes an error. <code>lettuce</code>, however, is still there, so we can assign <code>lettuce</code> to <code>bunny</code> before the scope closes, which pops <code>lettuce</code> off the stack.</p><p>Now the only things on the stack are <code>bunny</code> and <code>carrot</code>, in that order (if the compiler hasn&rsquo;t moved things around). We are about to leave the function, and the function is also surrounded by <code>{</code> and <code>}</code>. This is because a function is, itself, a scope, so that means all variables declared inside of that scope are also destroyed in the order they were declared in. First <code>carrot</code> is destroyed, then <code>bunny</code> is destroyed, and then the function&rsquo;s parameters <code>argc</code> and <code>argv</code> are destroyed (however the compiler can push those on to the stack in whatever order it wants, so we don&rsquo;t know the order they get popped off), until finally the <em>function itself</em> is popped off the stack, which returns program flow to whatever called it. In this case, the function was <code>main</code>, so program flow is returned to the parent operating system, which does cleanup and terminates the process.</p><p>You can declare anything that has a size determined at compile time on the stack. This means if you have an array that has a constant size, you can declare it on the stack:</p><p><pre class=language-cpp><code>int array[5]; //Array elements are not initialized and therefore are undefined!
int array[5] = {0,0,0,0,0}; //Elements all initialized to 0
//int array[5] = {0}; // Compiler error - your initialization must match the array size
</code></pre>You can also let the compiler infer the size of the array:</p><p><pre class=language-cpp><code>int array[] = {1,2,3,4}; //Declares an array of 4 ints on the stack initialized to 1,2,3,4
</code></pre>Not only that, but you can declare class instances and other objects on the stack.</p><p><pre class=language-cpp><code>Class instance(arg1, arg2); //Calls a constructor with 2 arguments
Class instance; //Used if there are no arguments for the constructor
//Class instance(); //Causes a compiler error! The compiler will think its a function.
</code></pre><a id=struct-init>In fact</a>, if you have a very simple data structure that uses only default constructors, you can use a shortcut for initializing its members. I haven&rsquo;t gone over classes and structs in C++ yet (See <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-3-classes-and/>Part 3</a>), but here is the syntax anyway:</p><p><pre class=language-cpp><code>struct Simple
{
  int a;
  int b;
  const char* str;
};

Simple instance = { 4, 5, &#34;Sparkles&#34; };
//instance.a is now 4
//instance.b is now 5
//instance.str is now &#34;Sparkles&#34;
</code></pre>All of these declare variables on the stack. C# actually does this with trivial datatypes like <code>int</code> and <code>double</code> that don&rsquo;t require a <code>new</code> statement to allocate, but otherwise forces you to use the Heap so its garbage collector can do the work.</p><p>Wait a minute, stack variables automatically destroy themselves when they go out-of-scope, but how do you delete variables allocated from the Heap? In C#, you didn&rsquo;t need to worry about this because of Garbage Collection, which everyone likes because it reduces memory leaks (but even I have still managed to cause a memory leak in C#). In C++, you must explicitly delete all your variables declared with the <code>new</code> keyword, and you must keep in mind which variables were declared as arrays and which ones weren&rsquo;t. In both C# and C++, there are two uses of the <code>new</code> keyword - instantiating a single object, and instantiating an array. In C++, there are also two uses of the <code>delete</code> keyword - deleting a single object and deleting an array. <strong>You cannot mix up <code>delete</code> statements!</strong></p><p><pre class=language-cpp><code>int* Fluffershy = new int();
int* ponies = new int[10];

delete Fluffershy; // Correct
//delete ponies; // WRONG, we should be using delete [] for ponies
delete [] ponies; // Just like this
//delete [] Fluffershy; // WRONG, we can&#39;t use delete [] on Fluffershy because we didn&#39;t
                        // allocate it as an array.

int* one = new int[1];

//delete one; // WRONG, just because an array only has one element doesn&#39;t mean you can
              // use the normal delete!
delete [] one; // You still must use delete [] because you used new [] to allocate it.
</code></pre>As you can see, it is much easier to deal with stack allocations, because they are automatically deallocated, even when the function terminates unexpectedly. <code>[std::auto_ptr](http://www.cplusplus.com/reference/std/memory/auto_ptr/)</code> takes advantage of this by taking ownership of a pointer and automatically deleting it when it is destroyed, so you can allocate the <code>auto_ptr</code> on the stack and benefit from the automatic destruction. However, in <code>C++0x</code>, this has been superseded by <code>[std::unique_ptr](http://msdn.microsoft.com/en-us/library/ee410601.aspx)</code>, which operates in a similar manner but uses some complex move semantics introduced in the new standard. I won&rsquo;t go into detail about how to use these here as its out of the scope of this tutorial. Har har har.</p><p>For those of you who like throwing exceptions, I should point out common causes of memory leaks. The most common is obviously just flat out forgetting to delete something, which is usually easily fixed. However, consider the following scenario:</p><p><pre class=language-cpp><code>void Kenny()
{
  int* kenny = new int();
  throw &#34;BLARG&#34;;
  delete kenny; // Even if the above exception is caught, this line of code is never reached.
}

int main(int argc, char* argv[])
{
  try {
  Kenny();
  } catch(char * str) { 
    //Gotta catch&#39;em all.
  }
  return 0; //We&#39;re leaking Kenny! o.O
}
</code></pre>Even this is fairly common:</p><p><pre class=language-cpp><code>int main(int argc, char* argv[])
{
  int* kitty = new int();

  *kitty=rand();
  if(*kitty==0)
    return 0; //LEAK
  
  delete kitty;
  return 0;
}
</code></pre>These situations <em>seem</em> obvious, but they will happen to you once the code becomes enormous. This is one reason you have to be careful when inside functions that are very large, because losing track of <code>if</code> statements may result in you forgetting what to delete. A good rule of thumb is to make sure you delete everything whenever you have a return statement. However, the opposite can also happen. If you are too vigilant about deleting everything, you might delete something you never allocated, which is just as bad:</p><p><pre class=language-cpp><code>int main(int argc, char* argv[])
{
  int* rarity = new int();
  int* spike;

  if(rarity==NULL)
  {
    spike=new int();
  }
  else
  {
    delete rarity;
    delete spike; // Suddenly, in an alternate dimension, earth ceased to exist
    return 0;
  }
  
  delete rarity; // Since this only happens if the allocation failed and returned a NULL
                 // pointer, this will also blow up.
  delete spike;
  return 0;
}
</code></pre>Clearly, one must be careful when dealing with allocating and destroying memory in C++. Its usually best to encapsulate as much as possible in classes that automate such things. But wait, what about that <code>NULL</code> pointer up there? Now that we&rsquo;re familiar with memory management, we&rsquo;re going to dig into pointers again, starting with the <code>NULL</code> pointer.</p><p>Since a pointer points to a piece of memory that&rsquo;s somewhere between 0 and 4294967295, what happens if its pointing at 0? Any pointer to memory location 0 <strong>is always invalid</strong>. All you need to know is that the operating system does some magic voodoo to ensure that any attempted access of memory location 0 will always throw an error, no matter what. 1, 2, 3, and any other double or single digit low numbers are also always invalid. <code>0xfdfdfdfd</code> is what the VC++ debugger sets uninitialized memory to, so that pointer location is also always invalid. A pointer set to 0 is called a <strong>Null Pointer</strong>, and is usually used to signify that a pointer is empty. Consequently if an allocation function fails, it tends to return a null pointer. Null pointers are returned when the operation failed and a valid pointer cannot be returned. Consequently, you may see this:</p><p><pre class=language-cpp><code>int main(int argc, char* argv[])
{
  int* blink = new int();
  if(blink!=0) delete blink;
  blink=0;
  return 0;
}
</code></pre>This is known as a <strong>safe deletion</strong>. It ensures that you only delete a pointer if it is valid, and once you delete the pointer you set the pointer to 0 to signify that it is invalid. Note that <code>NULL</code> is defined as 0 in the standard library, so you could also say <code>blink = NULL</code>.</p><p>Since pointers are just integers, we can do <strong>pointer arithmetic</strong>. What happens if you add 1 to a pointer? If you think of pointers as just integers, one would assume it would simply move the pointer forward a single byte.</p><div class=imgwrap style=max-width:486px><a href=/img/tut2.png target=_blank><img src=/img/tut2.png alt="Moving a Pointer 1 byte" width=100%></a></div><p>This isn&rsquo;t what happens. Adding 1 to a pointer of type <code>integer</code> results in the pointer moving forward 4 bytes.</p><div class=imgwrap style=max-width:486px><a href=/img/tut3.png target=_blank><img src=/img/tut3.png alt="Moving a Pointer 4 bytes" width=100%></a></div><p><strong>Adding or subtracting an integer $i$ from a pointer moves that pointer $i\cdot n$ bytes, where $n$ is the size, in bytes, of the pointer&rsquo;s type</strong>. This results in an interesting parallel - adding or subtracting from a pointer is the same as treating the pointer as an array and accessing it via an index.</p><p><pre class=language-cpp><code>int main(int argc, char* argv[])
{
  int* kitties = new int[14];
  int* a = &amp;kitties[7];
  int* b = kitties+7; //b is now the same as a
  int* c = &amp;a[4];
  int* d = b+4; //d is now the same as c
  int* e = &amp;kitties[11];
  int* f = kitties+11; 
  //c,d,e, and f now all point to the same location
}
</code></pre>So pointer arithmetic is identical to accessing a given index and taking the address. But what happens when you try to add two pointers together? Adding two pointers together is undefined because it tends to produce total nonsense. <em>Subtracting</em> two pointers, however, is defined, provided you subtract a smaller pointer from a larger one. The reason this is allowed is so you can do this:</p><p><pre class=language-cpp><code>int main(int argc, char* argv[])
{
  int* eggplants = new int[14];
  int* a = &amp;eggplants[7];
  int* b = eggplants+10;
  int diff = b-a; // Diff is now equal to 3
  a += (diff*2); // adds 6 to a, making it point to eggplants[13]
  diff = a-b; // diff is again equal to 3
  diff = a-eggplants; //diff is now 13
  ++a; //The increment operator is valid on pointers, and operates the same way a += 1 would
  // So now a points to eggplants[14], which is not a valid location, but this is still
  // where the &#34;end&#34; of the array technically is.
  diff = a-eggplants; // Diff now equals 14, the size of the array
  --b; // Decrement works too
  diff = a-b; // a is pointing to index 14, b is pointing to 9, so 14-9 = 5. Diff is now 5.
  return 0;
}
</code></pre>There is a mistake in the code above, can you spot it? I used a <em>signed</em> <code>integer</code> to store the difference between the two pointers. What if one pointer was above 2147483647 and the other was at 0? The difference would overflow! Had I used an unsigned integer to store the difference, I&rsquo;d have to be really damn sure that the left pointer was larger than the right pointer, or the negative value would <em>also</em> overflow. This complexity is why you have to goad windows into letting your program deal with pointer sizes over 2147483647.</p><p>In addition to arithmetic, one can compare two pointers. We already know we can use <code>==</code> and <code>!=</code>, but we can also use <code>&lt; > &lt;=</code> and <code>>=</code>. While you can get away with comparing two completely unrelated pointers, these comparison operators are usually used in a context like the following:</p><p><pre class=language-cpp><code>int main(int argc, char* argv[])
{
  int* teapots = new int[15];
  int* end = teapots+15;
  for(int* s = teapots; s&lt;end; ++s)
    *s = 0;
  return 0;
}
</code></pre>Here the for loop increments the pointer itself rather than an index, until the pointer reaches the end, at which point it terminates. But, what if you had a pointer that didn&rsquo;t have any type at all? <code>void*</code> is a legal pointer type, that any pointer type can be implicitly converted to. You can also explicitly cast <code>void*</code> to any pointer type you want, which is why you are allowed to explicitly cast any pointer type to another pointer type (<code>int* p; short* q = (short*)p;</code> is entirely legal). Doing so, however, is obviously dangerous. <code>void*</code> has its own problems, namely, how big is it? The answer is, you don&rsquo;t know. <strong>Any attempt to use any kind of pointer arithmetic with a <code>void*</code> pointer will cause a compiler error</strong>. It is most often used when copying generic chunks of memory that only care about size in bytes, and not what is actually contained in the memory, like <code>memcpy()</code>.</p><p><pre class=language-cpp><code>int main(int argc, char* argv[])
{
  int* teapots = new int[15];
  void* p = (void*)teapots;
  p++; // compiler error
  unsigned short* d = (unsigned short*)p;
  d++; // No compiler error, but you end up pointing to half an integer
  d = (unsigned short*)teapots; // Still valid
  return 0;
}
</code></pre>Now that we know all about pointer manipulation, we need to look at pointers to pointers, and to anchor this in a context that actually makes sense, we need to look at how C++ does <strong>multidimensional arrays</strong>. In C#, <a href="http://msdn.microsoft.com/en-us/library/2yd9wwz4(v=vs.71).aspx">multidimensional arrays</a> look like this:</p><p><pre class=language-csharp><code>int[,] table = new int[4,5];
</code></pre>C++ has a different, but fairly reasonable stack-based syntax. When you want to declare a multidimensional array <em>on the heap</em>, however, things start getting <em>weird</em>:</p><p><pre class=language-cpp><code>int unicorns[5][3]; // Well this seems perfectly reasonable, I wonder what-
  int (*cthulu)[50] = new int[10][50]; // OH GOD GET IT AWAY GET IT AWAAAAAY...!
  int c=5;
  int (*cthulu)[50] = new int[c][50]; // legal
  //int (*cthulu)[] = new int[10][c]; // Not legal. Only the leftmost parameter
                                      // can be variable
  //int (*cthulu)[] = new int[10][50]; // This is also illegal, the compiler is not allowed
                                       // to infer the constant length of the array.
</code></pre>Why isn&rsquo;t the multidimensional array here just an <code>int**</code>? Clearly if <code>int* x</code> is equivalent to <code>int x[]</code>, shouldn&rsquo;t <code>int** x</code> be equivalent to <code>int x[][]</code>? Well, it <em>is</em> - just look at the <code>main()</code> function, its got a multidimensional array in there that can be declared as just <code>char** argv</code>. The problem is that there are two kinds of multidimensional arrays - <strong>square</strong> and <strong>jagged</strong>. While both are accessed in identical ways, how they work is fundamentally different.</p><p>Let&rsquo;s look at how one would go about allocating a 3x5 square array. We can&rsquo;t allocate a 3x5 chunk out of our computer&rsquo;s memory, because memory isn&rsquo;t 2-dimensional, its 1-dimensional. Its just freaking huge line of bytes. Here is how you squeeze a 2-dimensional array into a 1-dimensional line:</p><div class=imgwrap style=max-width:727px><a href=/img/tut4.png target=_blank><img src=/img/tut4.png alt="Allocating a 3x5 array" width=100%></a></div><p>As you can see, we just allocate each row right after the other to create a 15-element array ($5\cdot 3 = 15$). But then, how do we access it? Well, if it has a width of 5, to access another &ldquo;row&rdquo; we&rsquo;d just skip forward by 5. In general, if we have an $n$ by $m$ multidimensional array being represented as a one-dimensional array, the proper index for a coordinate $(x,y)$ is given by: <code>array[x + (y*n)]</code>. This can be extended to 3D and beyond but it gets a little messy. This is all the compiler is really doing with multidimensional array syntax - just automating this for you.</p><p>Now, if this is a <strong>square</strong> array (as evidenced by it being a square in 2D or a cube in 3D), a jagged array is one where each array is a different size, resulting in a &ldquo;jagged&rdquo; appearance:</p><div class=imgwrap style=max-width:320px><a href=/img/tut5.png target=_blank><img src=/img/tut5.png alt="Jagged array visualization" width=100%></a></div><p>We can&rsquo;t possibly allocate this in a single block of memory unless we did a lot of crazy ridiculous stuff that is totally unnecessary. However, given that arrays in C++ are just pointers to a block of memory, what if you had a pointer to a block of memory that was an array of pointers to more blocks of memory?</p><div class=imgwrap style=max-width:400px><a href=/img/tut6.png target=_blank><img src=/img/tut6.png alt="Jagged array pointers" width=100%></a></div><p>Suddenly we have our jagged array that can be accessed just like our previous arrays. It should be pointed out that with this format, each inner-array can be in a totally random chunk of memory, so the last element could be at position 200 and the first at position 5 billion. Consequently, pointer arithmetic only makes sense within each column. Because this is an array of arrays, we declare it by creating an array of pointers. This, however, does <strong>not</strong> initialize the entire array; all we have now is an array of <em>illegal pointers</em>. Since each array could be a different size than the other arrays (this being the entire point of having a jagged array in the first place), the only possible way of initializing these arrays is individually, often by using a <code>for loop</code>. Luckily, the syntax for accessing jagged arrays is the exact same as with square arrays.</p><p><pre class=language-cpp><code>int main(int argc, char* argv[])
{
  int** jagged = new int*[5]; //Creates an array of 5 pointers to integers.
  for(int i = 0; i &lt; 5; ++i)
  {
    jagged[i] = new int[3+i]; //Assigns each pointer to a new array of a unique size
  }
  jagged[4][1]=0; //Now we can assign values directly, or...
  int* second = jagged[2]; //Pull out one column, and
  second[0]=0; //manipulate it as a single array

  // The double-access works because of the order of operations. Since [] is just an
  // operator, it is evaluated from left to right, like any other operator. Here it is
  // again, but with the respective types that each operator resolves to in parenthesis.
  ( (int&amp;) ( (int*&amp;) jagged[4] ) [1] ) = 0;
}
</code></pre>As you can see above, just like we can have pointers to pointers, we can also have references to pointers, since pointers are just another data type. This allows you to re-assign pointer values inside jagged arrays, like so: <code>jagged[2] = (int*)kitty</code>. However, until <code>C++0x</code>, those references didn&rsquo;t have any meaningful data type, so even though the compiler was using <code>int*&</code>, using that in your code will throw a compiler error in older compilers. If you need to make your code work in non-<code>C++0x</code> compilers, you can simply avoid using references to pointers and instead use a pointer to a pointer.<pre class=language-cpp><code>int* bunny;
int* value = new int[5];

int*&amp; bunnyref = bunny; // Throws an error in old compilers
int** pbunny = &amp;bunny; // Will always work
bunnyref = value; // This does the same exact thing as below.
*pbunny = value;

// bunny is now equal to value
</code></pre>This also demonstrates the other use of a pointer-to-pointer data type, allowing you to remotely manipulate a pointer just like a pointer allows you to remotely manipulate an integer or other value type. So obviously you can do pointers to pointers to pointers to pointers to an absurd degree of lunacy, but this is <em>exceedingly rare</em> so you shouldn&rsquo;t need to worry about it. Now you should be strong in the art of pointer-fu, so our next tutorial will finally get into object-oriented techniques in C++ in comparison to C#. <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-3-classes-and/>Part 3: Classes and Structs and Inheritance OH MY!</a></p></article></div></section><section><div class=dim><aside><h2>C# to C++ Tutorial - Part 1: Basics of Syntax</h2><ul></ul></aside><article><p>[ <strong>1</strong> · <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-2-pointers/>2</a> · <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-3-classes-and/>3</a> · <a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-4-operator-overload/>4</a> <span style=color:#aaa>· 5 · 6 · 7</span> ]</p><p>When moving from C# to C++, one must have a very deep knowledge of what C# is actually doing when you run your program. Doing so allows you to recognize the close parallels between both languages, and why and how they are different. This tutorial will assume you have a fairly strong grasp of C#, but may not be familiar with some of its more arcane attributes.</p><p>In C#, everything is an object, or a static member of an object. You can&rsquo;t have a function just floating around willy-nilly. However, like all programs, a C# program must have an entry-point. If you have primarily done GUI-based design, you probably aren&rsquo;t aware of the entry-point that is automatically generated, but it is definitely there, and like everything else, it&rsquo;s part of an object. C# actually allows you to change the entry point function, but a default C# project will automatically generate a Program.cs file that looks like this:</p><p><pre class=language-csharp><code>using System;
using System.Collections.Generic;
using System.Linq;
using System.Windows.Forms;

namespace ScheduleTimer
{
  static class Program
  {
    /// &lt;summary&gt;
    /// The main entry point for the application.
    /// &lt;/summary&gt;
    [STAThread]
    static void Main()
    {
      Application.EnableVisualStyles();
      Application.SetCompatibleTextRenderingDefault(true);
      Application.Run(new frmMain());
    }
  }
}
</code></pre><code>static void Main()</code> is the real entry point for your application, which simply initializes visual styles and then immediately launches the form that most C# users are accustomed to working with. Now, we can compare this with a simple &ldquo;Hello World&rdquo; C++ program:</p><p><pre class=language-cpp><code>#include &lt;iostream&gt;

int main(int argc, char *argv[])
{
  std::cout &lt;&lt; &#34;Hello World&#34;;
  return 0;
}
</code></pre>This program, to a C# user, immediately looks foreign and possibly even outright hostile. However, almost everything in it has a direct analogue in C#, despite the rather inane syntax that is being used. The most glaring example here is the insertion operator, &#171;, because almost no one ever uses it except for in streams and the fact that it&rsquo;s in a C++ Hello World program creates an absurd amount of confusion. It&rsquo;s just a fancy way of doing this:<pre class=language-cpp><code>
#include &lt;iostream&gt;

int main(int argc, char *argv[])
{
  std::cout.write(&#34;Hello World&#34;,11);
  return 0;
}
</code></pre>Now, counting the number of bytes you are pumping into the stream is really annoying, and that&rsquo;s what the insertion operator does for you; it properly formats everything automatically. That&rsquo;s all. It&rsquo;s not a demon from hell bent on destroying your life, its just weird syntax. I don&rsquo;t know why they don&rsquo;t also have this functionality in a much easier to understand overloaded function, but there are a lot of things that they don&rsquo;t do, so we&rsquo;ll just have to live with it.</p><p>The <code>main()</code> function here serves the same exact purpose as the <code>Main()</code> function in C#. Strict C++ requires you to have a <code>main()</code> function to serve as an entry point, but various operating systems modify it and, in the case of Windows, outright replace it. As such, you will notice that your &ldquo;hello world&rdquo; C++ program, when built, opens in a command line. You will learn later how to prevent this by using Windows&rsquo; proprietary entry function. For those of you familiar with C#, this is exactly the same as C#&rsquo;s ability to change around the entry point of the application, and you can even make a command line application in C# too by properly changing the compiler settings. The same concept applies to C++, but unlike C#, which defaults to a GUI, C++ defaults to a command line. Changing the compiler settings properly will result in a C++ program that starts in a GUI, just like C# (although unlike C#, C++ doesn&rsquo;t have any help, which turns GUI programming into a complete nightmare).</p><p>So now that we have a direct analogue between C# and C++ in terms of where our application <em>starts</em>, we need to deal with a conceptual difference in how C# and C++ handle <em>dependencies</em>. In C#, your class file is just <code>Class.cs</code>, your helper class is <code>Helper.cs</code>, and both of them can call the other one provided they are in the same namespace, or if you are inheriting someone else&rsquo;s, using the correct <code>using</code> statements to resolve the code. If these concepts are not familiar to you, you should learn more about C# before delving further into C++.</p><p>C++, on the other hand, does not do behind-the-scenes magic to help you resolve your dependencies. To understand what C++ is doing, one must understand how any compiler resolves references inside code (including C#). When the C# compiler is compiling your project, it goes through each of your code files <strong>one by one</strong> and compiles everything to an intermediate object code that will later be compiled down into the machine code (or, in this case, bytecode, since C# is an interpreted language). But wait, what if it&rsquo;s compiling <code>Class.cs</code> before <code>Helper.cs</code> even though <code>Class</code> instantiates a <code>Helper</code> object and calls some functions inside of it that then instantiate another <code>Class</code> object? Well, what if you compiled <code>Helper.cs</code> first&mldr; but <code>Helper.cs</code> needs <code>Class.cs</code> to be compiled first because its instantiating a <code>Class</code> object inside the function that the <code>Class</code> object is calling! That&rsquo;s a circular dependency! <em><del>THIS IS IMPOSSIBLE OH GOD WE&rsquo;RE GOING TO DIE</del></em> No, it&rsquo;s actually quite simple to deal with. Enter <em>prototypes</em>. If you have the following C# class:</p><p><pre class=language-csharp><code>using System;

namespace FunFunBunBuns
{
  class Class
  {
    private int _yay;
    private int _bunnies;

    // Constructor
    public Class(int yay)
    {
      _yay = yay;
      _bunnies = 0; // :C
    }

    // Destructor
    public ~Class()
    {
      _yay = 0;
    }

    public void IncrementYay()
    {
      _yay++;
    }
    
    public int MakeBunnies(int num) // :D
    {
      _bunnies = _bunnies + num;
      return _bunnies;
    }
  }
}
</code></pre>Making &ldquo;prototypes&rdquo; of these functions (which C# doesn&rsquo;t have so this will be invalid syntax) would be the following:</p><p><pre class=language-csharp><code>using System;

namespace FunFunBunBuns
{
  class Class
  {
    private int _yay;
    private int _bunnies;

    // Constructor
    public Class(int yay);
    // Destructor
    public ~Class();
    public void IncrementYay();
    public int MakeBunnies(int num);
  }
}
</code></pre>Notice the distinct lack of code - this is how circular references get resolved. It turns out that to properly compile your program, the compiler only has to know what a function takes in as arguments, and what it returns. By treating the function as a &ldquo;black box&rdquo; of sorts, the compiler can ignore whatever code is inside it. Notice that this applies to constructors and destructors as well - they are simply special functions inside the class. In this manner the entire class can be treated as a bunch of black-box functions that don&rsquo;t actually have any code that needs to be compiled in them. What the C# compiler does is create a bunch of these prototypes behind the scenes and feed them in front of all your code files, so it first compiles <code>Class.cs</code> using a prototype of the <code>Helper</code> class, which allows it to instantiate and use any functions that <code>Helper</code> defines without actually knowing the code inside them. Then, it compiles <code>Helper.cs</code>, compiling assigning code to the previously empty black-box functions defined in the <code>Helper</code> prototype, using a prototype of <code>Class</code> so that it can also instantiate and call functions from <code>Class</code>. In this way, both <code>Helper.cs</code> and <code>Class.cs</code> can be compiled in any order.</p><p>But wait, what if <code>Class</code> inherits <code>Helper</code>? In reality, this changes nothing. An important lesson here is that, in C++, you will not be able to simply ignore the fact that <em>everything is a function</em>. Classes are just an abstraction - in reality, inheritance, constructors, deconstructors, operators, everything is just various special functions. Python&rsquo;s class syntax is interesting because requires that all class functions explicitly define the <code>self</code> parameter (which is identical to the <code>this</code> reference in C++ and C#), even the class constructor. Both C++ and C# hide all this from you, so Constructors and Destructors and class functions all magically just work, even though underneath it all they&rsquo;re just ordinary functions with a special parameter that&rsquo;s hidden from view. This is, in fact, how one mimics class behavior in C, which does not have object-oriented features - simply build a struct and make a bunch of functions for it that take a &ldquo;this&rdquo; pointer, or a pointer to a specific struct on which the function operates. This behavior can be (needlessly) duplicated using C# - let&rsquo;s transform our <code>Class</code> class to C-style function implementations, ignoring the slightly invalid C# syntax.</p><p><pre class=language-csharp><code>using System;

namespace FunFunBunBuns
{
  struct Class
  {
    private int _yay;
    private int _bunnies;
  };

  public Constructor(Class this, int yay)
  {
    this._yay = yay;
    this._bunnies = 0; // :C
  }

  public Destructor(Class this)
  {
    this._yay = 0;
  }

  public void IncrementYay(Class this)
  {
    this._yay++;
  }
    
  public int MakeBunnies(Class this, int num) // :D
  {
    this._bunnies = this._bunnies + num;
    return this._bunnies;
  }
}
</code></pre>Thankfully, we don&rsquo;t have to worry about this, since thinking of class functions as functions that operate on the object is a lot more intuitive. However, one must be aware that even in inheritance scenarios, everything is just a function, or an overload of a virtual function, or something similar (if you do not know what virtual functions are, you need to learn more C# before proceeding). Consequently, our ability to declare function prototypes solves <em>all</em> the dependency issues, because <em>everything</em> is a function.</p><p>This is where we get into exactly what the <code>#include</code> directive is for. In C#, all your files are automatically accessible from every other file, and this isn&rsquo;t a problem because compilation is nigh-instantaneous. C++ is much more intensive to compile, partially because it doesn&rsquo;t have a precompiled 400 MB library of crap to work off of, and partially due to a much more complicated precompiler. That means in C++, if you want a given file to have access to another file, you have to <code>#include</code> that file. In our Hello World application, we are including <code>iostream</code>, which does not have a .h file extension on the end for stupid regulatory reasons. However, what about the file our code is in? Our code is not in a .h file, its in a .cpp file. This is where we get to a critical difference between C# and C++. While C# just has .cs files for code, <strong>C++ has two types of files: header files and code files</strong>.</p><code>.cpp == C++ (C-plus-plus) code file
.h = C++ Header file</code><p>Header files contain class and function prototypes, and code files contain all the actual code. A C++ project is therefore defined entirely by a list of .cpp files that need to be compiled. Header files are just little helper files that make resolving dependencies easier. C# does this for you - C++ does not. Note that because these are technically arbitrary file distinctions, you can put whatever you want in either file type; nothing will stop you from doing <code>#include "main.cpp"</code>, its just ridiculous and confusing. Both <code>#include &lt;></code> and <code>#include ""</code> are valid syntax for the <code>#include</code> directive, there is no <em>real</em> difference. Standard procedure, however, is that <code>#include &lt;></code> is used for any header files outside of your project, and <code>#include ""</code> is used for header files inside your project, or closely related to it.</p><p>So what we&rsquo;re doing when we say <code>#include &lt;iostream></code> is that we&rsquo;re including a bunch of prototypes for various input/output stream (i/o stream &ndash;> iostream) related classes defined in the standard library, which your compiler already has the corresponding .cpp implementations of built into it. So, the compiler links the application against this header file, and when you use <code>std::cout</code>, it just treats everything in it (including that ridiculously obtuse <code>&lt;&lt; operator</code>, which is really just another function) as a black-box function.</p><p><a id=header-code>Consequently</a>, unless you know what your doing, you should keep code out of header files. C++ doesn&rsquo;t prevent you from throwing functions that aren&rsquo;t attached to classes all over the place, like C# does, so what would happen if you defined <code>int ponies() { return 0; }</code> in a header file that you include in two seperate .cpp files? The compiler will try to compile the function twice, and on the second time it will explode because the function it tried to put code into already had code in it, since it wasn&rsquo;t a prototype! <strong>EVERYTHING DIES!</strong> So until you get to the more advanced areas of C++, don&rsquo;t put code in your header files (unless you want to watch your compiler die, you monster).</p><p>At this point I want to clarify what <code>std::</code> is, because it looks rather weird to a C# programmer. In C#, the <code>. operator</code> works on everything - you just have <code>System.Forms.Whatever.Help.Im.Trapped.In.A.Universe.Factory.Your.Class.Member.Function()</code> and its all good. In C++, that&rsquo;s not going to work anymore. The <code>:: operator</code> is known as the Scope Resolution Operator. It&rsquo;s a lot easier to explain if I first explain what the <code>. operator</code> has been demoted to. You can only use the <code>. operator</code> on a <em>reference or value type</em> of an <em>instantiated object</em> (basically everything you&rsquo;ve ever worked with in C#). The important distinction here is that <strong>static functions cannot be accessed with the <code>. operator</code> anymore</strong>. This is because Static functions, along with namespaces and typedefs and <em>everything else</em> must use the Scope Resolution Operator. Consequently, you can think of the <code>. operator</code> as being demoted to just calling class functions, and everything else now uses the <code>:: operator</code>. So, <code>std::cout</code> just means that we&rsquo;re access the <code>cout</code> class in the <code>std</code> namespace.</p><p>Now we just have one more hurdle to overcome with the &ldquo;Hello World&rdquo; application, that funky <code>char* argv[]</code> parameter in <code>main()</code>. Most C# programmers can correctly infer that it is probably an array of some sort, but we don&rsquo;t know what type <code>char*</code> is, other than its clearly related to <code>char</code>.</p><p><code>char*</code> is a <strong>pointer</strong>. Yes, the same scary pointers you hear about all the time. No, they aren&rsquo;t really scary. In fact, you have been using similar concepts in C# all the time without actually realizing it. First, however, let&rsquo;s take a hard look at what a pointer really is.</p><p>Everything in your entire program takes up <strong>memory</strong>. Since this tutorial is designed for people who know C# already, I really, really hope you already knew that. What you might not know is that all this memory has a specific location on the machine. In fact, on a 32-bit machine, <strong>every single possible location of a byte</strong> can be contained in <strong>an unsigned 32-bit integer</strong>. This is why we are currently moving to 64-bit CPUs, because an unsigned 32-bit integer can only hold up to 4294967295 possible byte locations, which amounts to 4.2 gigs of memory. That&rsquo;s why you are limited to 4 gigs of RAM on a 32-bit machine, and windows has difficulty using more than 2 gigs because a lot of older programs assumed that a <strong>signed</strong> 32-bit integer was sufficient for all memory addresses, so windows has to do some funky memory paging techniques to get programs that ignore the last bit to use memory locations above 2147483647.</p><p>So, if you allocate a float, either on the stack or on the heap, <em>it must exist somewhere within those 4294967295 possible byte locations</em>. Consequently, lets say you want to call a function that modifies that float, but the function has to have a void return value for some arbitrary reason. If you know where in memory that float is, you can tell the function where to find the float and modify it to the desired value without ever returning a value. Here is an example C++ function doing just that (which is syntactically valid all by itself because C++ allows functions outside of classes):</p><p><pre class=language-cpp><code>void ModifyFloat(float* p)
{
  *p = 100.0;
}

int main(int argc, char* argv[])
{
  float x = 0; //x is equal to 0.0
  ModifyFloat( &amp;x );
  // x is now equal to 100.0
}
</code></pre>What&rsquo;s going on here? First, we have our <code>ModifyFloat()</code> function. This takes a pointer to a <code>float</code>, which is declared by adding a <code>*</code> to the desired type we want to make a pointer to. Remember that pointers are really just 32-bit integers (or 64-bit if you have a 64-bit operating system), but C++ assigns them a type so that if you try to assign a <code>double</code> to a pointer to a <code>float</code>, it throws an error instead of overflowing 4 extra bytes, causing a heap corruption and destroying the universe. So <code>char*</code> is a pointer to a <code>char</code>, a <code>double*</code> points to a <code>double</code>, and <code>Helper*</code> is a pointer to our own <code>Helper</code> class.</p><p>The next thing done in <code>ModifyFloat()</code> is <code>*p</code>. In this case, the <code>* operator</code> is the <em>dereference</em> operator. So unfortunately <code>*</code> is the multiply, pointer, and dereference operator in C++. Yes, this is retarded. I&rsquo;m sorry. But what the heck does dereference even mean? It takes a pointer type and turns it into a <em>reference</em>. You already know what a reference is, even if you don&rsquo;t realize it. In C#, you can pass a variable of your <code>Helper</code> class into a function, modify the class in the function, and the original variable will get modified too! This is because, by default, classes are passed <em>by-reference</em> in C#. That means, even though it looks identical to a variable passed <em>by value</em>, any changes made to the variable are in fact made to whatever variable it references. So, this idea of passing variables in by reference should be familiar to an experienced C# programmer. C++ has references too, I just haven&rsquo;t gone over their syntax. Here&rsquo;s a more explicit version of the function:</p><p><pre class=language-cpp><code>void ModifyFloat(float* p)
{
  float&amp; ref_p = *p;
  ref_p = 100.0;
}
</code></pre>This is the exact same as the previous function, but here we can clearly see the reference. In C#, if you wanted a variable normally passed by value, like a struct, to get passed by reference, you had to override the default behavior by adding <code>ref</code>. In C++, a variable that is a reference to a given type is declared in a similar manner to a pointer. The <code>& operator</code> is used instead of <code>*</code>, so in this example, float& is a reference to a <code>float</code>. We assign it to the value produced by turning our pointer into a <code>float</code> reference. Then we just set our reference equal to 100.0 and it magically alters the original variable, just like it would in C#. In fact, here is the same function written in (slightly illegal) C#:</p><p><pre class=language-csharp><code>public static void ModifyFloat(ref float p)
{
  p=100.0;
}
</code></pre>This does the same thing, just without the pointer. In fact, we can totally ignore the pointer in C++ too, if we want (which I tend to prefer, when possible, because its a lot easier to work with):</p><p><pre class=language-cpp><code>void ModifyFloat(float&amp; ref_p)
{
  ref_p = 100.0;
}
int main(int argc, char* argv[])
{
  float x = 0; //x is equal to 0.0
  ModifyFloat( x );
  // x is now equal to 100.0
}
</code></pre>Now, in this implementation, you will notice that our call to <code>ModifyFloat</code> is now equivalent to what it would be in C#, in that we just pass in the variable. What happened to that random <code>& operator</code> we had there before? The <code>& operator</code> is also known as the <code>address-of operator</code>, meaning when its applied to a variable as opposed to a type, it returns a pointer to that variable (yay, more context-dependent redundant operators). So, we could rewrite our function as follows to make it a bit more clear:</p><p><pre class=language-cpp><code>void ModifyFloat(float* p)
{
  float&amp; ref_p = *p; //get a reference from the pointer
  ref_p = 100.0; //modify the reference
}
int main(int argc, char* argv[])
{
  float x = 0; //x is equal to 0.0
  float* p_x = &amp;x; //get a pointer to x
  ModifyFloat( p_x ); //pass pointer into function
  // x is now equal to 100.0 
}
</code></pre>As we can see, pointers are just the underlying work behind references. If you ever go into Managed C++, you&rsquo;ll find out that all C# references are really just pointers, but the language treats them as references so they&rsquo;re hidden from you. In C++, you can have both pointers and references. It is important to note that you can only <em>initialize</em> a reference variable. Any subsequent operators will be applied to whatever variable its referencing, making it impossible to get the address of a reference variable or do anything to the reference variable itself - for all intents and purposes, it just <em>is</em> the variable it references. This is why pointers are handy - you CAN reassign the actual pointer variable while also accessing the variable its pointing to. Consequently, you can also get the address of a pointer variable, since just like any other variable, including the reference variable, it must occupy memory, and therefore has a location that you can get a pointer to (we&rsquo;ll get to that syntax in a minute). But there&rsquo;s one more thing&mldr;</p><p><strong>What about arrays?</strong> In C#, arrays are actually a built-in class that has lots of fancy functions and whatnot. Interestingly, they are still of fixed size. C++ arrays are also fixed size, but they are manipulated as raw memory. Let&rsquo;s compare initializing an array in C++ and initializing an array in C#:</p><p><pre class=language-csharp><code>int[] numbers = new int[5];
</code></pre><pre class=language-cpp><code>int* numbers = new int[5];
</code></pre>It should be pretty obvious at this point that <em>arrays are pointers</em> in C++. I can even rewrite the above in C++ using an empty array syntax, and it will be equally as valid:</p><p><pre class=language-cpp><code>int numbers[] = new int[5];
</code></pre><strong><code>int *x*[]</code> is identical to <code>int* *x*</code></strong>. There is no difference. Observe the following modification of our original Hello World function:</p><p><pre class=language-cpp><code>int main(int argc, char** argv);
</code></pre>Same thing. In fact, if you watch your compiler output carefully, you might even see the compiler internally convert all the arrays to pointers when its resolving types. Now, as a C# programmer, you should already know what arrays are. You should probably also be at least dimly aware that each element of an array occupies memory directly after the element proceeding it. So, if you know where the address of the <em>first</em> element is, you know the second element is exactly <em>x</em> bytes afterwards, where <em>x</em> is the number of bytes your type takes up. This is why pointers have types associated with them - we know that <code>float*</code> points to an array of elements, and that each element takes up 4 bytes. To verify this, the <code>sizeof()</code> built-in function/operator/whatever will return the number of bytes a given type, class, or struct takes up. That&rsquo;s the number of bytes we skip ahead to get to the next element in an array. This is all done transparently in C++ using the same array index operator as C# uses:</p><p><pre class=language-cpp><code>int main(int argc, char** argv)
{
  int* ponies = new int[5];
  ponies[0] = 1; //First element..
  ponies[1] = 2; //Second element...
}
</code></pre>So pointers can be treated as arrays that behave exactly the same way a C# array does. However, the astute C# programmer would ask, how do you know how long the array is?</p><p><span style=color:red><h5>YOU DON'T</h5></span>Enter every single buffer overflow error that has been the bane of man since the beginning of time. <em>YOU</em> have to keep track of how long the array is, and you&rsquo;d better be damn sure you don&rsquo;t get it wrong. Consequently any function taking an array of variable size will also require a separate argument telling the function how many elements are in the array. Usually arrays are just constructed on the stack with a constant, known size, which is often harmless and pretty hard to screw up. If you start doing funky things with them, though, you might want to look up <a href=http://www.cplusplus.com/reference/stl/vector/>std::vector</a> for an encapsulated dynamic array.</p><p>So C++ arrays are just like C# arrays, except they are pointers to the first element, and you don&rsquo;t know how long they are (and they might cause the destruction of the universe if you screw up). You should already know that a string is an array, and consequently in C++ the standard string type is <code>const char*</code>, not <code>string</code>. You also can&rsquo;t put them in switch() statements. Sorry.</p><p>There&rsquo;s a lot of stuff about pointers that this tutorial hasn&rsquo;t covered, like function pointers and pointer arithmetic, which we&rsquo;ll get to next time.</p><p><a href=https://erikmcclure.com/blog/c-to-c-tutorial-part-2-pointers/>Part 2: Pointers To Everything</a></p></article></div></section><section><div class=dim><aside><h2>The Ninth Circle of Bugs</h2><ul></ul></aside><article><p>So I&rsquo;m rewriting my <a href=www.blackspherestudios.com/storage/McClure_Erik_Regional_KD_Trees.pdf>2D culling kd-tree</a> for my graphics engine, and a strange bug pops up. On release mode, one of the images vanished. Since it didn&rsquo;t happen in debug mode, it was already a <code>heisenbug</code>. A <code>heisenbug</code> is defined as a bug that vanishes when you try to find it. It took me almost a day to trace the bug to the <code>rebalance</code> function. At first I thought the image had simply been removed from a node accidentally, but this wasn&rsquo;t the case. It took another day to finally figure out that the <code>numimages</code> variable was getting set to 0, thus causing the node to think it was empty and resulted in it deleting itself and removing itself from the tree (which caused all sorts of other problems).</p><p>Unfortunately, I could not verify the tree. Any attempt that so much as <em>touched</em> the tree&rsquo;s memory would wipe out the bug, or so I thought. Then I tried adding the verification function into an if statement that would only activate if the bug appeared - it did not. The act of adding a line of code <em>that was never executed</em> actually <strong>caused the bug to vanish</strong>.</p><p>I was absolutely stunned. This was completely insane. Following the advice of a friend, I was forced to assume the compiler somehow screwed something up, so I randomly disabled various optimizations in release mode. It turned out that disabling the <code>Omit Frame Pointers</code> optimization removed the bug. I didn&rsquo;t actually know what frame pointers <em>were</em>, only that I had turned on this optimization in many other projects for the hell of it and it had never caused any problems (no optimizations ever should, for that matter). What I discovered <a href="http://www.nynaeve.net/?p=91">was astonishing</a>. Frame pointers couldn&rsquo;t be omitted from a function if it got too complicated or needed to unwind the stack due to a possible exception. On a hunch, instead of adding the verification function to the chunk of code that was only executed if the error occurred, I instead added a vestigial <code>'throw "derp";'</code> line.</p><p><em>The problem vanished</em>.</p><p>I knew instantly that either the problem was caused by the omission of frame pointers, which would indicate a bug in the VC++ 2010 compiler (unlikely), or when the frame pointers were included, it masked the bug (much more likely). But I also had another piece of knowledge at my disposal - exactly how I could modify the function without masking the bug. I considered decompiling the function and forcing VC++ to use the flawed assembly, but that didn&rsquo;t allow me to modify the assembly in any meaningful way. A bit more experimentation revealed that any access of the root node, or for that matter, the <code>'this'</code> pointer itself (unless it was for calling a function) caused the inclusion of the frame pointer. I realized that a global variable would be exempt from this, and that I might be able to get by this limitation by assigning the address of whatever variable I needed to the global variable and passing that into the function instead.</p><p>This approach, however, failed. In fact most attempts to get around the frame pointer inclusion failed. I did, however, notice what appeared to be a separate bug in another part of the tree. A short investigation later revealed an unrelated bug in the tree caused by the solve function. However, what was causing this bug (duplicated parentC pointers) still threw up errors after solving the first bug, indicating that it was possible this mysterious insane compiler induced bug was just a symptom of a deeper one that would be easier to detect. After more hunting, a second unrelated bug was found. Clearly this tree was not nearly as stable as I had thought it was.</p><p>A third bug was eventually found, and I discovered the root cause of this bug to be an #NaN float value in the tree. This should never ever, ever happen, because it destabilizes the tree, but sure enough, I finally found the cause.</p><code>_totalremove(node->total,(const float (&)[4])currect);</code><p>Casting from a float* that was previous cast from a float[4] causes read errors at totally random times, despite this being completely valid under the circumstances. My only guess is that the compiler somehow interpreted this cast as undefined behavior and went crazy. I will never know. All I know is that I should never, ever, ever, ever, ever cast to that data type ever again, because guess what? After removing all my debug equipement and putting the cast back in, I was able to reliable reproduce the bug that started this whole mess, and removing the cast made the bug vanish.</p><p>This entire week long trek through hell was because the compiler fucked up on a goddamn <em>variable cast</em>. It wasn&rsquo;t a memory leak, it wasn&rsquo;t a buffer overrun, it was just a goddamn <em>miscast variable</em>.</p><p>Lesson: Re-validate every inch of your data structure the instant you realize you have a heisenbug, and make sure your validation function properly checks for all things that can screw things up.</p></article></div></section><section><div class=dim><aside><h2>Investigating Low-level CPU Performance</h2><ul></ul></aside><article><p>While reconstructing my threaded Red-Black tree data structure, I naturally assumed that due to invalid branch predictions costing significant amounts of performance, by eliminating branching in low-level data structures, one can significant enhance the performance of your application. I did some profiling and was stunned to discover that my new, optimized Red Black tree was&mldr; <em>SLOWER</em> then the old one! This can&rsquo;t be right, I eliminated several branches and streamlined the whole thing, how can it be <em>SLOWER?!</em> I tested again, and again, and again, but the results were clear - even with fluctuations of up to 5% in the results, the average speed for my new tree was roughly 7.5% larger then my old one (the following numbers are the average of 5 tests).</p><p>Old: 626699 ticks
New: 674000 ticks</p><pre class=language-cpp><code>//Old
c = C(key, y-&gt;_key);
if(c==0) return y;
if(c&lt;0) y=y-&gt;_left;
else y=y-&gt;_right;

//New
if(!(c=C(key,y-&gt;_key)))
return y;
else
y=y-&gt;_children[(++c)&gt;&gt;1];
</code></pre><p>Now, those of you familiar with CPU branching and other low-level optimizations might point out that the compiler may have optimized the old code path more effectively, leaving the new code path with extra instructions due to the extra increment and bitshift operations. <strong>Wrong.</strong> Both code paths have the <em>exact same number of instructions</em>. Furthermore, there are only <em><strong>FOUR</strong></em> instructions that are different between the two implementations (highlighted in red below).</p><span><table cellpadding=0 cellspacing=0><tr><td><b>New</b><pre>00F72DE5  mov         esi,dword ptr [esp+38h]  
00F72DE9  mov         eax,dword ptr 
00F72DEE  cmp         esi,eax  
00F72DF0  je          main+315h (0F72E25h)  
00F72DF2  mov         edx,dword ptr [esp+ebx*4+4ECh]
<span style=color:red>00F72DF9  lea         esp,[esp]</span>
00F72E00  mov         edi,dword ptr [esi+4]  
00F72E03  cmp         edx,edi  
00F72E05  jge         main+2FCh (0F72E0Ch)  
00F72E07  or          ecx,0FFFFFFFFh  
00F72E0A  jmp         main+303h (0F72E13h)  
00F72E0C  xor         ecx,ecx  
00F72E0E  cmp         edx,edi  
00F72E10  setne       cl  
00F72E13  movsx       ecx,cl  
00F72E16  test        ecx,ecx  
00F72E18  je          main+317h (0F72E27h)  
<span style=color:red>00F72E1A  inc         ecx  </span>
<span style=color:red>00F72E1B  sar         ecx,1  </span>

00F72E1D  mov         esi,dword ptr [esi+ecx*4+18h]  
00F72E21  cmp         esi,eax  
00F72E23  jne         main+2F0h (0F72E00h)  
00F72E25  xor         esi,esi  
00F72E27  mov         eax,dword ptr [esi]  
00F72E29  add         dword ptr [esp+1Ch],eax</pre></td><td><b>Old</b><pre>00F32DF0  mov         edi,dword ptr [esp+38h]  
00F32DF4  mov         ebx,dword ptr 
00F32DFA  cmp         edi,ebx  
00F32DFC  je          main+31Dh (0F32E2Dh)  
00F32DFE  mov         edx,dword ptr [esp+eax*4+4ECh]  

00F32E05  mov         esi,dword ptr [edi+4]  
00F32E08  cmp         edx,esi  
00F32E0A  jge         main+301h (0F32E11h)  
00F32E0C  or          ecx,0FFFFFFFFh  
00F32E0F  jmp         main+308h (0F32E18h)  
00F32E11  xor         ecx,ecx  
00F32E13  cmp         edx,esi  
00F32E15  setne       cl  
00F32E18  movsx       ecx,cl  
00F32E1B  test        ecx,ecx  
00F32E1D  je          main+31Fh (0F32E2Fh)  
<span style=color:red>00F32E1F  jns         main+316h (0F32E26h)  </span>
<span style=color:red>00F32E21  mov         edi,dword ptr [edi+18h]  </span>
<span style=color:red>00F32E24  jmp         main+319h (0F32E29h)  </span>
00F32E26  mov         edi,dword ptr [edi+1Ch]  
00F32E29  cmp         edi,ebx  
00F32E2B  jne         main+2F5h (0F32E05h)  
00F32E2D  xor         edi,edi  
00F32E2F  mov         ecx,dword ptr [edi]  
00F32E31  add         dword ptr [esp+1Ch],ecx</pre></td></tr></table></span><p>I have no real explanation for this behavior, but I do have a hypothesis: The important instruction is the extra <em>LEA</em> in my new method that appears to be before the branch itself. As a result, it may be possible for the CPU to be doing branch prediction in such a way it shaves off one instruction, which gives it a significant advantage. It may also be that the branching is just <em>faster</em> then my increment and bitshift, although I find this highly unlikely. At this point I was wondering if anything I knew about optimization held any meaning in the real world, or if everything was just a lot of guesswork and profiling because <em>what the fuck?!</em> However, it then occurred to me that there was an optimization possible for the old version - Move the if(c==0) statement to the bottom so the CPU does the (c&lt;0) and (c>0) comparisons first, since the c==0 comparison only happens once in the traversal. Naturally I was a bit skeptical of this having any effect on the assembly-rewriting, branch-predicting, impulsive teenage bitch that my CPU was at this point, but I tried it anyway.</p><p>It worked. There was a small but noticeable improvement in running time by using the old technique and rewriting the if statements as such:<pre class=language-cpp><code>c = C(key, y-&gt;_key);
if (c &lt; 0)  y = y-&gt;_left;
else if(c &gt; 0) y = y-&gt;_right;
else return y;
</code></pre></p><p>Optimized: 610161.8 Ticks</p><p>The total performance improvement over my failed optimization attempt and my more successful branch-manipulation technique is a whopping 63838.2 Ticks, or a ~10% improvement in speed, caused by simply rearranging 4 or 5 instructions. These tests were done on a randomized collection of 500000 integers, so that means the optimized version can pack in 10% more comparisons in the same period of time as the bad optimization. That&rsquo;s 550000 vs 500000 elements, which seems to suggest that delicate optimization, even in modern CPUs, can have significant speed improvements. Those of you who say that toying around with low level code can&rsquo;t infer significant performance increases should probably reconsider exactly what you&rsquo;re claiming. This wouldn&rsquo;t directly translate to 50000 extra players on your server, but a 10% increase in speed isn&rsquo;t statistically insignificant.</p></article></div></section><section><div class=dim><aside><h2>The IM Failure</h2><ul></ul></aside><article><p>This is completely insane. First, Microsoft has rendered its new Windows Live Messenger (previously MSN messenger) almost completely useless.</p><ul><li>No more handwriting</li><li>No more setting your name to anything other then your first and last name.</li><li>All links you click on redirect you to a page from Microsoft warning you about the dangers of the internet, requiring you to click a link to proceed.</li><li>All photosharing is now incompatible with previous versions of messenger, and instead of actually just sending the file, it will instead fail completely.</li><li>Any youtube video, image, or any other link you copy paste into the window will automatically trigger a sharing session whether you like it or not.</li><li>It will, at times, randomly decide none of your messages are getting through.</li><li>You can no longer have a one-sided webcam session. WLM will simply leave your webcam as a giant, useless blank image underneath your conversation partner, demanding that you buy a webcam.</li><li>It&rsquo;s new emoticons must have been influenced by H.R.Giger (you can&rsquo;t turn them off and leave custom ones on).</li></ul><p>Naturally I wasn&rsquo;t able to put up with this for very long and moved to pidgin. Pidgin has many issues of its own, including an ass-backwards UI design and this really annoying tendency to create a new popup window to confirm every single file transfer, among other truly bizarre UI design elements. I was willing to put up with these, because honestly pidgin is designed for Linux and just happens to work on windows too, and their libpurple library is the basis of almost all open-source IM clients.</p><p>Pidgin, however, has now stopped working as well. It now spastically refuses to connect to the MSN service because the SSL certificate is invalid. I can appreciate it trying to protect my privacy, but there is no way to override this. So, pidgin is now out of the question.</p><p>Well, how about Trillian? During its installation, Trillian informed me that &ldquo;The Adobe Flash 9.0 plugin for Internet Explorer could not be found. This plugin is required for some of Trillian&rsquo;s features.&rdquo;</p><p>Trillian is no longer on my computer.</p><p>After digging around on wikipedia, I came up with Miranda IM, which seemed to be my last hope for a multi-protocol service that didn&rsquo;t suck total ass. It supported WLM, AIM, and&mldr; not google talk? Not XMPP, the most useful extensible open source protocol? Um, ok. Its UI design is more compact then Pidgin but arguably even worse, and it doesn&rsquo;t support custom emoticons or hardly ANYTHING on the WLM protocol. It served its purpose by at least letting me log the fuck in like I wanted to, though.</p><p>This is driving me up the wall. If anything else happens, I&rsquo;m going to snap and simply <em>make time</em> to work on <em><strong>my own</strong></em> IM client implementation that doesn&rsquo;t have <strong>glaring design flaws</strong> like <em>every single other one</em>. Honestly, requiring the Internet Explorer flash plugin to run everything? What the fuck are they smoking? What the hell is wrong with these developers?!</p><p>If only D had a good development environment, then I could write my IM client in it.</p></article></div></section><section><div class=dim><aside><h2>Album For Sale! [Renascent]</h2><ul></ul></aside><article><p>Due to Bandcamp&rsquo;s sudden threat to turn all of my free downloads into paid ones, I decided to go ahead and start selling my music properly. <a href=http://erikmcclure.bandcamp.com/album/renascent>Renascent</a> is now available for $3, or about as much as a gallon of milk costs. It contains remastered, super high quality (lossless if you choose to download in FLAC format) versions of all 14 songs, in addition to the original FLP project files used to create them. If you have ever wondered how I made a particular song, this might be another incentive to purchase the album. Note that these FLPs are released under the <a href=http://creativecommons.org/licenses/by-nc-nd/3.0/>Creative Commons Attribution-NonCommercial-NoDerivs 3.0</a> license, so you can&rsquo;t go running off with them like free candy.</p><p><strong>Track List:</strong></p><ol><li>On The Edge (2:56)</li><li>Renascent (4:06)</li><li>The Boundless Sea (6:49)</li><li>Duress (2:40)</li><li>Seaside Lookout (4:54)</li><li>Sapphire [Redux] (2:20)</li><li>Absolutia (3:04)</li><li>The Plea (3:46)</li><li>Now (2:34)</li><li>Alutia (4:10)</li><li>Rite (5:20)</li><li>Crystalline Cloudscape (4:04)</li><li>All Alone (3:06)</li><li>SunStorm (4:12)</li></ol><p><em>Total Time: 56:44</em></p><p><a href=http://erikmcclure.bandcamp.com/album/renascent>Listen and Buy It Here</a></p></article></div></section><section><div class=dim><aside><h2>WavSaver</h2><ul></ul></aside><article><p>There is a documented bug in windows 7 that has pissed me off a few times and recently crippled a friend of mine, where a .wav file with corrupted metadata causes explorer.exe to go into an infinite loop. My friend has a large collection of wavs that somehow got corrupted, so I wrote this program to strip them of all metadata. Due to the nature of the bug, the program can&rsquo;t delete them (you must use the command prompt to do that), but rather creates a folder called &ldquo;safe&rdquo; with all the stripped wav files inside of it.</p><p>Hosted in case anyone else has corrupted wav files they need to save. Just stick it inside a folder and run it - it&rsquo;ll automatically strip all wav files in the same folder as the executable.</p><p><a href=http://www.blackspherestudios.com/storage/WavSaver.zip>WavSaver</a></p></article></div></section><section><div class=dim><aside><h2>Pixel Perfect Hit Testing</h2><ul></ul></aside><article><p>After beating World of Goo after stabilizing things in my game and renaming it, I wondered how easy it was to decompile C# applications and simultaneously thought this would be a great opportunity to get pixel perfect hit testing to work on my engine. So, I decompiled GearGOD&rsquo;s composition example and quickly discovered that his method of detecting mouse messages was&mldr; well something completely different then his extremely bad attempt at explaining it to me had suggested.</p><p>Basically, he did not run into the window event issues that I was having because&mldr; he didn&rsquo;t use them. XNA keeps track of the mouse coordinates in its own separate update function, most likely using its special input hook, and hence there is no mousemove to keep track of. Instead of occurring when the user moves the mouse, the hit tests occur <em>every single frame</em>.</p><p>Hence, once you have utilized <code>WS_EX_TRANSPARENT|WS_EX_COMPOSITED|WS_EX_LAYERED</code> to make your window click-through-able, you then simply do a hit test on a given pixel after everything has been drawn, and swap out <code>WS_EX_TRANSPARENT</code> depending on the value. <code>GetCursorPos</code> and <code>ScreenToClient</code> will get the mouse coordinates you need, although they can be off your app window entirely so check for that too.</p><pre class=language-cpp><code>if(_dxDriver-&gt;MouseHitTest(GetMouseExact()))
  SetWindowLong(_window,GWL_EXSTYLE,((GetWindowLong(_window,GWL_EXSTYLE))&amp;(~WS_EX_TRANSPARENT)));
else
  SetWindowLong(_window,GWL_EXSTYLE,((GetWindowLong(_window,GWL_EXSTYLE))|WS_EX_TRANSPARENT));</code></pre><p>To get the pixel value, its a bit trickier. You have two options - you can make a lockable render target, or you can copy the render target to a temporary texture and lock that instead. The DirectX docs said that locking a render target is so expensive you should just copy it over, but after GearGOD went and yelled at me I tested the lockable render target method, and it turns out to be significantly faster. Futher speed gains can be achieved by making a 1x1 lockable render target and simply copying a single pixel from the backbuffer into the lockable render target and testing that.</p><pre class=language-cpp><code>void cDirectX_real::ActivateMouseCheck()
{
  if(_mousehittest) _mousehittest-&gt;Release();
  DX3D_device-&gt;CreateRenderTarget(1,1,_holdparams.BackBufferFormat,D3DMULTISAMPLE_NONE,0,TRUE,&amp;_mousehittest,NULL);
}
bool cDirectX_real::MouseHitTest(const cPositioni&amp; mouse)
{
  RECT rect = { mouse.x,mouse.y,mouse.x+1,mouse.y+1 };
  DX3D_device-&gt;StretchRect(_backbuffer,&amp;rect,_mousehittest,0,D3DTEXF_NONE);

  if(mouse.x&lt;0 || mouse.y &lt; 0 || mouse.x &gt; (int)_width || mouse.y &gt; (int)_height)
    return false; //off the stage
  D3DLOCKED_RECT desc = { 0,0 };
  if(FAILED(_mousehittest-&gt;LockRect(&amp;desc, 0,D3DLOCK_READONLY)))
    return true;    

  unsigned char color = (*((unsigned long*)desc.pBits))&gt;&gt;24;
  _mousehittest-&gt;UnlockRect();
  return color&gt;_alphacutoff;
}
</code></pre><p>Using this method, the performance hit is 620 FPS to 510 FPS at 1280x1024, which is fairly reasonable. However, my <a href=http://www.blackspherestudios.com/storage/PlaneShader.zip>Planeshader SDK example</a> is still at 0.9.71, which does not have this updated, fast version, so it will be using a much slower method to do it. The end result is the same though.</p></article></div></section><section><div class=dim><aside><h2>8-bit color cycling</h2><ul></ul></aside><article><p>Someone linked me to this awesome webpage that uses HTML5 to do 8-bit palette color cycling using Mark Ferrari&rsquo;s technique and art. I immediately wanted to implement it in my graphics engine, but soon realized that the technique is so damn old that no modern graphics card supports it anymore. So, I have come up with a pixel shader that creates the same functionality, either by having one image with an alpha channel containing the palette indices and a separate texture acting as the palette, or you can combine them into a single image. This is supposed to support variable palette sizes (up to 256) but I haven&rsquo;t had much ability to test the thing because its so damn hard to get the images formatted correctly. So while all of these variations i&rsquo;m about to show you should work there is no guarantee they necessarily will.</p><p>Video Link</p><p>8-bit cycling multi-image</p><pre class=language-cpp><code>ps 2.0 HLSL
// Global variables
float frame;
float xdim;
float xoff;

// Samplers
sampler s0 : register(s0);
sampler s1 : register(s1);

float4 ps_main( float2 texCoord : TEXCOORD0 ) : COLOR0
{
float4 mainlookup = tex2D( s0, texCoord );
float2 palette = float2(mainlookup.a*xdim + xoff,frame);
mainlookup = tex2D(s1, palette);
return mainlookup;
}

 
ps 1.4 ASM
ps.1.4
texld r0, t0
mad r0.x, r0.a, c1, c2
mov r0.y, c0
phase
texld r1, r0
mov r0, r1
 
It is also possible to write the shader in ps.1.1 but it requires crazy UV coordinate hacks.

frame is a value from 0.0 to 1.0 (ps.1.4 will not allow you to wrap this value, but ps.2.0 will) that specifies how far through the palette animation you are.
xdim = 255/(width of palette)
xoff = 1/(2*(width of palette))

Note that all assembly registers correspond to a variable in order of its declaration. So, c0 = frame, c1 = xdim, c2 = xoff.

8-bit cycling single-image

ps 2.0 HLSL
// Global variables
float frame;
float xdim;
float xoff;

// Samplers
sampler s0 : register(s0);

float4 ps_main( float2 texCoord : TEXCOORD0 ) : COLOR0
{
float4 mainlookup = tex2D(s0, texCoord );
float2 palette = float2(mainlookup.a*xdim + xoff,frame);
mainlookup = tex2D(s0, palette);
mainlookup.a = 1.0f;
return mainlookup;
}

 
ps 1.4 ASM
ps.1.4
def c3, 1.0, 1.0, 1.0, 1.0
texld r0, t0
mov r1, r0
mad r1.x, r1.a, c1, c2
mov r1.y, c0
phase
texld r0, r1
mov r0.a, c3
</code></pre><p>frame is now a value between 0.0 and (palette height)/(image height).
xdim = 255/(image width)
xoff = 1/((image width)*2)</p><p>24-bit cycling</p><pre class=language-cpp><code>ps 2.0 HLSL
// Global variables
float frame;
float xdim;
float xoff;

// Samplers
sampler s0 : register(s0);
sampler s1 : register(s1);

float4 ps_main( float2 texCoord : TEXCOORD0 ) : COLOR0
{
float4 mainlookup = tex2D( s0, texCoord );
float2 palette = float2(mainlookup.a*xdim + xoff,frame*fElapsedTime);
float4 lookup = tex2D(s1, palette);
return lerp(mainlookup,lookup,lookup.a);
}

 
ps 1.4 ASM
ps.1.4
def c3, 1.0, 1.0, 1.0, 1.0
texld r0, t0
mov r2, c0
mad r2.x, r0.a, c1, c2
phase
texld r1, r2
mov r0.a, c3
lrp r0, r1.a, r1, r0
</code></pre><p>Variables are same as 8-bit multi-image.</p><p>All this variation does is make it possible to read an alpha value off of the palette texture, which is then interpolated between the palette and the original color value. This way, you can specify 0 alpha palette indexes to have full 24-bit color, and then just use the palette swapping for small, animated areas.</p><p>If I had infinite time, I&rsquo;d write a program that analyzed a palette based image and re-assigned all of the color indexes based on proximety, which would make animating using this method much easier. This will stay as a proof of concept until I get some non-copyrighted images to play with, at which point I&rsquo;ll probably throw an implementation of it inside my engine.</p></article></div></section><section><div class=dim><aside><h2>Physics Networking</h2><ul></ul></aside><article><p>I&rsquo;m still working on integrating physics into my game, but at some point here I am going to hit on that one major hurdle: syncing one physics environment with another that could be halfway across the globe. There are a number of ways to do this; some of them are bad, and some of them are absolutely terrible.</p><p>If any of you have played Transformice, you will know what I mean by terrible. While I did decompile the game, I never bothered to examine their networking code, but I would speculate that they are simply mass-updating all the clients and not properly interpolating received packets. Consequently, when things get laggy, everyone doesn&rsquo;t just hop around, they completely clip through objects, even ones that they should never clip though <em>no matter what the other players are doing</em>.</p><p>The question is, how do you properly handle physics networking without flooding the network with unnecessary packets, especially when you are dealing with very large numbers of physics objects spread across a large map with many people interacting in complex ways?</p><p>In a proper setup, a client processes input from the user, and sends this to the server. While it&rsquo;s waiting for a response, it will interpolate the physics by guessing what all the other players are doing. If there are no other players this interpolation can, for the most part, be considered to be perfectly accurate. This will generally hold true if there are players that are far enough away from each other they can&rsquo;t directly or indirectly influence each other&rsquo;s interpolation.</p><p>Meanwhile, the server receives the input a little while later - say, 150 ms. In an ideal scenario, the entire physics world is rewound 150 ms and then re-simulated taking into account the player&rsquo;s action. The server then broadcasts new physics locations and the player&rsquo;s action to all other clients.</p><p>All the other clients now get these packets <em>another</em> 150 ms later. In an ideal scenario, all they would need to know is that the other player pressed a button 300 ms ago, rewind the simulation that much, then re-simulate to take it into account.</p><p>We are, obviously, not in an ideal scenario.</p><p>What exactly does this entail? In any interpolation function, speed is gained by making assumptions that introduce possible errors. This error margin grows as the player has more and more objects he might have interacted with. However, this also applies to each physics object against each <em>other</em> physics object, and in turn each other physics object from that. Hence, this boils down to the math equation: p = n<sup>n!</sup>. That is a worst case scenario. Best case scenario is that none of the physics objects interact with each other, so error margin p = n and is therefore linear.</p><p>Hence, we now know that interaction with physics objects - or possibly, any sort of nonuniform physical force, like an explosion - is what creates the uncertainty problem. This is why the server always maintains its own physics world that is synced to everyone else, so that even if its not always <em>right</em>, its at least somewhat <em>consistent</em>. The question is, when do we need to send a physics packet, and when do we <em>not</em> need to send one?</p><p>If the player is falling through the air and jumps, and there is nothing he could possibly interact with, we can assume that any half-decent interpolation function will be almost perfect. Hence, we don&rsquo;t actually have to send any physics packets because the interpolation should be perfect. However, the more possible objects that are involved, the more uncertain things get and the more we need to send physics updates in case the interpolation functions get confused. In addition, we have to send packets for all affected <em>objects</em> as well. However, we only have to do this when the player gives input to the game. If the player&rsquo;s input does not change, then he is obeying the interpolation laws of everyone else and no physics update is needed, since the interpolation always assumes the player&rsquo;s input status has not changed.</p><p>Hence, in a semi-ideal scenario, we only have to send physics packets for the player and any physics objects he might have collided with, and only when the player changes their input status. Right?</p><p>But wait, that works for the server, but not all the other clients. Those clients receive those physics packets 150 ms late, and have to interpolate <em>those</em> as well, introducing more uncertainty that cannot be eliminated. In addition, we aren&rsquo;t even in a semi-ideal scenario - the interpolation functions become more unreliable over time regardless of the player&rsquo;s input status.</p><p>However, this uncertainty itself is still predictable. The more objects a player is potentially interacting with, the more uncertain those object states are. This grows exponentially when other players are in the mix because we simply cannot guess what they might do in those 150 ms.</p><p>Consequently, one technique would be to send physics updates for all potentially affected objects surrounding the player whenever the player changes their input <em>or interacts with another physics object</em>. This alone will only work when the player is completely alone, and even then require some intermediate packets for higher uncertainty. Hence, one can create an update pattern that looks something like this:</p><p><strong>Physics packet rapidity: (number of potential interacting physics objects) * ( (other players) * (number of their potential interacting objects) )</strong></p><p>More precise values could be attained by taking into account distance and understand exactly what and how the interpolation function is working. Building an interpolation function that is capable of rewinding and accurately re-simulating the small area around the player is obviously a very desirable course of action, because it removes the primary source of uncertainty and would allow for a lot more breathing room.</p><p>Either way, the general idea still stands - the closer your player is to a physics object, the more often that object (and the player) need to be updated. The closer your player is to another player, the update speed goes up exponentially. And always remember to send velocity data too!</p><p>I will make a less theoretical post on this after I&rsquo;ve had an opportunity to do testing on what <em>really</em> works.</p></article></div></section><section><div class=dim><aside><h2>Assembly CAS implementation</h2><ul></ul></aside><article><p><pre class=language-cpp><code>inline unsigned char BSS_FASTCALL asmcas(int *pval, int newval, int oldval)
  {
      unsigned char rval;
      __asm {
#ifdef BSS_NO_FASTCALL //if we are using fastcall we don&#39;t need these instructions
        mov EDX, newval
        mov ECX, pval
#endif
        mov EAX, oldval
        lock cmpxchg [ECX], EDX
        sete rval // Note that sete sets a &#39;byte&#39; not the word
      }
      return rval;
  }</code></pre>This was an absolute bitch to get working in VC++, so maybe this will be useful to someone, somewhere, somehow. The GCC version I based this off of can be found <a href=http://pages.cs.wisc.edu/~remzi/Classes/537/Fall2005/Projects/P3/cas.c>here</a>.</p><p>Note that, obviously, this will only work on x86 architecture.</p></article></div></section><section><div class=dim><aside><h2>Function Pointer Speed</h2><ul></ul></aside><article><p>So after a lot of misguided profiling where I ended up just testing the stupid CPU cache and its ability to fucking predict what my code is going to do, I have, for the most part, demonstrated the following:</p><code>if(!(rand()%2d)) footest.nothing();
else footest.nothing2();</code><p>is slightly faster then</p><code>(footest.*funcptr[rand()%2])();</code><p>where funcptr is an array of the possible function calls. I had suspected this after I looked at the assembly, and a basic function pointer call like that takes around 11 instructions whereas a normal function call takes a single instruction.</p><p>In debug mode, however, if you have more then 2 possibilities, a switch statement&rsquo;s very existence takes up almost as many instructions as a single function pointer call, so the function pointer array technique is significantly faster with 3 or more possibilities. However, in release mode, if you write something like switch(rand()%3) and then just the function calls, the whole damn thing gets its own super special optimization that reduces it to about 3 instructions and hence makes the switch statement method slightly faster.</p><p>In all of these cases though the speed difference for 1000 calls is about 0.007 milliseconds and varies wildly. The CPU is doing so much architectural optimization that it most likely doesn&rsquo;t really matter which method is used. I do find it interesting that the switch statement gets super-optimized in certain situations, though.</p></article></div></section><section><div class=dim><aside><h2>Most Bizarre Error Ever</h2><ul></ul></aside><article><p>Ok probably not the most bizarre error <em>ever</em> but it&rsquo;s definitely the weirdest for me.</p><p>My graphics engine has a Debug, a Release, and a special Release STD version that&rsquo;s compatible with CLI function requirements and other dependencies. These are organized as 3 separate configurations in my solution for compiling. Pretty normal stuff.</p><p>My example applications are all set to be dependent on the graphics engine project, which means visual studio automatically compiles the proper lib file into the project.</p><p>Well, normally.</p><p>My graphics engine examples suddenly and inexplicably stopped working in Release mode, but not Debug mode. While this normally signals an uninitialized variable problem, I had only flipped a few negative signs since the last build, so it was completely impossible for that to be the cause. I was terribly confused so I went into the code and discovered that it was failing because the singleton instance of the engine was null.</p><p>Now, if you know what a singleton is, you should know that this is absolutely, completely impossible. Under normal conditions, that is. At first I thought my engine instance assignment had been fucked, but that was working fine. In fact, the engine existed for one call, and then didn&rsquo;t exist for the other.</p><p>Then I checked <em>where the calls were coming from</em>. What I discovered next blew my mind.</p><p>One call was from Planeshader.dll; The other call was from <em>Planeshader_std.dll</em> - oh crap.</p><p>Somehow, visual studio had managed to link my executable to <em><strong>both</strong></em> instances of my graphics engine <strong>at the same time</strong>. I&rsquo;m not entirely sure how it managed that feat since compiling two identical lib files creates thousands of collisions, but it appears that half the function calls were being sent to one dll, and half the function calls being sent to the other. My engine was trying to run in <em>two dlls simultaneously</em>.</p><p>I solved the problem simply by explicitly specifying the lib file in the project properties.</p><p>Surely my skill at breaking things knows no bounds.</p></article></div></section><section><div class=dim><aside><h2>Physics-oriented Network Interpolation</h2><ul></ul></aside><article><p>Syncing a game over a client server connection is not an easy task. It&rsquo;s actually extraordinarily difficult and is almost completely reliant on the quality of interpolation. Due to the nature of interpolation, it gets exponentially more inaccurate the more time is spent doing it. Therefore, a game designer should want to minimize the amount needed. This is not an easy task, but it usually involves using the server as a middleman to halve the interpolation time. The only issue with this is that the server&rsquo;s interpolation becomes reality, so while a client interpolation can be fairly inaccurate and simply corrected later on, the server&rsquo;s world is reality.</p><p>This has two consequences, one is fairly obvious: the server interpolation between the time the player hit the move button and their current location must be extremely accurate, while the client interpolation can be fairly sloppy. If however, the server is a dedicated server (or has a parallel physics world), then a small trick can be employed - the server&rsquo;s physics world need only be updated with every physics packet received, enabling an accurate physics simulation for a fraction of a second and eliminating a small amount of interpolation. An additional measure can be taken by utilizing a peer-to-peer connection between the clients and sending tiny packets of button notifications. These, if they happen to get to the client before the server packet does, can improve perceived responsiveness by giving the client a heads up on whether a player has fired something.</p><p>Another possible method of interpolation involves knowing where a shot should be and where it is currently in the view of a network player, and accelerating it until it reaches its intended destination. This is, however, problematic in terms of multiplayer shooter games because a shot quite often will hit another player or some object within the timespan of the ping and subsequently cause massive confusion on part of the player who thinks his shot is on the bottom of the screen rather then the top. In cases where accuracy is crucial, it is often best to simply have shots appear &ldquo;out of nowhere&rdquo; in front of the player that shot them, but <em><strong>play the shooting animation at the same time</strong></em>. This visual feedback triggers an instinctual &ldquo;oh its lag&rdquo; response, instead of &ldquo;where the hell did that shot come from.&rdquo;</p><p>All of these methods are applied in anticipation of a perfect interpolation function, which is of course impossible. Hence, while we can mitigate most of the problems that exist even with a perfect interpolation problem, it comes down to simply coding a better, faster interpolation function.</p></article></div></section></main><footer><p><span>Copyright &copy;2025 Erik McClure</span> <a href=https://erikmcclure.com/sitemap.xml>Sitemap</a> | <a href=https://erikmcclure.com/blog/index.xml>RSS Feed</a></p></footer></div><script>"use strict";window.onload=function(){e=document.getElementsByClassName("math");for(var e,t=0,n=e.length;t<n;t++)renderMathInElement(e[t],{delimiters:[{left:"$$",right:"$$",display:!1},{left:"\\[",right:"\\]",display:!0}]})}</script></body></html>