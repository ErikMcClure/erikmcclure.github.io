+++
categories = ["blog"]
date = "2018-09-18T11:03:49-07:00"
draft = true
title = "Software Engineering Is Bad, But It's Not That Bad"

+++
I've been writing code for over 12 years, and for a while I've been disgusted by the sorry state of programming. That's why I felt a deep kinship with Nikita Prokopov's article, [Software Disenchantment](http://tonsky.me/blog/disenchantment/ "Software Disenchantment"). It captures the intense feeling of frustration I have with the software industry as a whole and the inability of modern programmers to write anything even remotely resembling efficient systems.

**Unfortunately, most of it is wrong.**

While I wholeheartedly agree with what Nikita is saying, I am afraid that he doesn't seem to understand how modern computers work. This is unfortunate, because a misinformed article like this weakens both our positions and makes it more difficult to convince people that software bloat is a problem. Most of the time, his frustrations are valid, but his reasoning is misdirected. So, in this article, I'm going to write counterpoints to some of the more problematic claims.

**1.Smooth Scroll**

One of my hobbies is game development, and I can assure you that doing anything at 4K resolution at 60 FPS on a laptop is _insanely hard_. Most games struggle to render at 4K 60 FPS with hugely powerful GPUs, and 2D games that render at that speed are usually graphically simplistic, in that they can have lots of fancy drawings, but drawing 200 fancy images on the screen with simplistic blending is not very difficult. A video is just a single 4K image rendered 60 times a second, which is trivial. Web renderers can't do that, because HTML has extremely specific composition rules that will break a na√Øve graphics pipeline. There is also another crucial difference between a 4K video, a video game, and a webpage: **text**.

High quality anti-aliased and sometimes sub-pixel hinted text at 10 different sizes on different colored backgrounds blended with different transparencies is just really goddamn hard to render. Games don't do any of that. They have simple UIs with one or two fonts that are often either pre-rendered or use signed distance fields to approximate them. A web browser is rendering arbitrary unicode text that could include emojis and god knows what else. Sometimes it's even doing transformation operations _in realtime_ on an SVG vector image. This is _hard_, and getting it to run on a GPU is even harder. One of the most impressive pieces of modern web technology is Firefox's [WebRender](https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/), which actually pushes almost the entire composition pipeline to the GPU, allowing it to serve most webpages at a smooth 60 FPS. This is basically as fast as you could possibly make any of this, which is why this is one of the weirdest complaints, because Firefox's WebRender is incredibly fast by any measure.

I think the real issue here, and perhaps what Nikita was getting at, is that the _design_ of modern webpages is so bloated that the web browsers can't keep up. They're getting inundated with `&lt;div&;gt;` trees the size of Mount Everest and 10 megs worth of useless javascript bootstrapping ad campaigns that load entire miniature videos. However, none of this has anything to do with the resolution or refresh rate of your screen. Useless crap is going to be slow no matter what your GPU is. Inbox taking 13 seconds to load anything is completely unacceptable, but animating anything other than a white box in HTML is far more expensive than you think, and totally unrelated.

**2.Latency**

Latency has to be one of the least understood values in computer electronics today. It is true that many text editors have abysmal response times caused by terrible code, but it's a lot easier to screw this up than a lot of people seem to realize. While CPUs have gotten exponentially faster, the latency between hardware components hasn't improved at all, and in many cases _cannot possibly improve_. This is because latency is dominated by physical separation and connective material. The speed of light hasn't changed in the past 48 years, so why would the latency change? 

The reason this is a problem is that you cannot put anything on top of a system without increasing its latency, and you can't decrease the latency. At all. That 42-year-old emacs system was basically operating at its theoretical maximum because there was barely anything between the terminal and the keyboard. It is simply physically impossible to make that system more responsive, no matter how fast the CPU gets. Saying it's surprising that a modern text editor is somehow slower than a system _operating at the minimum possible latency_ makes absolutely no sense, because if you start putting more and more things between the keyboard and the screen, it _has_ to make the latency go up. This has _literally nothing to do_ with how fast your GPU is. **Nothing**.

It's actually much worse, because old computers didn't have to worry about silly things like _composition_. They'd do v-sync themselves, manually, drawing the cursor or text in between vertical blanks of the monitor. Modern graphics draw to a separate buffer, which is then flipped to the screen on it's next refresh. The _consequence_, however, is that if you start drawing a new frame right after a vertical blank, [you ignore all the input you got that frame!](https://blackhole12.com/blog/problem-of-vsync/) You can only start drawing things after you've processed all of the user input, which means once you start drawing, it's game over. This means that if a vertical blank happens every 16 ms, and you start drawing at the beginning of this frame, you have to wait 16 ms to process the user input, then draw the frame and _wait an additional 16 ms for the new buffer to get flipped to the screen!_

That's 32ms of latency right there, and that's if you don't screw anything up. As modern hardware connections get more and more complex to the point where your USB jacks have drivers, they introduce more latency. Wireless systems introduce _even more_ latency. Hardware abstraction layers, badly written drivers, and even the motherboard BIOS can all negatively impact the latency and _we haven't even gotten to the application yet_. But again, nothing can remove latency from a system. At best, perfectly written software would add negligible latency and aspire to approach the latency of your emacs terminal, but could never surpass it (unless we start using graphene).

We should all be pushing for low-latency systems, but bloated apps are hardly the only culprit here. This is something everyone, from the hardware to the OS to the libraries, has to cooperate on if we want responsive computers.

**3.Features**

I find it kind of silly to try and argue that computers today have almost no new features. Of course they have new features. A lot of the features are ones I don't use or care about, but they do sometimes get new features and _occasionally_ they are actually nice. I think the real problem here that each new feature, for some inexplicable reason, requires exponentially more resources than the feature before it, often for no apparent reason. Other times, basic features that are trivial to implement are left out, also for no apparent reason. 

For example, [Discord](https://discordapp.com/) still doesn't know how to de-duplicate resent client messages over a spotty connection despite this being a solved problem for decades, and if a message is deleted too quickly, the deletion packet is received before the message itself, and the client just... never deletes it. This could be trivially solved with a tombstone or even just a temporary queue of unmatched deletion messages, yet the client instead creates a ghost message that you can't get rid of until you _restart the client_. There is absolutely no reason for this feature to not exist. It's not even bloat, it's just ridiculous.

**4.Compilers**

Now, this is one I really don't understand. Any language other than C++ or Rust basically compiles instantly for anything that's less than 100k lines of code. At work, we have a C# monstrosity that's half a million lines of code and compiles in 20 seconds. That's pretty fast. Most other langauge are JIT-compiled, so you can just run them instantly. Even then, you don't really want to optimize for compile time on Release mode unless you're just removing unnecessary bloat, and many modern compilers take a long time to compile things because they're doing ridiculously complex optimizations that may or may not require solving NP-hard optimization problems, [which some of them actually do](https://github.com/google/souper) for small functions.

The original C language and Jonathon Blow's language compile really fast because they don't do anything. They don't help you, they have an incredibly basic type system, they don't do advanced memory analysis or a bunch of absurd optimizations to take advantage of the labyrinthine x86-64 instruction set. Languages in the 1990s compiled instantly because they had no features. Modern languages do all sorts of hyperoptimization in the compiler level that programmers couldn't even dream of back then. That's why it takes forever to compile on release mode. Heck, sometimes compilation is actually _disk bound_, which is why getting an SSD can dramatically improve compile times for large projects. That has _nothing to do_ with the compiler!

Years ago, I wrote my own article on this, and I feel it is still relevant today. I asked [if anyone actually wants good software](https://blackhole12.com/blog/does-anyone-actually-want-good-software/). At least now, I know some people do.